# ğŸ›¡ï¸ Adversarial Robustness Benchmark

An end-to-end ML pipeline that trains, attacks, defends, and deploys deep neural networks to demonstrate adversarial robustness. Users can interact with a live demo to upload images, apply adversarial attacks in real-time, and compare standard vs. robust model predictions.

**Live Demo:** [HuggingFace Spaces](https://huggingface.co/spaces/YOUR_USERNAME/adversarial-robustness-demo)

## ğŸ¯ What This Project Demonstrates

- **Model Training**: Train ResNet-18 on CIFAR-10 from scratch using PyTorch
- **Adversarial Attacks**: Implement FGSM, PGD, and DeepFool attacks
- **Adversarial Training**: Train a robust model using PGD-based adversarial training
- **Model Evaluation**: Compare accuracy, robustness, and confidence under attack
- **Experiment Tracking**: Log all experiments with MLflow
- **Deployment**: Serve models via FastAPI + Streamlit on HuggingFace Spaces
- **CI/CD**: Automated testing and linting with GitHub Actions

## ğŸ“ Project Structure

```
adversarial-robustness-demo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # Model architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ resnet.py    # ResNet-18 for CIFAR-10
â”‚   â”œâ”€â”€ attacks/         # Adversarial attack implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fgsm.py      # Fast Gradient Sign Method
â”‚   â”‚   â”œâ”€â”€ pgd.py       # Projected Gradient Descent
â”‚   â”‚   â””â”€â”€ deepfool.py  # DeepFool attack
â”‚   â”œâ”€â”€ training/        # Training loops
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ standard.py  # Standard training
â”‚   â”‚   â””â”€â”€ adversarial.py # Adversarial training (PGD-AT)
â”‚   â”œâ”€â”€ api/             # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ utils/           # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data.py      # Data loading & transforms
â”‚   â”‚   â””â”€â”€ metrics.py   # Evaluation metrics
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py           # Streamlit frontend
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb # Training & analysis notebook
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml      # Hyperparameters
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py         # Training entry point
â”‚   â””â”€â”€ evaluate.py      # Evaluation entry point
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_attacks.py  # Unit tests
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml       # GitHub Actions CI
â”œâ”€â”€ Dockerfile           # For HuggingFace Spaces
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Setup
```bash
git clone https://github.com/YOUR_USERNAME/adversarial-robustness-demo.git
cd adversarial-robustness-demo
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 2. Train Models
```bash
# Train standard model
python scripts/train.py --mode standard --epochs 50

# Train adversarial model (PGD-AT)
python scripts/train.py --mode adversarial --epochs 50

# View experiments in MLflow
mlflow ui
```

### 3. Evaluate
```bash
python scripts/evaluate.py --model-path checkpoints/standard_best.pt --attacks fgsm pgd deepfool
python scripts/evaluate.py --model-path checkpoints/robust_best.pt --attacks fgsm pgd deepfool
```

### 4. Run Demo Locally
```bash
# Option A: Streamlit only (simple)
streamlit run frontend/app.py

# Option B: FastAPI + Streamlit (production-like)
uvicorn src.api.main:app --port 8000 &
streamlit run frontend/app.py
```

## ğŸ“Š Results

| Model    | Clean Acc | FGSM (Îµ=8/255) | PGD-20 (Îµ=8/255) | DeepFool |
|----------|-----------|-----------------|-------------------|----------|
| Standard | ~93%      | ~25%            | ~0.5%             | ~15%     |
| Robust   | ~84%      | ~55%            | ~48%              | ~52%     |

## ğŸ› ï¸ Tech Stack

- **ML Framework**: PyTorch
- **Experiment Tracking**: MLflow
- **Backend**: FastAPI
- **Frontend**: Streamlit
- **Deployment**: HuggingFace Spaces / Docker
- **CI/CD**: GitHub Actions

## ğŸ“– Research Context

This project is inspired by adversarial robustness research, particularly:
- Goodfellow et al., "Explaining and Harnessing Adversarial Examples" (FGSM)
- Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks" (PGD)
- Moosavi-Dezfooli et al., "DeepFool: A Simple and Accurate Fooling Method" (DeepFool)

## ğŸ“„ License

MIT License
