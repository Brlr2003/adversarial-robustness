% Thesis Metadata
% Fill in your personal information and thesis details here

% Thesis type
\MSc  % For Master of Science

% Author information
\author{Omar}{Alsafarti}
\studentid{42012418}

% Thesis titles
\title{ADVERSARIAL ATTACKS ON DEEP NEURAL NETWORKS: GENERATION, EVALUATION, AND ROBUST TRAINING}
\titleenglish{ADVERSARIAL ATTACKS ON DEEP NEURAL NETWORKS: GENERATION, EVALUATION, AND ROBUST TRAINING}
\titlealbanian{SULMET KUNDËRSHTARE NË RRJETET NEURALE TË THELLA: GJENERIMI, VLERËSIMI DHE TRAJNIMI I QËNDRUESHËM}

% Dates
\date{June 2026}
\dateenglish{June 2026}

% Supervisor
\supervisor{Assoc. Prof. Dr. Arban Uka}
\supervisorenglish{Assoc. Prof. Dr. Arban Uka}

% Department and Faculty
\department{Computer Engineering}
\departmentenglish{Computer Engineering}
\faculty{Faculty of Architecture and Engineering}
\facultyenglish{Faculty of Architecture and Engineering}

% Head of Department (for approval page)
\headofdepartment{Dr. Florenc Skuka} 


% Abstract (English)
\abstract{
Deep learning models achieve impressive performance on tasks like image classification, but they have a serious weakness: small, carefully crafted changes to an input, often invisible to the human eye, can cause these models to misclassify with high confidence. These manipulated inputs are known as adversarial examples, and their existence raises real concerns for systems used in autonomous driving, biometrics, and cybersecurity.

This thesis presents a practical study of adversarial attacks on image classification models. A ResNet-18 convolutional neural network is trained on the CIFAR-10 benchmark dataset, and three white-box attacks (FGSM, PGD, DeepFool) are implemented and compared. The standard model achieves 94.1\% clean accuracy but drops to 1.2\% under a 20-step PGD attack at $\epsilon = 8/255$. PGD-based adversarial training is then applied, producing a robust model that maintains 45.1\% accuracy under the same attack while losing 10.2 percentage points of clean accuracy. DeepFool analysis shows that the standard model's decision boundaries are on average only 0.248 $L_2$ distance from the data points, while adversarial training pushes this distance to 0.892. These results confirm how severe adversarial vulnerability is in deep neural networks and show that adversarial training is an effective and practical defense.
}

% Keywords (English)
\keywordsenglish{Adversarial Examples, Deep Neural Networks, FGSM, PGD, DeepFool, Adversarial Training, Robustness}

% Abstrakt (Albanian)
\abstrakt{
Modelet e mësimit të thellë (Deep Learning) arrijnë performancë të jashtëzakonshme në detyra si klasifikimi i imazheve, megjithatë ato janë të njohura për cenueshmërinë ndaj shembujve kundërshtarë (Adversarial Examples), hyrje që janë ndryshuar me ndryshime të vogla, shpesh të padukshme, të cilat shkaktojnë klasifikim të gabuar me besueshmëri të lartë. Kjo cenueshmëri ngre shqetësime serioze për sigurinë e sistemeve të përdorura në drejtimin autonom, biometrikë dhe sigurinë kibernetike.

Kjo tezë fokusohet në studimin praktik të sulmeve kundërshtare kundër modeleve të klasifikimit të imazheve. Një rrjet nervor konvolucionar (Convolutional Neural Network) ResNet-18 trajnohet në grupin e të dhënave CIFAR-10 dhe tre sulme white-box (FGSM, PGD, DeepFool) implementohen dhe krahasohen. Modeli standard arrin saktësi 94.1\% në të dhëna të pastra por bie në 1.2\% nën sulmin PGD me 20 hapa. Trajnimi kundërshtar (Adversarial Training) i bazuar në PGD aplikohet më pas, duke prodhuar një model të qëndrueshëm që ruan 45.1\% saktësi nën të njëjtin sulm, me koston e një rënieje prej 10.2 pikësh përqindjeje në saktësinë e pastër. Këto rezultate konfirmojnë ashpërsinë e cenueshmërisë kundërshtare dhe vërtetojnë trajnimin kundërshtar si një mekanizëm mbrojtës efektiv.
}

% Keywords (Albanian)
\keywords{Shembuj Kundërshtarë, Rrjete Neurale të Thella, FGSM, PGD, DeepFool, Trajnim Kundërshtar, Qëndrueshmëri}

% Acknowledgments
\acknowledgments{
I would like to express my sincere gratitude to my supervisor, Assoc. Prof. Dr. Arban Uka, for his invaluable guidance, support, and encouragement throughout my research. The expert knowledge which he possessed about the subject matter together with his constructive criticism created the foundation for this research project.

I thank my family for their ongoing support and understanding during my academic studies, and I also appreciate the academic atmosphere which my friends and colleagues at Epoka University created.

I want to express my gratitude to Epoka University for granting me the chance to complete my master's degree and for offering the resources needed to conduct my research.
}

% Dedication (optional - uncomment if needed)
% \dedication{
%     To my family...
% }

% Abbreviations
\abbreviations{
\begin{tabular}{ll}
AE & Adversarial Example \\
BIM & Basic Iterative Method \\
C\&W & Carlini-Wagner Attack \\
CNN & Convolutional Neural Network \\
DNN & Deep Neural Network \\
FGSM & Fast Gradient Sign Method \\
GPU & Graphics Processing Unit \\
MPS & Metal Performance Shaders \\
PGD & Projected Gradient Descent \\
PGD-AT & PGD-based Adversarial Training \\
ResNet & Residual Network \\
SGD & Stochastic Gradient Descent \\
UAP & Universal Adversarial Perturbation \\
\end{tabular}
}

% Optional: Disable table or figure lists if not needed
% \NoTableList
% \NoFigureList