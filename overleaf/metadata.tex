% Thesis Metadata
% Fill in your personal information and thesis details here

% Thesis type
\MSc  % For Master of Science

% Author information
\author{Omar}{Alsafarti}
\studentid{42012418}

% Thesis titles
\title{ADVERSARIAL ATTACKS ON DEEP NEURAL NETWORKS: GENERATION, EVALUATION, AND ROBUST TRAINING}
\titleenglish{ADVERSARIAL ATTACKS ON DEEP NEURAL NETWORKS: GENERATION, EVALUATION, AND ROBUST TRAINING}
\titlealbanian{SULMET KUNDËRSHTARE NË RRJETET NEURALE TË THELLA: GJENERIMI, VLERËSIMI DHE TRAJNIMI I QËNDRUESHËM}

% Dates
\date{June 2026}
\dateenglish{June 2026}

% Supervisor
\supervisor{Assoc. Prof. Dr. Arban Uka}
\supervisorenglish{Assoc. Prof. Dr. Arban Uka}

% Department and Faculty
\department{Computer Engineering}
\departmentenglish{Computer Engineering}
\faculty{Faculty of Architecture and Engineering}
\facultyenglish{Faculty of Architecture and Engineering}

% Head of Department (for approval page)
\headofdepartment{Dr. Florenc Skuka} 


% Abstract (English)
\abstract{
Deep learning models achieve strong performance on image classification and related tasks, yet they are vulnerable to adversarial examples: inputs modified by small, human-imperceptible perturbations that cause confident misclassification. This vulnerability raises practical concerns for applications in autonomous driving, biometrics, and security.

In this thesis, we implement three white-box adversarial attacks --- FGSM, PGD, and DeepFool --- from scratch and evaluate them on a ResNet-18 trained on CIFAR-10. The standard model achieves 94.1\% clean accuracy but drops to 1.2\% under a 20-step PGD attack at $\epsilon = 8/255$. We then apply PGD-based adversarial training, which produces a model that retains 45.1\% accuracy under the same attack at a cost of 10.2 percentage points of clean accuracy. DeepFool analysis reveals that the standard model's decision boundaries lie only 0.248 $L_2$ from the data on average; adversarial training increases this distance to 0.892. The results demonstrate both the severity of adversarial vulnerability and the effectiveness of adversarial training as a defense.
}

% Keywords (English)
\keywordsenglish{Adversarial Examples, Deep Neural Networks, FGSM, PGD, DeepFool, Adversarial Training, Robustness}

% Abstrakt (Albanian)
\abstrakt{
Modelet e mësimit të thellë (Deep Learning) arrijnë performancë të jashtëzakonshme në detyra si klasifikimi i imazheve, megjithatë ato janë të njohura për cenueshmërinë ndaj shembujve kundërshtarë (Adversarial Examples), hyrje që janë ndryshuar me ndryshime të vogla, shpesh të padukshme, të cilat shkaktojnë klasifikim të gabuar me besueshmëri të lartë. Kjo cenueshmëri ngre shqetësime serioze për sigurinë e sistemeve të përdorura në drejtimin autonom, biometrikë dhe sigurinë kibernetike.

Kjo tezë fokusohet në studimin praktik të sulmeve kundërshtare kundër modeleve të klasifikimit të imazheve. Një rrjet nervor konvolucionar (Convolutional Neural Network) ResNet-18 trajnohet në grupin e të dhënave CIFAR-10 dhe tre sulme white-box (FGSM, PGD, DeepFool) implementohen dhe krahasohen. Modeli standard arrin saktësi 94.1\% në të dhëna të pastra por bie në 1.2\% nën sulmin PGD me 20 hapa. Trajnimi kundërshtar (Adversarial Training) i bazuar në PGD aplikohet më pas, duke prodhuar një model të qëndrueshëm që ruan 45.1\% saktësi nën të njëjtin sulm, me koston e një rënieje prej 10.2 pikësh përqindjeje në saktësinë e pastër. Këto rezultate konfirmojnë ashpërsinë e cenueshmërisë kundërshtare dhe vërtetojnë trajnimin kundërshtar si një mekanizëm mbrojtës efektiv.
}

% Keywords (Albanian)
\keywords{Shembuj Kundërshtarë, Rrjete Neurale të Thella, FGSM, PGD, DeepFool, Trajnim Kundërshtar, Qëndrueshmëri}

% Acknowledgments
\acknowledgments{
I would like to thank my supervisor, Assoc. Prof. Dr. Arban Uka, for his guidance throughout this work. His input shaped both the direction of the research and the way I approached the problems. Many of the ideas in this thesis came out of our discussions, and I am grateful for the time he invested.

I also thank my family for their support and patience during my studies. Finally, I thank Epoka University for providing the opportunity and resources that made this research possible.
}

% Dedication (optional - uncomment if needed)
% \dedication{
%     To my family...
% }

% Abbreviations
\abbreviations{
\begin{tabular}{ll}
AE & Adversarial Example \\
BIM & Basic Iterative Method \\
C\&W & Carlini-Wagner Attack \\
CNN & Convolutional Neural Network \\
DNN & Deep Neural Network \\
FGSM & Fast Gradient Sign Method \\
GPU & Graphics Processing Unit \\
MPS & Metal Performance Shaders \\
PGD & Projected Gradient Descent \\
PGD-AT & PGD-based Adversarial Training \\
ResNet & Residual Network \\
SGD & Stochastic Gradient Descent \\
UAP & Universal Adversarial Perturbation \\
\end{tabular}
}

% Optional: Disable table or figure lists if not needed
% \NoTableList
% \NoFigureList