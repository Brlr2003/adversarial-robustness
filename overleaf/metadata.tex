% Thesis Metadata
% Fill in your personal information and thesis details here

% Thesis type
\MSc  % For Master of Science

% Author information
\author{Omar}{Alsafarti}
\studentid{42012418}

% Thesis titles
\title{ADVERSARIAL ATTACKS ON DEEP NEURAL NETWORKS: GENERATION, EVALUATION, AND ROBUST TRAINING}
\titleenglish{ADVERSARIAL ATTACKS ON DEEP NEURAL NETWORKS: GENERATION, EVALUATION, AND ROBUST TRAINING}
\titlealbanian{SULMET KUNDËRSHTARE NË RRJETET NEURALE TË THELLA: GJENERIMI, VLERËSIMI DHE TRAJNIMI I QËNDRUESHËM}

% Dates
\date{June 2026}
\dateenglish{June 2026}

% Supervisor
\supervisor{Assoc. Prof. Dr. Arban Uka}
\supervisorenglish{Assoc. Prof. Dr. Arban Uka}

% Department and Faculty
\department{Computer Engineering}
\departmentenglish{Computer Engineering}
\faculty{Faculty of Architecture and Engineering}
\facultyenglish{Faculty of Architecture and Engineering}

% Head of Department (for approval page)
\headofdepartment{Dr. Florenc Skuka} 


% Abstract (English)
\abstract{
Deep learning models can be fooled by adversarial examples --- inputs with small, carefully chosen perturbations that are imperceptible to humans but cause confident misclassification. This vulnerability has direct implications for safety-critical applications such as autonomous driving, biometric authentication, and cybersecurity.

This thesis investigates adversarial attacks on image classifiers through hands-on implementation and experimentation. A ResNet-18 is trained on CIFAR-10, and three white-box attacks --- FGSM, PGD, and DeepFool --- are implemented from scratch and evaluated. The standard model achieves 94.1\% clean accuracy; a 20-step PGD attack at $\epsilon = 8/255$ reduces this to 1.2\%. PGD-based adversarial training yields a robust model that retains 45.1\% accuracy under the same attack, at the cost of 10.2 percentage points in clean accuracy. DeepFool reveals that the standard model's decision boundaries sit only 0.248 $L_2$ distance from the data on average; adversarial training moves them to 0.892. These findings quantify both the severity of adversarial vulnerability in deep networks and the effectiveness of adversarial training as a defense.
}

% Keywords (English)
\keywordsenglish{Adversarial Examples, Deep Neural Networks, FGSM, PGD, DeepFool, Adversarial Training, Robustness}

% Abstrakt (Albanian)
\abstrakt{
Modelet e mësimit të thellë (Deep Learning) arrijnë performancë të jashtëzakonshme në detyra si klasifikimi i imazheve, megjithatë ato janë të njohura për cenueshmërinë ndaj shembujve kundërshtarë (Adversarial Examples), hyrje që janë ndryshuar me ndryshime të vogla, shpesh të padukshme, të cilat shkaktojnë klasifikim të gabuar me besueshmëri të lartë. Kjo cenueshmëri ngre shqetësime serioze për sigurinë e sistemeve të përdorura në drejtimin autonom, biometrikë dhe sigurinë kibernetike.

Kjo tezë fokusohet në studimin praktik të sulmeve kundërshtare kundër modeleve të klasifikimit të imazheve. Një rrjet nervor konvolucionar (Convolutional Neural Network) ResNet-18 trajnohet në grupin e të dhënave CIFAR-10 dhe tre sulme white-box (FGSM, PGD, DeepFool) implementohen dhe krahasohen. Modeli standard arrin saktësi 94.1\% në të dhëna të pastra por bie në 1.2\% nën sulmin PGD me 20 hapa. Trajnimi kundërshtar (Adversarial Training) i bazuar në PGD aplikohet më pas, duke prodhuar një model të qëndrueshëm që ruan 45.1\% saktësi nën të njëjtin sulm, me koston e një rënieje prej 10.2 pikësh përqindjeje në saktësinë e pastër. Këto rezultate konfirmojnë ashpërsinë e cenueshmërisë kundërshtare dhe vërtetojnë trajnimin kundërshtar si një mekanizëm mbrojtës efektiv.
}

% Keywords (Albanian)
\keywords{Shembuj Kundërshtarë, Rrjete Neurale të Thella, FGSM, PGD, DeepFool, Trajnim Kundërshtar, Qëndrueshmëri}

% Acknowledgments
\acknowledgments{
I owe a great deal to my supervisor, Assoc. Prof. Dr. Arban Uka, who guided this work from start to finish. His deep knowledge of the subject and his willingness to challenge my ideas at every stage shaped both the research and my thinking about it. I am grateful for his patience and for the many hours he spent discussing this work with me.

I also want to thank my family, who put up with a lot during my studies --- late nights, missed gatherings, and the general stress of thesis writing. Their support made all of this possible. My friends and colleagues at Epoka University made the experience more enjoyable than it otherwise would have been.

Finally, I am thankful to Epoka University for giving me the opportunity to pursue this degree and for providing the resources that made the research possible.
}

% Dedication (optional - uncomment if needed)
% \dedication{
%     To my family...
% }

% Abbreviations
\abbreviations{
\begin{tabular}{ll}
AE & Adversarial Example \\
BIM & Basic Iterative Method \\
C\&W & Carlini-Wagner Attack \\
CNN & Convolutional Neural Network \\
DNN & Deep Neural Network \\
FGSM & Fast Gradient Sign Method \\
GPU & Graphics Processing Unit \\
MPS & Metal Performance Shaders \\
PGD & Projected Gradient Descent \\
PGD-AT & PGD-based Adversarial Training \\
ResNet & Residual Network \\
SGD & Stochastic Gradient Descent \\
UAP & Universal Adversarial Perturbation \\
\end{tabular}
}

% Optional: Disable table or figure lists if not needed
% \NoTableList
% \NoFigureList