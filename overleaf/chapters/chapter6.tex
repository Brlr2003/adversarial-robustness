\chapter{CONCLUSION AND FUTURE WORK}
\label{ch:conclusion}

\section{Conclusion}

In this thesis I set out to study how vulnerable deep neural networks are to adversarial attacks and whether adversarial training can meaningfully improve their robustness. I implemented three white-box attacks (FGSM, PGD, DeepFool), tested them on CIFAR-10 using a ResNet-18, and then evaluated PGD-based adversarial training as a defense.

The results come down to three main findings. First, standard models are extremely fragile. My ResNet-18 achieved 94.1\% clean accuracy, but a 20-step PGD attack at $\epsilon = 8/255$ (perturbations you cannot see) brought it down to 1.2\%. Second, if you only test with FGSM you get a misleading picture of how safe your model is. FGSM suggested the model still had 18.3\% accuracy, but PGD showed the real number was 1.2\%. That gap matters a lot for anyone trying to evaluate model security. Third, adversarial training works. It improved robust accuracy from near-zero to 45.1\% under PGD, at the cost of losing 10.2 percentage points of clean accuracy, which is a reasonable trade-off in safety-critical settings.

I also noticed that adversarially trained models have better confidence calibration. The standard model gives near-100\% confidence to its wrong answers on adversarial inputs, while the robust model spreads its probabilities out more and is more honest about uncertainty. This is a useful property in practice, because in real systems you want to know when the model is unsure.

\section{Research Contributions}

Here is what I think this research contributes:

\begin{enumerate}
    \item \textbf{Attack comparison:} I ran FGSM, PGD, and DeepFool on the same ResNet-18 with the same evaluation setup. PGD was the strongest (1.2\% surviving accuracy at $\epsilon=8/255$), FGSM was weaker (18.3\%), and DeepFool achieved 91.1\% success with perturbations $5.4\times$ smaller in $L_2$ norm.

    \item \textbf{Adversarial training numbers:} The adversarially trained model got 45.1\% robust accuracy under PGD and 83.9\% clean accuracy. These match what Madry et al. reported, so my implementation is on the right track.

    \item \textbf{Open-source framework:} All the code is public on GitHub: three attack implementations, two training loops, MLflow integration, and a live Streamlit demo on HuggingFace Spaces. Adding a new attack means writing one Python file.

    \item \textbf{Calibration finding:} The robust model does not just get adversarial examples right more often, it also gives more reasonable confidence scores. The standard model says ``99\% cat'' on a wrong answer; the robust model says ``55\% cat'' on a right one.
\end{enumerate}

\section{Limitations}

There are obvious limitations:

\begin{enumerate}
    \item \textbf{Only CIFAR-10.} The images are tiny ($32 \times 32$, 10 classes). $\epsilon = 8/255$ on a 32-pixel image is different from $\epsilon = 8/255$ on a 224-pixel image. I cannot claim these results transfer to ImageNet without running those experiments.

    \item \textbf{Only ResNet-18.} Vision Transformers, for example, might be more or less vulnerable. I do not know because I did not test them.

    \item \textbf{Only white-box $L_\infty$.} No black-box attacks, no transfer attacks, no $L_2$ or $L_0$ threat models. I also skipped Carlini-Wagner because it would have taken too long to run on my hardware.

    \item \textbf{Only one defense.} PGD adversarial training is not the only option. Certified defenses, randomized smoothing, and input preprocessing exist too. I did not have time to compare them all.

    \item \textbf{Hardware limitations.} Everything ran on an Apple M4 Pro laptop. Adversarial training took 4.5 hours for 50 epochs, which did not leave much room for trying different hyperparameter combinations.
\end{enumerate}

\section{Future Work}

If I had more time, here is what I would do next.

\subsection{Short-term}

\begin{itemize}
    \item \textbf{Add Carlini-Wagner.} It is the gold standard for breaking defenses and I want to know how my adversarially trained model holds up against it.

    \item \textbf{Run the same experiments on MNIST.} The MNIST literature is huge, and it would be good to have a direct comparison. Plus it would show how dataset difficulty affects vulnerability.

    \item \textbf{Test transferability.} Do adversarial examples made for the standard model also fool the robust one? That matters for black-box threat scenarios.

    \item \textbf{Use more PGD restarts.} I only used one random restart, and multiple restarts would give tighter robustness estimates.
\end{itemize}

\subsection{Long-term}

\begin{itemize}
    \item \textbf{Scale up to ImageNet.} The real question is whether these results hold on bigger images with more classes and bigger models.

    \item \textbf{Try certified defenses.} Randomized smoothing gives mathematical guarantees on robustness. That would be a nice complement to the empirical approach I took here.

    \item \textbf{Domain-specific testing.} Medical imaging and autonomous driving are where adversarial robustness actually matters. Running these experiments on X-ray images or traffic sign datasets would be more meaningful than CIFAR-10 cats and dogs.

    \item \textbf{Compare architectures.} Are Vision Transformers more or less vulnerable than CNNs? Nobody seems to have a clear answer yet, and my framework could be extended to test this.

    \item \textbf{Speed up adversarial training.} 4.5 hours for 50 epochs on a small dataset is already painful. Methods like Adversarial Training for Free~\cite{shafahi2019adversarial} could help with that.
\end{itemize}

\section{Final Remarks}

Adversarial vulnerability is one of the biggest open problems for deploying deep learning in the real world. Through this thesis I have shown just how severe the problem is (a 94\% accurate model reduced to near-zero by invisible perturbations) and that adversarial training is a practical way to fight back, even if it comes at a cost.

As deep learning gets used more and more in autonomous vehicles, medical diagnosis, security systems, and other high-stakes applications, making sure these models can withstand adversarial attacks is going to become increasingly important. I hope that the implementations, results, and interactive demo from this research can serve as both a reference for others working in adversarial robustness and a concrete illustration of why this work matters.