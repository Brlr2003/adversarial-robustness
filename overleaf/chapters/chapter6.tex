\chapter{CONCLUSION AND FUTURE WORK}
\label{ch:conclusion}

\section{Conclusion}

We set out to study how vulnerable deep neural networks are to adversarial attacks and whether adversarial training provides an effective defense. We implemented three attacks --- FGSM, PGD, and DeepFool --- from scratch, evaluated them on a ResNet-18 trained on CIFAR-10, and applied PGD-based adversarial training.

We find that standard models are extremely vulnerable. Our ResNet-18 achieves 94.1\% clean accuracy, but a 20-step PGD attack at $\epsilon = 8/255$ reduces this to 1.2\%. The perturbation is imperceptible. We also find that evaluating with FGSM alone (18.3\% robust accuracy) gives a misleading picture --- the gap between FGSM and PGD illustrates why multi-step attacks are necessary for reliable evaluation.

Adversarial training brings PGD robust accuracy from 1.2\% to 45.1\%, at a cost of 10.2 percentage points of clean accuracy (94.1\% to 83.9\%). Whether this trade-off is acceptable depends on the application. For safety-critical systems, it is clearly preferable to have a model that degrades gracefully under attack.

We also observe that the adversarially trained model exhibits better confidence calibration. The standard model assigns near-100\% confidence to incorrect predictions under attack. The adversarially trained model produces more reasonable confidence scores, which may be valuable on its own for applications that need to detect unreliable predictions.

\section{Research Contributions}

The contributions of this thesis are:

\begin{enumerate}
    \item \textbf{Attack comparison:} We evaluate FGSM, PGD, and DeepFool on the same model with the same evaluation protocol. PGD is the strongest attack (1.2\% surviving accuracy), FGSM is weaker (18.3\%), and DeepFool achieves a 91.1\% success rate with perturbations that are $5.4\times$ smaller in $L_2$.

    \item \textbf{Adversarial training evaluation:} We report 45.1\% PGD robust accuracy and 83.9\% clean accuracy, consistent with the results of Madry et al.~\cite{madry2018towards}.

    \item \textbf{Open-source code:} We release the full codebase on GitHub, including all attack implementations, training scripts, evaluation code, and an interactive demo on HuggingFace Spaces. The modular design allows new attacks to be added by writing a single file.

    \item \textbf{Confidence calibration:} We observe that the adversarially trained model produces better-calibrated confidence scores. The standard model outputs 99\% confidence on incorrect predictions; the adversarially trained model outputs approximately 55\% on correct ones.
\end{enumerate}

\section{Limitations}

We acknowledge the following limitations:

\begin{enumerate}
    \item \textbf{CIFAR-10 only.} Our experiments use $32 \times 32$ images with 10 classes. The perceptual meaning of $\epsilon = 8/255$ differs at higher resolutions. We cannot claim these results generalize to ImageNet without running those experiments.

    \item \textbf{ResNet-18 only.} We tested a single architecture. Other architectures, such as Vision Transformers, may exhibit different robustness properties.

    \item \textbf{White-box $L_\infty$ only.} We did not evaluate black-box attacks, transfer attacks, or $L_2$/$L_0$ threat models. We had planned to include the Carlini-Wagner attack but it was too computationally expensive for our hardware.

    \item \textbf{Single defense.} We evaluated only PGD adversarial training. Certified defenses, randomized smoothing, and input preprocessing are also worth investigating.

    \item \textbf{Hardware constraints.} Training on an Apple M4 Pro limited our ability to run extensive hyperparameter searches. Adversarial training on CIFAR-10 alone took 4.5 hours.
\end{enumerate}

\section{Future Work}

We identify several directions for future work.

\subsection{Short-term}

\begin{itemize}
    \item \textbf{Carlini-Wagner attack.} This attack is the standard for evaluating defenses. Testing whether our adversarially trained model withstands C\&W would strengthen the robustness claims.

    \item \textbf{MNIST experiments.} MNIST experiments would allow direct comparison with a larger body of existing work and would provide insight into whether simpler datasets are more or less vulnerable.

    \item \textbf{Transferability.} Testing whether adversarial examples generated against the standard model transfer to the adversarially trained model is relevant for black-box threat models.

    \item \textbf{Additional PGD restarts.} Using a single restart provides a lower bound on vulnerability. More restarts would tighten this bound.
\end{itemize}

\subsection{Long-term}

\begin{itemize}
    \item \textbf{ImageNet-scale experiments.} Larger images, more classes, and larger models would test whether our findings generalize.

    \item \textbf{Certified defenses.} Randomized smoothing provides formal robustness guarantees and would complement the empirical approach taken here.

    \item \textbf{Domain-specific evaluation.} CIFAR-10 is suitable for benchmarking, but the practical applications are in medical imaging and autonomous driving. Evaluating on domain-specific data (e.g., X-ray images, traffic signs) is a natural next step.

    \item \textbf{Architecture comparison.} The relative robustness of Vision Transformers versus CNNs is an open question. Our framework could be extended to study this.

    \item \textbf{Efficient adversarial training.} Methods such as Adversarial Training for Free~\cite{shafahi2019adversarialfree} reduce the computational cost of adversarial training and would make it feasible to run on larger-scale problems.
\end{itemize}

\section{Final Remarks}

Adversarial vulnerability is one of the main open problems in deploying deep learning systems. Our experiments demonstrate both the severity of the problem --- a 94.1\%-accurate model reduced to 1.2\% by an imperceptible perturbation --- and the effectiveness of adversarial training as a defense, despite its cost. As deep learning is deployed in settings where errors have real consequences, addressing adversarial robustness becomes increasingly important. We hope that the code, results, and demo from this thesis are useful to others working in this area.

