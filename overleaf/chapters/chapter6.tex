\chapter{CONCLUSION AND FUTURE WORK}
\label{ch:conclusion}

\section{Conclusion}

In this thesis I set out to study how vulnerable deep neural networks are to adversarial attacks and whether adversarial training can meaningfully improve their robustness. I implemented three white-box attacks (FGSM, PGD, DeepFool), tested them on CIFAR-10 using a ResNet-18, and then evaluated PGD-based adversarial training as a defense.

The results come down to three main findings. First, standard models are extremely fragile. My ResNet-18 achieved 94.1\% clean accuracy, but a 20-step PGD attack at $\epsilon = 8/255$ (perturbations you cannot see) brought it down to 1.2\%. Second, if you only test with FGSM you get a misleading picture of how safe your model is. FGSM suggested the model still had 18.3\% accuracy, but PGD showed the real number was 1.2\%. That gap matters a lot for anyone trying to evaluate model security. Third, adversarial training works. It improved robust accuracy from near-zero to 45.1\% under PGD, at the cost of losing 10.2 percentage points of clean accuracy, which is a reasonable trade-off in safety-critical settings.

I also noticed that adversarially trained models have better confidence calibration. The standard model gives near-100\% confidence to its wrong answers on adversarial inputs, while the robust model spreads its probabilities out more and is more honest about uncertainty. This is a useful property in practice, because in real systems you want to know when the model is unsure.

\section{Research Contributions}

Here is what I think this research contributes:

\begin{enumerate}
    \item \textbf{Empirical Attack Comparison:} I compared FGSM, PGD, and DeepFool side by side under the same experimental conditions, on the same model and dataset. This gave a clear picture of how they differ in effectiveness, perturbation characteristics, and computation time. PGD came out as the strongest first-order attack, and DeepFool found perturbations $5.4\times$ smaller in $L_2$ norm than the fixed-budget attacks.

    \item \textbf{Adversarial Training Analysis:} I quantified the accuracy versus robustness trade-off under PGD-based adversarial training, getting 45.1\% PGD robustness and 83.9\% clean accuracy. These numbers match the published benchmarks, which validates my implementation and provides a reference point for the CIFAR-10/ResNet-18 setting.

    \item \textbf{Reproducible Framework:} I built a complete, modular implementation with MLflow tracking, custom implementations of all three attacks, both training procedures, and an interactive demo on HuggingFace Spaces. The code is publicly available and set up so that new attacks or defenses can be added easily.

    \item \textbf{Confidence Calibration Observation:} I found that adversarial training improves confidence calibration. The robust model gives more spread-out predictions compared to the standard model, which is overconfident and wrong on adversarial inputs.
\end{enumerate}

\section{Limitations}

That said, this research has several limitations I want to be upfront about:

\begin{enumerate}
    \item \textbf{Dataset Scope:} I only tested on CIFAR-10, which has small $32 \times 32$ images in 10 classes. The results might not generalize directly to larger datasets like ImageNet, where the same perturbation budgets would look different visually.

    \item \textbf{Architecture Scope:} I only used ResNet-18. Other architectures, especially Vision Transformers (ViTs), might have different vulnerability profiles and could respond differently to adversarial training.

    \item \textbf{Attack Types:} I focused on white-box $L_\infty$ attacks. I did not look at black-box attacks, transfer attacks, or attacks under the $L_2$ and $L_0$ threat models. I also did not implement the Carlini-Wagner attack because of how computationally expensive it is, though I discussed it in the literature review.

    \item \textbf{Defense Mechanisms:} I only tested PGD-based adversarial training as a defense. There are other approaches like certified defenses, input preprocessing, randomized smoothing, and detection-based methods that I did not include.

    \item \textbf{Computational Constraints:} I trained everything on a single Apple M4 Pro instead of dedicated GPU hardware, which limited how many epochs, attack iterations, and random restarts I could try. Adversarial training alone took about 4.5 hours for 50 epochs, so there was not much room for extensive hyperparameter tuning.
\end{enumerate}

\section{Future Work}

There are several directions I would like to explore if I continue this work.

\subsection{Short-term Future Work}

In the near term, these are the extensions I would prioritize:

\begin{itemize}
    \item \textbf{Carlini-Wagner Attack:} Implementing the C\&W $L_2$ attack would provide a more complete picture of the adversarially trained model's robustness, particularly since C\&W was specifically designed to circumvent defenses that gradient-masking attacks cannot break.
    
    \item \textbf{MNIST Experiments:} Extending the evaluation to MNIST with a smaller CNN would enable direct comparison with the extensive MNIST results in the literature and demonstrate how adversarial vulnerability varies with dataset complexity.
    
    \item \textbf{Transferability Study:} Investigating whether adversarial examples crafted on the standard model transfer to the robust model (and vice versa) would provide insights into the practical threat of black-box attacks in multi-model deployments.
    
    \item \textbf{Multiple Random Restarts:} Evaluating PGD with multiple random restarts would provide tighter robustness bounds and a more thorough security evaluation.
\end{itemize}

\subsection{Long-term Future Work}

Looking further out, these are some directions I think are worth pursuing:

\begin{itemize}
    \item \textbf{Large-Scale Evaluation:} Extending the framework to ImageNet-scale datasets with larger architectures would validate whether the observed trade-offs and relative attack strengths hold at scale.
    
    \item \textbf{Certified Defenses:} Investigating certified defense mechanisms such as randomized smoothing, which provide provable robustness guarantees within a specified perturbation budget, would complement the empirical defenses studied here.
    
    \item \textbf{Real-World Applications:} Studying adversarial robustness in specific domains like medical imaging (where misclassification has direct safety implications) or autonomous driving (where physical-world adversarial examples have already been demonstrated) would help bridge the gap between benchmark results and practical deployment.
    
    \item \textbf{Architectural Robustness:} Comparing the adversarial vulnerability and trainability of different architectures (CNNs, Vision Transformers, hybrid models) could reveal whether certain architectural choices provide inherent robustness advantages.
    
    \item \textbf{Efficient Adversarial Training:} Exploring methods such as Adversarial Training for Free~\cite{shafahi2019adversarial} and other efficient variants could address the significant computational overhead of PGD-based adversarial training observed in this study.
\end{itemize}

\section{Final Remarks}

Adversarial vulnerability is one of the biggest open problems for deploying deep learning in the real world. Through this thesis I have shown just how severe the problem is (a 94\% accurate model reduced to near-zero by invisible perturbations) and that adversarial training is a practical way to fight back, even if it comes at a cost.

As deep learning gets used more and more in autonomous vehicles, medical diagnosis, security systems, and other high-stakes applications, making sure these models can withstand adversarial attacks is going to become increasingly important. I hope that the implementations, results, and interactive demo from this research can serve as both a reference for others working in adversarial robustness and a concrete illustration of why this work matters.