\chapter{CONCLUSION AND FUTURE WORK}
\label{ch:conclusion}

\section{Conclusion}

This thesis set out to answer a simple question: how vulnerable are standard deep learning models to adversarial attacks, and can adversarial training do anything about it? To find out, I implemented three white-box attacks --- FGSM, PGD, and DeepFool --- tested them against a ResNet-18 trained on CIFAR-10, and then evaluated PGD-based adversarial training as a defense.

The answer, in short: standard models are extremely fragile, and adversarial training helps substantially but not for free. My ResNet-18 achieves 94.1\% accuracy on clean data. A 20-step PGD attack at $\epsilon = 8/255$ --- a perturbation invisible to the human eye --- drops that to 1.2\%. FGSM, being a weaker attack, only gets the model down to 18.3\%, which would give a misleadingly optimistic picture of the model's security if used as the sole evaluation. The gap between these two numbers (18.3\% vs 1.2\%) illustrates why multi-step attacks like PGD are necessary for honest robustness evaluation.

Adversarial training improves the picture considerably. The PGD-trained model achieves 45.1\% robust accuracy under PGD attack, up from near-zero, while giving up 10.2 percentage points of clean accuracy. For safety-critical applications, this seems like a reasonable trade-off.

One additional observation: the adversarially trained model produces noticeably better-calibrated confidence scores. The standard model outputs near-100\% confidence on its wrong adversarial predictions. The robust model spreads its probability mass more evenly, which means you can actually tell when it is uncertain. In practice, this calibration property could be quite valuable.

\section{Research Contributions}

Concretely, this thesis contributes the following:

\begin{enumerate}
    \item \textbf{Attack comparison:} A side-by-side evaluation of FGSM, PGD, and DeepFool on the same model with the same protocol. PGD was the strongest attack (1.2\% surviving accuracy at $\epsilon = 8/255$). FGSM was substantially weaker (18.3\%). DeepFool achieved a 91.1\% fooling rate with perturbations $5.4\times$ smaller in $L_2$ norm than FGSM/PGD.

    \item \textbf{Adversarial training evaluation:} The adversarially trained model achieved 45.1\% robust accuracy under PGD and 83.9\% clean accuracy --- numbers that match the benchmarks reported by Madry et al., confirming the correctness of my implementation.

    \item \textbf{Open-source framework:} The complete codebase is publicly available on GitHub: three attack implementations, two training pipelines, MLflow integration, and an interactive Streamlit demo on HuggingFace Spaces. The modular design means adding a new attack is a matter of writing one Python file.

    \item \textbf{Calibration observation:} The robust model not only gets adversarial examples right more often --- it also produces more honest confidence scores. The standard model says ``99\% cat'' when it is wrong; the robust model says ``55\% cat'' when it is right.
\end{enumerate}

\section{Limitations}

This work has clear limitations:

\begin{enumerate}
    \item \textbf{Only CIFAR-10.} Images are $32 \times 32$ with 10 classes. An $\epsilon = 8/255$ perturbation on a 32-pixel image is perceptually different from the same $\epsilon$ on a 224-pixel image. I cannot claim these results generalize to ImageNet-scale data without running those experiments.

    \item \textbf{Only ResNet-18.} Other architectures --- Vision Transformers, for instance --- may exhibit different vulnerability profiles. I did not test any alternatives.

    \item \textbf{Only white-box $L_\infty$.} No black-box attacks, no transfer attacks, no $L_2$ or $L_0$ threat models. I also did not implement Carlini-Wagner due to its computational requirements on my hardware.

    \item \textbf{Only one defense.} PGD adversarial training is one option among several. Certified defenses, randomized smoothing, and input preprocessing were not evaluated.

    \item \textbf{Hardware constraints.} All experiments ran on an Apple M4 Pro laptop. Adversarial training took 4.5 hours for 50 epochs, leaving little room for hyperparameter exploration.
\end{enumerate}

\section{Future Work}

Several extensions would strengthen and broaden these results.

\subsection{Short-term}

\begin{itemize}
    \item \textbf{Add Carlini-Wagner attack.} C\&W is widely considered the gold standard for evaluating defenses. Testing the adversarially trained model against it would provide a more complete robustness picture.

    \item \textbf{MNIST experiments.} Running the same evaluation pipeline on MNIST would enable comparison with a large body of existing results and show how dataset complexity affects vulnerability.

    \item \textbf{Transferability tests.} Do adversarial examples crafted for the standard model transfer to the robust model? This is relevant to black-box threat scenarios and I did not test it.

    \item \textbf{Multiple PGD restarts.} I used a single random restart for evaluation. Multiple restarts would give tighter robustness bounds.
\end{itemize}

\subsection{Long-term}

\begin{itemize}
    \item \textbf{ImageNet scale.} The key question is whether these findings hold on larger images, more classes, and bigger models.

    \item \textbf{Certified defenses.} Randomized smoothing provides formal robustness guarantees. Comparing it with the empirical approach taken here would be informative.

    \item \textbf{Domain-specific evaluation.} Medical imaging and autonomous driving are the application areas where adversarial robustness matters most. Experiments on X-ray images or traffic sign datasets would be more directly relevant than CIFAR-10.

    \item \textbf{Architecture comparison.} The question of whether Vision Transformers are more or less vulnerable than CNNs does not have a settled answer. My framework could be extended to investigate this.

    \item \textbf{Faster adversarial training.} 4.5 hours for 50 epochs on CIFAR-10 is already slow. Methods like Adversarial Training for Free~\cite{shafahi2019adversarial} could reduce the overhead significantly.
\end{itemize}

\section{Final Remarks}

Adversarial vulnerability remains one of the central unsolved problems in deploying deep learning to the real world. The experiments in this thesis make the severity of the problem concrete: a 94\% accurate model can be reduced to near-zero by perturbations that are invisible to the human eye. Adversarial training offers a practical defense, but it comes with a cost in both clean accuracy and computational budget.

As deep learning continues its expansion into autonomous vehicles, medical diagnostics, security systems, and other high-stakes domains, addressing adversarial robustness will only grow more urgent. I hope the implementations, experimental results, and interactive demo from this work can be useful --- both as a practical reference and as an illustration of why this line of research matters.
