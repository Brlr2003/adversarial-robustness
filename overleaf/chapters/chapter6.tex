\chapter{CONCLUSION AND FUTURE WORK}
\label{ch:conclusion}

\section{Conclusion}

The question I started with was straightforward: how vulnerable are deep neural networks to adversarial attacks, and does adversarial training actually help? I implemented three attacks --- FGSM, PGD, DeepFool --- tested them on a ResNet-18 trained on CIFAR-10, and then applied PGD-based adversarial training.

The short answer: standard models are extremely fragile, and adversarial training helps a lot but costs you something.

My ResNet-18 gets 94.1\% on clean images. PGD with 20 steps at $\epsilon = 8/255$ drops that to 1.2\%. You cannot see the perturbation. The model is basically useless. And if I had only tested with FGSM, I would have thought the model still had 18.3\% accuracy --- bad, but not catastrophic. PGD tells the real story: 1.2\%. That 16-point gap between FGSM and PGD is exactly why people insist on multi-step attacks for serious evaluation.

Adversarial training: the robust model gets 45.1\% under PGD. That is up from 1.2\%. The cost is 10.2 percentage points of clean accuracy (down from 94.1\% to 83.9\%). Whether that trade-off is worth it depends entirely on the application. For anything safety-related, I think it clearly is.

One thing I did not expect was the confidence calibration effect. The standard model does not just get adversarial examples wrong --- it gets them wrong with 99\% confidence. It has no idea it is being attacked. The adversarially trained model, by contrast, gives more reasonable confidence scores. It knows when something is off. That property alone might be worth the accuracy penalty in some applications.

\section{Research Contributions}

What this thesis contributes:

\begin{enumerate}
    \item \textbf{Attack comparison:} FGSM, PGD, and DeepFool evaluated side by side on one model, same evaluation setup. PGD is the strongest (1.2\% surviving accuracy), FGSM is weaker (18.3\%), and DeepFool fools 91.1\% of images with perturbations that are $5.4\times$ smaller in $L_2$.

    \item \textbf{Adversarial training numbers:} 45.1\% PGD robustness, 83.9\% clean accuracy. These match Madry et al.'s benchmarks.

    \item \textbf{Open-source code:} Everything is on GitHub --- attacks, training, evaluation, and a live demo on HuggingFace Spaces. Modular design. Adding a new attack means writing one file.

    \item \textbf{Confidence calibration:} The robust model not only gets adversarial examples right more often, it also produces more calibrated confidence scores. Standard model: ``99\% cat'' on a wrong answer. Robust model: ``55\% cat'' on a right answer. That second one is much more useful.
\end{enumerate}

\section{Limitations}

I want to be upfront about the limitations:

\begin{enumerate}
    \item \textbf{CIFAR-10 only.} $32 \times 32$ images, 10 classes. $\epsilon = 8/255$ on a 32-pixel image and on a 224-pixel image are not the same thing perceptually. I cannot say these results transfer to ImageNet without running those experiments.

    \item \textbf{ResNet-18 only.} Maybe Vision Transformers would be more robust, or less. I do not know. I did not test them.

    \item \textbf{White-box $L_\infty$ only.} No black-box, no transfer attacks, no $L_2$ or $L_0$ threat models. Carlini-Wagner was too expensive to run on my laptop.

    \item \textbf{One defense.} PGD adversarial training is not the only game in town. Certified defenses, randomized smoothing, input preprocessing --- all worth testing but I did not have time.

    \item \textbf{Hardware.} Apple M4 Pro. 4.5 hours for adversarial training on CIFAR-10. Not a lot of room for hyperparameter tuning.
\end{enumerate}

\section{Future Work}

Things I would do next, given more time:

\subsection{Short-term}

\begin{itemize}
    \item \textbf{Carlini-Wagner.} It is the standard for breaking defenses. I want to know if my robust model survives it.

    \item \textbf{MNIST experiments.} Would provide direct comparison with a lot of existing work. Also interesting to see whether a simpler dataset means more or less vulnerability.

    \item \textbf{Transferability.} If I craft adversarial examples against the standard model, do they also fool the robust one? Important for black-box scenarios.

    \item \textbf{More PGD restarts.} One restart gives a lower bound on vulnerability. Multiple restarts tighten that bound.
\end{itemize}

\subsection{Long-term}

\begin{itemize}
    \item \textbf{ImageNet.} Bigger images, more classes, bigger models. Does everything still hold?

    \item \textbf{Certified defenses.} Randomized smoothing gives formal guarantees. Would be a good complement to the empirical approach here.

    \item \textbf{Domain-specific testing.} CIFAR-10 cats and dogs are fine for benchmarking, but the real applications are medical imaging and autonomous driving. X-ray images, traffic signs --- that is where adversarial robustness actually matters.

    \item \textbf{Architectures.} Vision Transformers versus CNNs --- who is more robust? The literature does not have a clear answer. My framework could be extended to find out.

    \item \textbf{Faster adversarial training.} 4.5 hours on CIFAR-10 is already painful. Methods like Adversarial Training for Free~\cite{shafahi2019adversarialfree} claim to cut the cost significantly.
\end{itemize}

\section{Final Remarks}

Adversarial vulnerability is, as far as I can tell, one of the most important unsolved problems in deploying deep learning to the real world. What these experiments show is how bad things really are: a 94\%-accurate model reduced to 1.2\% by an invisible perturbation. But they also show that we are not helpless --- adversarial training genuinely works, even if it is not free.

Deep learning is increasingly used in places where mistakes have real consequences. Making these models robust to adversarial inputs is going to matter more and more. I hope the code, the results, and the demo from this thesis are useful to others working on this problem.
