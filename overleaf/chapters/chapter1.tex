\chapter{INTRODUCTION}
\label{ch:introduction}

\section{Introduction}

Deep neural networks achieve high accuracy on image classification, speech recognition, and natural language tasks. At the same time, these models are vulnerable to adversarial examples --- inputs modified by small, human-imperceptible perturbations that cause confident misclassification~\cite{szegedy2014intriguing,goodfellow2015explaining}. This vulnerability is not restricted to the digital domain. Kurakin et al.~\cite{kurakin2016adversarialphysical} demonstrated that adversarial perturbations survive printing and recapture by a camera, and similar attacks have been shown to work on physical road signs. Given the deployment of deep learning in autonomous driving, medical imaging, and security systems, understanding and addressing this vulnerability is a practical concern~\cite{yuan2019adversarial,ren2020adversarial,chakraborty2018survey,zhou2022adversarial,macas2024adversarial}.

\section{Background and Motivation}

Szegedy et al.~\cite{szegedy2014intriguing} were the first to show that well-trained neural networks can be fooled by perturbations that are nearly invisible to humans. Goodfellow et al.~\cite{goodfellow2015explaining} offered an explanation rooted in the linear behavior of neural networks in high-dimensional input spaces: a small perturbation aligned with the gradient of the loss, applied across many dimensions, accumulates into a large change in the network's output. They introduced FGSM, a single-step attack that produces adversarial examples with one gradient computation. The simplicity and effectiveness of FGSM made it clear that adversarial vulnerability is not an edge case but a systematic property of these models.

We chose this topic because we wanted to move beyond theoretical discussion and actually measure how fragile standard models are, what adversarial training costs, and how different attacks compare when evaluated under identical conditions. Implementing the attacks from scratch, rather than relying on existing libraries, was a deliberate choice --- it forced us to confront the implementation details that papers often leave out.

% TODO: Consider adding a figure showing adversarial example concept
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/adversarial_example.png}
%     \caption{Illustration of an adversarial example: a small perturbation causes misclassification.}
%     \label{fig:adversarial_example}
% \end{figure}

\section{Problem Statement}

We study convolutional neural networks trained on MNIST and CIFAR-10 for image classification. The thesis addresses three questions:

\begin{enumerate}
    \item How effective are common white-box attacks (FGSM, BIM/PGD, DeepFool) against standard models, and how does effectiveness scale with perturbation budget?
    \item Does PGD-based adversarial training yield meaningful robustness, and what is the cost in clean accuracy?
    \item How do the three attacks compare in terms of attack success rate, perturbation magnitude, and computational cost?
\end{enumerate}

We restrict our scope to evasion attacks at test time in the image classification setting.

\section{Research Objectives}

We observe that much of the existing work either remains theoretical or relies on adversarial ML libraries without investigating the underlying mechanics. To this end, we set the following objectives:

\begin{itemize}
    \item Implement FGSM, BIM/PGD, and DeepFool from scratch in PyTorch and evaluate them on MNIST and CIFAR-10.
    \item Evaluate at multiple perturbation budgets to characterize how accuracy degrades as $\epsilon$ increases.
    \item Apply PGD-based adversarial training and measure the resulting trade-off between robustness and clean accuracy.
    \item Compare all three attacks under identical conditions --- same model, same data, same evaluation protocol.
\end{itemize}

\section{Contributions}

The contributions of this thesis are as follows:

\begin{enumerate}
    \item We provide a controlled comparison of three adversarial attacks on the same model with the same evaluation setup. This addresses the difficulty of comparing results across papers that use different configurations.
    \item We report the accuracy--robustness trade-off from PGD adversarial training with sufficient detail for reproducibility.
    \item We release an open-source framework containing all attack implementations, training scripts, evaluation code, and an interactive web demo.
\end{enumerate}

\section{Thesis Organization}

\textbf{Chapter 2} reviews the literature on adversarial examples, attacks, and defenses.

\textbf{Chapter 3} presents the methodology: datasets, models, attack formulations, defense setup, and evaluation metrics.

\textbf{Chapter 4} describes the implementation, including code architecture, engineering decisions, and development tools.

\textbf{Chapter 5} reports the experimental results and analysis.

\textbf{Chapter 6} gives conclusions and directions for future work.

