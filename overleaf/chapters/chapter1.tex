\chapter{INTRODUCTION}
\label{ch:introduction}

\section{Introduction}

Over the past decade, deep neural networks (DNNs) have become the go-to approach for tasks ranging from computer vision to speech recognition. They work remarkably well in practice, but they come with a surprising flaw. If someone adds a tiny amount of carefully chosen noise to an image, noise so small that a person would never notice it, the network can suddenly produce a completely wrong prediction, and it will do so with high confidence~\cite{szegedy2014intriguing,goodfellow2015explaining}. These manipulated inputs are called adversarial examples.

What makes this problem particularly worrying is that it is not limited to laboratory settings. Researchers have shown that adversarial examples can fool models in the physical world too, for instance through printed images held up to a camera or subtly altered road signs that confuse self-driving car systems~\cite{kurakin2016adversarialphysical}. When you consider that deep learning is being used more and more in areas where mistakes can be dangerous (think autonomous driving, medical imaging, or security screening), the stakes become clear. This has pushed a lot of researchers to look into both how these attacks work and how we might defend against them~\cite{yuan2019adversarial,ren2020adversarial,chakraborty2018survey,zhou2022adversarial,macas2024adversarial}.

\section{Background and Motivation}

The story of adversarial examples really began with the work of Szegedy et al.~\cite{szegedy2014intriguing}. They were the first to show, in a systematic way, that even well-trained neural networks could be tricked by perturbations that are barely visible. Shortly after, Goodfellow et al.~\cite{goodfellow2015explaining} offered an explanation for why this happens. Their argument was that deep networks behave too linearly in high-dimensional spaces, which makes them easy to exploit. They backed this up by introducing the Fast Gradient Sign Method (FGSM), which generates adversarial examples using just one gradient computation.

My motivation for this research comes from a straightforward observation: if we are going to trust deep learning models in applications like autonomous vehicles, medical diagnosis, or security systems, we need to understand how fragile they really are. Studying how adversarial attacks work is, in my view, the necessary first step before we can build models that are genuinely robust.

% TODO: Consider adding a figure showing adversarial example concept
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/adversarial_example.png}
%     \caption{Illustration of an adversarial example: a small perturbation causes misclassification.}
%     \label{fig:adversarial_example}
% \end{figure}

\section{Problem Statement}

I am working with standard image classification models, specifically convolutional neural networks trained on MNIST and CIFAR-10, and this thesis tries to answer three questions:

\begin{enumerate}
    \item How vulnerable are these models to common white-box adversarial attacks (FGSM, BIM/PGD, DeepFool) when we vary the perturbation norms and budgets?
    \item How well does adversarial training (particularly PGD-based) improve robustness, and how much clean accuracy do we lose in the process?
    \item How do the different attacks compare when we look at success rate, perturbation size, and computational cost?
\end{enumerate}

I focus specifically on evasion attacks at test time and use image classification as a concrete setting for studying these questions about neural network robustness.

\section{Research Objectives}

A lot of the existing work on adversarial attacks stays at the theoretical level, so I wanted to get my hands dirty with actual implementations and see the results for myself. My objectives were:

\begin{itemize}
    \item Build working implementations of FGSM, BIM/PGD, and DeepFool from scratch and test them on MNIST and CIFAR-10.
    \item Run these attacks with different perturbation budgets and see exactly how much damage they do to a convolutional neural network.
    \item Train a model with adversarial training and measure whether the robustness gains are worth the accuracy cost.
    \item Put all the numbers side by side so I can see the clean accuracy vs. robust accuracy trade-off clearly.
\end{itemize}

\section{Contributions}

The main contributions of this thesis are:

\begin{enumerate}
    \item An empirical comparison of three classic adversarial attacks on a standard deep learning model, showing how they differ in effectiveness and computational cost.
    \item A practical demonstration of how adversarial training changes a model's robustness profile, including the trade-offs involved.
    \item A structured and reproducible experimental framework that ties together the implementation with the theoretical background covered in the literature.
\end{enumerate}

\section{Thesis Organization}

\textbf{Chapter 2} is a literature review where I go through the theory behind adversarial examples, the main attack algorithms people have come up with, and the defenses that exist.

\textbf{Chapter 3} covers my methodology: the math behind each attack, which datasets and models I used, and how I set up the experiments.

\textbf{Chapter 4} is about the implementation itself. I explain the code structure, the engineering decisions I made, and what tools I used.

\textbf{Chapter 5} has all the experimental results. I report attack success rates, perturbation sizes, and the adversarial training numbers.

\textbf{Chapter 6} is where I discuss what it all means, what the limitations are, and what I would do differently or explore next.
