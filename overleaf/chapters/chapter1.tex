\chapter{INTRODUCTION}
\label{ch:introduction}

\section{Introduction}

Neural networks have gotten very good at a lot of things. Image classification, speech, translation --- the list keeps growing, and the benchmarks keep improving. But there is something strange going on underneath all of that performance. If you take an image that a network classifies correctly and add a tiny bit of noise to it --- noise that is structured in a particular way, but so small you literally cannot see it --- the network will suddenly give a completely wrong answer. And not just wrong: wrong with very high confidence~\cite{szegedy2014intriguing,goodfellow2015explaining}. These perturbed inputs are called adversarial examples.

This is not just a lab curiosity. Kurakin et al.~\cite{kurakin2016adversarialphysical} showed that you can print adversarial images on paper, hold them up to a camera, and the network still gets fooled. People have done similar things with road signs. When you think about the fact that deep learning is being deployed in self-driving cars, in medical imaging systems, in security checkpoints --- well, it starts to feel like a real problem, not just a theoretical one. And indeed a lot of research effort has gone into both understanding why this happens and figuring out what to do about it~\cite{yuan2019adversarial,ren2020adversarial,chakraborty2018survey,zhou2022adversarial,macas2024adversarial}.

\section{Background and Motivation}

The discovery of adversarial examples goes back to Szegedy et al.~\cite{szegedy2014intriguing}. Their paper was one of the first to systematically show that well-trained neural networks can be tricked by perturbations so small they are basically invisible. It was a surprising result at the time. Goodfellow et al.~\cite{goodfellow2015explaining} followed up with an explanation that I find quite intuitive: deep networks are too linear in high-dimensional spaces. Small perturbations, when aligned with the gradient, add up across thousands of dimensions and produce large changes in the output. They also introduced FGSM (Fast Gradient Sign Method), which can craft an adversarial example with a single gradient computation.

So why did I pick this topic? Honestly, it was a practical concern. If deep learning is going to be used for things that actually matter --- medical diagnoses, autonomous vehicles, security --- then somebody needs to sit down and carefully measure just how fragile these systems are. Not in the abstract. With actual models, actual attacks, actual numbers. That is what I wanted to do with this thesis.

% TODO: Consider adding a figure showing adversarial example concept
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/adversarial_example.png}
%     \caption{Illustration of an adversarial example: a small perturbation causes misclassification.}
%     \label{fig:adversarial_example}
% \end{figure}

\section{Problem Statement}

I work with convolutional neural networks trained on MNIST and CIFAR-10 --- nothing fancy, just standard image classification setups. The thesis tries to answer three questions:

\begin{enumerate}
    \item How badly do common white-box attacks (FGSM, BIM/PGD, DeepFool) break these models, and what happens as you increase the perturbation budget?
    \item Can adversarial training (PGD-based) actually fix this? And if so, how much clean accuracy do you give up?
    \item When you put all three attacks side by side, how do they compare? Success rate, perturbation size, speed --- the full picture.
\end{enumerate}

I am only looking at evasion attacks at test time. Image classification is the testbed.

\section{Research Objectives}

A lot of papers in this area stay fairly theoretical, or they use off-the-shelf attack libraries and just report numbers. I wanted to go deeper than that. Build everything from scratch, understand the implementation details, see what actually happens when you run these attacks. My specific objectives:

\begin{itemize}
    \item Write my own implementations of FGSM, BIM/PGD, and DeepFool and test them on both MNIST and CIFAR-10.
    \item Vary the perturbation budget systematically and measure how accuracy degrades --- I wanted to see the full curve, not just one data point.
    \item Train a model with PGD-based adversarial training and measure both what you gain (robustness) and what you lose (clean accuracy).
    \item Lay everything out in tables so you can see the trade-offs at a glance.
\end{itemize}

\section{Contributions}

The contributions of this thesis:

\begin{enumerate}
    \item A head-to-head comparison of three classic adversarial attacks on one model, with consistent evaluation across all of them. The comparison shows pretty clearly how they differ in effectiveness and cost.
    \item A detailed look at what adversarial training does to a model's robustness profile, including the trade-offs that come with it.
    \item An experimental framework --- code, scripts, configs --- that ties the theory to actual reproducible results.
\end{enumerate}

\section{Thesis Organization}

\textbf{Chapter 2} covers the literature: why adversarial examples exist, what attacks people have developed, what defenses are out there.

\textbf{Chapter 3} describes the methodology. The math behind each attack, the datasets and models I used, how I set up the experiments.

\textbf{Chapter 4} goes into the implementation. Code structure, engineering decisions, tools.

\textbf{Chapter 5} has all the results. Attack evaluations, adversarial training numbers, comparisons.

\textbf{Chapter 6} wraps up with conclusions, limitations, and ideas for future work.
