\chapter{INTRODUCTION}
\label{ch:introduction}

\section{Introduction}

Deep neural networks have, over the last decade or so, become the default tool for a wide range of tasks --- image classification, speech recognition, natural language processing, and so on. Their performance on benchmarks is often remarkable. But there is a catch that has troubled the research community since at least 2014: these networks can be fooled by tiny, almost invisible perturbations to their inputs~\cite{szegedy2014intriguing,goodfellow2015explaining}. Add a small amount of carefully constructed noise to an image --- noise that no human would ever notice --- and the network confidently produces the wrong output. These are adversarial examples, and their existence raises serious questions about deploying neural networks in any setting where reliability matters.

The problem is not purely theoretical. Kurakin et al.~\cite{kurakin2016adversarialphysical} demonstrated that adversarial perturbations survive in the physical world: printed images, modified road signs, and so on. This is concerning. Autonomous vehicles, medical imaging pipelines, security screening systems --- all of these rely increasingly on deep learning, and all of them operate in environments where an adversary could, at least in principle, exploit this vulnerability. A substantial body of work has emerged around understanding and mitigating the problem~\cite{yuan2019adversarial,ren2020adversarial,chakraborty2018survey,zhou2022adversarial,macas2024adversarial}.

\section{Background and Motivation}

Szegedy et al.~\cite{szegedy2014intriguing} were the first to document adversarial examples systematically. Their paper showed that imperceptible perturbations could cause misclassification in state-of-the-art networks, which was unexpected at the time. Goodfellow et al.~\cite{goodfellow2015explaining} then proposed an explanation: the high-dimensional linear behavior of these networks makes them inherently susceptible to such attacks. To demonstrate this, they introduced the Fast Gradient Sign Method (FGSM) --- a single gradient step that generates adversarial examples cheaply and effectively.

What motivated me to pursue this particular topic is fairly simple. If we want to use deep learning for anything consequential --- say, diagnosing disease from medical scans, or making decisions in a self-driving car --- then we had better understand how easy it is to break these models. And not just in theory. I wanted to actually implement the attacks, run them, see the numbers, and get a feel for how bad things really are. That seemed like a necessary starting point before anyone can seriously talk about building robust systems.

% TODO: Consider adding a figure showing adversarial example concept
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/adversarial_example.png}
%     \caption{Illustration of an adversarial example: a small perturbation causes misclassification.}
%     \label{fig:adversarial_example}
% \end{figure}

\section{Problem Statement}

The setting is standard image classification with convolutional neural networks, trained on MNIST and CIFAR-10. Within this setting, this thesis addresses three questions:

\begin{enumerate}
    \item How vulnerable are these models to common white-box attacks --- FGSM, BIM/PGD, and DeepFool --- across a range of perturbation budgets?
    \item Does adversarial training (specifically PGD-based) provide meaningful robustness, and what does it cost in terms of clean accuracy?
    \item How do the three attacks compare in terms of success rate, perturbation magnitude, and computational overhead?
\end{enumerate}

The scope is limited to evasion attacks at test time. Image classification serves as the concrete domain for investigating these questions.

\section{Research Objectives}

Much of the published work on adversarial attacks is theoretical or uses existing toolkits. I wanted to do something more hands-on --- build the attacks from scratch, run them myself on real models, and understand the practical details that papers tend to gloss over. Concretely, my objectives were:

\begin{itemize}
    \item Implement FGSM, BIM/PGD, and DeepFool from the ground up and evaluate them on MNIST and CIFAR-10.
    \item Sweep over different perturbation budgets to characterize how model accuracy degrades as a function of attack strength.
    \item Apply PGD-based adversarial training and quantify the resulting trade-off between clean and robust accuracy.
    \item Present all numbers in a unified comparison so the relative strengths of each attack are visible at a glance.
\end{itemize}

\section{Contributions}

This thesis makes the following contributions:

\begin{enumerate}
    \item A side-by-side empirical comparison of three adversarial attacks (FGSM, PGD, DeepFool) on a single architecture, using consistent evaluation methodology. The comparison reveals substantial differences in attack effectiveness and computational cost.
    \item A practical demonstration of PGD-based adversarial training, with detailed measurements of the accuracy--robustness trade-off.
    \item A reproducible experimental framework --- code, training pipelines, evaluation scripts --- that connects the theoretical background to concrete results.
\end{enumerate}

\section{Thesis Organization}

\textbf{Chapter 2} reviews the relevant literature: the theory of adversarial examples, the main attack algorithms, and existing defense strategies.

\textbf{Chapter 3} lays out the methodology --- the mathematical formulations, datasets, model architectures, and experimental design.

\textbf{Chapter 4} describes the implementation in detail: code structure, engineering decisions, and the tools I used.

\textbf{Chapter 5} presents all experimental results: attack evaluations, adversarial training outcomes, and comparative analysis.

\textbf{Chapter 6} discusses conclusions, limitations, and potential directions for future work.
