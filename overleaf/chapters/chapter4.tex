\chapter{IMPLEMENTATION}
\label{ch:implementation}

\section{Introduction}

This chapter is about the code --- the actual engineering that made the experiments possible. Everything is written in PyTorch. I coded FGSM, PGD, and DeepFool from scratch (no adversarial ML libraries), wrote two training loops (standard and adversarial), and put together a Streamlit web app on HuggingFace Spaces where people can try the attacks themselves.

Going from the methodology chapter to working software was not as smooth as I would have liked. Memory issues with PGD, numerical problems with DeepFool, a confusing batch normalization bug during adversarial training --- I will get into all of these. MLflow handled experiment tracking throughout.

\section{System Architecture}

Five modules. The \textbf{Model Module} has the CIFAR-10 ResNet-18. The \textbf{Attack Module} has all three attacks, written with the same function signature so they are interchangeable. The \textbf{Training Module} runs standard and adversarial training with MLflow logging. The \textbf{Evaluation Module} computes robustness metrics. And the \textbf{Frontend Module} is the Streamlit demo.

I wanted the code to be modular enough that adding a fourth attack later would just mean writing one file. No changes to anything else.

\section{Implementation Details}

\subsection{Model Implementation}

I used ResNet-18~\cite{he2016deep}, but it needed modification. The original ResNet-18 was built for ImageNet --- $224 \times 224$ images. CIFAR-10 has $32 \times 32$. The problem is the first layer: a $7 \times 7$ conv with stride 2 plus max pooling. On a $32 \times 32$ image, that shrinks things way too fast. By the time you reach the residual blocks, the feature maps are tiny and you have thrown away most of the spatial information. So I replaced the first layer with a $3 \times 3$ conv, stride 1, padding 1, and dropped the max pooling. This is a fairly standard modification for CIFAR-scale experiments.

After that, the architecture is normal ResNet-18. Four residual layers, two BasicBlocks each. Each BasicBlock has two $3 \times 3$ convolutions with batch norm and ReLU, plus a shortcut connection that adds the input to the output (the ``residual'' in residual network). Layers 2 through 4 downsample --- I use a $1 \times 1$ conv on the shortcut to match dimensions when that happens. Channels go 64, 128, 256, 512. Adaptive average pooling at the end reduces everything to $1 \times 1$, then a linear layer gives 10 logits. About 11.17 million parameters total.

Initialization: Kaiming normal for convolutions, constant init for batch norm. Standard stuff.

One thing I want to flag: I intentionally did not normalize the inputs (no mean subtraction, no std division). Why? Because adversarial perturbation budgets are defined in pixel space. When people say $\epsilon = 8/255$, they mean 8 intensity levels out of 255. If you normalize, then $\epsilon = 8/255$ no longer has that direct meaning --- you would need to rescale it. Keeping inputs in $[0, 1]$ keeps things simple and interpretable. This is the convention used in most of the adversarial robustness literature, including Madry et al.~\cite{madry2018towards}.

\subsection{Data Loading and Preprocessing}

CIFAR-10, loaded through torchvision. Training augmentation: random crop with 4 pixels padding, random horizontal flip, then ToTensor. Test: just ToTensor. Batch size 128. Four DataLoader workers, pinned memory for GPU transfers. Nothing unusual here.

\subsection{FGSM Attack Implementation}

The FGSM code follows Goodfellow et al.~\cite{goodfellow2015explaining}. Algorithm~\ref{alg:fgsm} shows it.

\begin{algorithm}[htbp]
\caption{Fast Gradient Sign Method (FGSM)}
\label{alg:fgsm}
\begin{algorithmic}[1]
\REQUIRE Input image $x$, true label $y$, model $f_\theta$, perturbation budget $\epsilon$
\ENSURE Adversarial example $x^{adv}$
\STATE Compute loss: $L = J(\theta, x, y)$
\STATE Compute gradient: $g = \nabla_x L$
\STATE Compute perturbation: $\delta = \epsilon \cdot \text{sign}(g)$
\STATE Generate adversarial example: $x^{adv} = \text{clip}(x + \delta, 0, 1)$
\RETURN $x^{adv}$
\end{algorithmic}
\end{algorithm}

In PyTorch terms: set \texttt{requires\_grad = True} on the input, do a forward pass, compute cross-entropy, call \texttt{backward()}, take the sign of the input gradient and scale by $\epsilon$. Clamp to $[0, 1]$. Runs on full batches, very fast.

\subsection{PGD Attack Implementation}

PGD is just FGSM iterated, with a projection step and random initialization. Algorithm~\ref{alg:pgd} shows the full procedure.

\begin{algorithm}[htbp]
\caption{Projected Gradient Descent (PGD) Attack}
\label{alg:pgd}
\begin{algorithmic}[1]
\REQUIRE Input $x$, label $y$, model $f_\theta$, budget $\epsilon$, step size $\alpha$, iterations $T$
\ENSURE Adversarial example $x^{adv}$
\STATE Initialize: $x^{adv}_0 = x + \text{Uniform}(-\epsilon, \epsilon)$ \COMMENT{Random start}
\FOR{$t = 0$ to $T-1$}
    \STATE Compute loss: $L = J(\theta, x^{adv}_t, y)$
    \STATE Compute gradient: $g = \nabla_x L$
    \STATE Update: $x^{adv}_{t+1} = x^{adv}_t + \alpha \cdot \text{sign}(g)$
    \STATE Project: $x^{adv}_{t+1} = \Pi_{x,\epsilon}(x^{adv}_{t+1})$ \COMMENT{Project to $\epsilon$-ball}
    \STATE Clip: $x^{adv}_{t+1} = \text{clip}(x^{adv}_{t+1}, 0, 1)$
\ENDFOR
\RETURN $x^{adv}_T$
\end{algorithmic}
\end{algorithm}

Random start inside the $\epsilon$-ball. Then iterate: gradient, step, project, clamp. The projection clamps $x^{adv} - x$ to $[-\epsilon, \epsilon]$ element-wise (that enforces $L_\infty$), and the final clamp keeps pixel values in $[0, 1]$.

A lesson I learned the hard way: you have to call \texttt{.detach()} on the adversarial image after each iteration. If you don't, PyTorch holds onto the entire computation graph from all iterations. Twenty iterations of PGD on a ResNet-18 and you are out of memory. I spent a while debugging this before I figured out what was happening.

\subsection{DeepFool Implementation}

DeepFool~\cite{moosavi2016deepfool} is the weird one. No epsilon budget. Instead it finds the \emph{minimum} perturbation that changes the prediction. It does this by linearizing the classifier at each step and finding the nearest decision boundary.

The catch: it has to process images individually. No batching. For each image, at each iteration, it computes the gradient of every class logit with respect to the input. Then for each class $k$ that is not the current predicted class $\hat{k}$, it computes:

\begin{equation}
    d_k = \frac{|f_k(x) - f_{\hat{k}}(x)|}{\|w_k\|_2}
\end{equation}

where $w_k = \nabla_x f_k(x) - \nabla_x f_{\hat{k}}(x)$. Pick the class with the smallest $d_k$, compute the minimal perturbation to cross that boundary, apply it, and repeat. Stop when the prediction changes or you hit the iteration limit (50 in my case). I also scale the final perturbation by $1.02$ (the ``overshoot'') to make sure the input actually crosses the boundary and doesn't just sit on it.

The per-image processing makes it slow --- much slower than FGSM or PGD. But you get something valuable: the actual distance from each input to the nearest decision boundary. That is the robustness radius.

\subsection{Adversarial Training Implementation}

Adversarial training follows Madry et al.~\cite{madry2018towards}. Conceptually simple: at each training step, generate PGD adversarial examples from the current batch, then train on those instead of the clean inputs.

In practice, there is a gotcha with batch normalization that took me a while to figure out. When you generate the adversarial examples, you run a forward pass through the model. If the model is in training mode during that forward pass, the adversarial inputs pollute the batch norm running statistics. The running mean and variance get corrupted. And then everything goes wrong --- the model never converges properly and you get bad robustness numbers.

The fix: \texttt{model.eval()} before generating the PGD attack, \texttt{model.train()} afterwards for the actual weight update. It seems obvious in retrospect, but it was not obvious at the time. The training just silently produced bad results and I had to figure out why.

Optimizer: SGD, momentum 0.9, weight decay $5 \times 10^{-4}$, learning rate starting at 0.1 with cosine annealing. Checkpoints saved based on PGD robust accuracy, not clean accuracy --- the whole point of adversarial training is robustness, so that is what the checkpoint selection should optimize for. Everything logged to MLflow.

\section{Development Environment}

\begin{itemize}
    \item \textbf{Operating System:} macOS (Apple Silicon)
    \item \textbf{Python Version:} 3.13
    \item \textbf{PyTorch Version:} 2.x with MPS (Metal Performance Shaders) backend
    \item \textbf{Hardware:} Apple M4 Pro, 16GB unified memory
    \item \textbf{Experiment Tracking:} MLflow
    \item \textbf{Deployment:} Docker, HuggingFace Spaces
\end{itemize}

Training used the MPS backend --- Apple's GPU acceleration for Apple Silicon. Standard training: 50 epochs, about 55 minutes. Adversarial training: also 50 epochs, but about 4.5 hours. The slowdown is roughly $7\times$, which makes sense: every training step now includes a 7-step PGD attack on top of the normal forward/backward pass.

\section{Challenges and Solutions}

I ran into several issues during development. Here they are, along with what I did:

\textbf{DeepFool gradients and numerical stability.} DeepFool needs gradients for all 10 classes in a loop. In PyTorch, after the first \texttt{backward()} call, the computation graph gets freed unless you pass \texttt{retain\_graph=True}. That was the first problem. The second: sometimes the gradient differences between classes are essentially zero, and the norm computation gives you NaN or division by zero. Adding $10^{-8}$ to the denominators solved it, but I only figured this out after seeing NaN losses during evaluation and tracing it back.

\textbf{PGD memory leak.} Already described above. Without \texttt{.detach()}, the computation graph accumulates across iterations and memory explodes. With 20 PGD steps, I was getting out-of-memory errors on 16GB of RAM. The fix was a one-liner, but finding the bug took longer than I'd like to admit.

\textbf{Batch norm during adversarial training.} Also described above. Switching to eval mode for attack generation was essential. Without it, training ``works'' in the sense that it runs without errors, but the resulting model is noticeably worse.

\textbf{MPS backend.} Apple's MPS support in PyTorch is not fully mature yet. I hit a couple of operations that weren't supported on MPS, and in some cases MPS and CPU gave slightly different numerical results. For anything that seemed off, I fell back to CPU and compared. Nothing major, but it added debugging time.

\section{Code Organization}

The codebase, organized by function:

\begin{verbatim}
adversarial-robustness/
+-- src/
|   +-- models/
|   |   +-- resnet.py          # ResNet-18 for CIFAR-10
|   +-- attacks/
|   |   +-- fgsm.py            # FGSM attack
|   |   +-- pgd.py             # PGD attack
|   |   +-- deepfool.py        # DeepFool attack
|   +-- training/
|   |   +-- standard.py        # Standard training loop
|   |   +-- adversarial.py     # PGD adversarial training
|   +-- utils/
|   |   +-- data.py            # Data loading and transforms
|   |   +-- metrics.py         # Evaluation metrics
|   +-- api/
|       +-- main.py            # FastAPI backend
+-- frontend/
|   +-- app.py                 # Streamlit demo interface
+-- scripts/
|   +-- train.py               # Training entry point
|   +-- evaluate.py            # Evaluation entry point
+-- configs/
|   +-- config.yaml            # Hyperparameters
+-- checkpoints/               # Saved model weights
+-- tests/
|   +-- test_attacks.py        # Unit tests
+-- Dockerfile                 # Container for deployment
\end{verbatim}

You run things through \texttt{scripts/train.py} and \texttt{scripts/\allowbreak evaluate.py}. Command-line arguments control whether you do standard or adversarial training, and what attack parameters to use for evaluation. Hyperparameters live in \texttt{configs/config.yaml}.

\section{Deployment}

I wanted a way for people to actually see what adversarial attacks look like, not just read about them in tables. So I built a Streamlit web app. You pick a CIFAR-10 image (or upload your own), choose an attack and an epsilon, and it shows you the original image, the perturbation (amplified so you can actually see it), and the adversarial image. It also shows the confidence scores from both models side by side. Containerized with Docker, deployed on HuggingFace Spaces.

\section{Summary}

That covers the implementation. Three attacks from scratch, two training pipelines, a web demo. The main engineering challenges were memory management in PGD, numerical stability in DeepFool, and the batch norm issue in adversarial training. Next chapter has the results.
