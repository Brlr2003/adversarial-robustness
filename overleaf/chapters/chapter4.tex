\chapter{IMPLEMENTATION}
\label{ch:implementation}

\section{Introduction}

This chapter is about the code. Everything was written in PyTorch --- the model, the three attacks, both training pipelines, and a web demo for interactive visualization. FGSM, PGD, and DeepFool were all implemented from scratch rather than using an existing adversarial ML library. I also built a Streamlit application, deployed on HuggingFace Spaces, that lets users run attacks in real time and see the results.

Getting from the methodology in the previous chapter to working software involved dealing with a number of practical issues: memory management in iterative attacks, numerical stability in DeepFool, batch normalization behavior during adversarial training, and quirks of Apple's MPS backend. MLflow was used throughout for experiment tracking.

\section{System Architecture}

The codebase is organized into five modules. The \textbf{Model Module} contains the CIFAR-10-adapted ResNet-18. The \textbf{Attack Module} provides FGSM, PGD, and DeepFool, each with the same function signature to make them interchangeable. The \textbf{Training Module} handles both standard and adversarial training loops with MLflow logging. The \textbf{Evaluation Module} computes all robustness metrics. The \textbf{Frontend Module} is the Streamlit demo.

I deliberately kept things modular. Adding a new attack means writing a single Python file that conforms to the existing interface --- nothing else needs to change.

\section{Implementation Details}

\subsection{Model Implementation}

The backbone is a ResNet-18~\cite{he2016deep}. The original architecture assumes $224 \times 224$ inputs (ImageNet), and CIFAR-10 images are only $32 \times 32$. The standard first layer --- a $7 \times 7$ convolution with stride 2, followed by max pooling --- would shrink the spatial dimensions too aggressively. By the time the input reaches the residual blocks, most spatial information would already be gone. So I replaced the first layer with a $3 \times 3$ convolution, stride 1, padding 1, and removed the max pooling entirely. This is a well-known modification for CIFAR-scale inputs.

The rest of the architecture follows the standard ResNet-18 design: four residual layers, each containing two BasicBlock modules. A BasicBlock consists of two $3 \times 3$ convolutions with batch normalization and ReLU, plus a skip connection. When downsampling occurs (layers 2 through 4), a $1 \times 1$ convolution on the shortcut path matches the spatial and channel dimensions. Channel counts progress as 64, 128, 256, 512 across the four layers. At the end, adaptive average pooling reduces the spatial dimensions to $1 \times 1$, and a linear layer maps to 10 class logits. The total parameter count is approximately 11.17 million.

Weight initialization: Kaiming normal for convolutional layers, constant initialization for batch normalization parameters. This is standard for ResNets.

One deliberate design choice: I did \emph{not} apply per-channel input normalization (mean subtraction and standard deviation scaling). Adversarial perturbation budgets ($\epsilon$) are defined in pixel space. If you normalize the inputs, then $\epsilon = 8/255$ no longer corresponds to 8 intensity levels per pixel --- you would need to rescale the budget to account for the normalization, which complicates things for no real gain. By keeping inputs in $[0, 1]$, the correspondence between $\epsilon$ and actual pixel changes is direct and transparent. This follows the convention used in the adversarial robustness literature, including Madry et al.~\cite{madry2018towards}.

\subsection{Data Loading and Preprocessing}

CIFAR-10 was loaded via torchvision. Training data used standard augmentation: random crops with 4 pixels of padding, random horizontal flips, and conversion to tensors. Test data was only converted to tensors --- no augmentation. Batch size was 128 throughout, with 4 DataLoader workers and pinned memory enabled for faster GPU transfers.

\subsection{FGSM Attack Implementation}

The FGSM implementation follows Goodfellow et al.~\cite{goodfellow2015explaining} directly. Algorithm~\ref{alg:fgsm} shows the procedure.

\begin{algorithm}[htbp]
\caption{Fast Gradient Sign Method (FGSM)}
\label{alg:fgsm}
\begin{algorithmic}[1]
\REQUIRE Input image $x$, true label $y$, model $f_\theta$, perturbation budget $\epsilon$
\ENSURE Adversarial example $x^{adv}$
\STATE Compute loss: $L = J(\theta, x, y)$
\STATE Compute gradient: $g = \nabla_x L$
\STATE Compute perturbation: $\delta = \epsilon \cdot \text{sign}(g)$
\STATE Generate adversarial example: $x^{adv} = \text{clip}(x + \delta, 0, 1)$
\RETURN $x^{adv}$
\end{algorithmic}
\end{algorithm}

In PyTorch, this means setting \texttt{requires\_grad = True} on the input tensor, computing the cross-entropy loss, calling \texttt{backward()}, and then taking the sign of the resulting gradient. The adversarial image is clamped to $[0,1]$ to remain a valid image. The whole thing operates on full batches.

\subsection{PGD Attack Implementation}

PGD extends FGSM by iterating the gradient step and projecting back onto the constraint set after each update. The full procedure is in Algorithm~\ref{alg:pgd}.

\begin{algorithm}[htbp]
\caption{Projected Gradient Descent (PGD) Attack}
\label{alg:pgd}
\begin{algorithmic}[1]
\REQUIRE Input $x$, label $y$, model $f_\theta$, budget $\epsilon$, step size $\alpha$, iterations $T$
\ENSURE Adversarial example $x^{adv}$
\STATE Initialize: $x^{adv}_0 = x + \text{Uniform}(-\epsilon, \epsilon)$ \COMMENT{Random start}
\FOR{$t = 0$ to $T-1$}
    \STATE Compute loss: $L = J(\theta, x^{adv}_t, y)$
    \STATE Compute gradient: $g = \nabla_x L$
    \STATE Update: $x^{adv}_{t+1} = x^{adv}_t + \alpha \cdot \text{sign}(g)$
    \STATE Project: $x^{adv}_{t+1} = \Pi_{x,\epsilon}(x^{adv}_{t+1})$ \COMMENT{Project to $\epsilon$-ball}
    \STATE Clip: $x^{adv}_{t+1} = \text{clip}(x^{adv}_{t+1}, 0, 1)$
\ENDFOR
\RETURN $x^{adv}_T$
\end{algorithmic}
\end{algorithm}

Random initialization within the $\epsilon$-ball helps explore different adversarial directions and avoids getting stuck. The projection step clamps the perturbation $x^{adv} - x$ to $[-\epsilon, \epsilon]$ element-wise, enforcing the $L_\infty$ constraint. A final clamp to $[0,1]$ ensures pixel validity.

One thing that tripped me up initially: you \emph{must} detach the adversarial tensor from the computation graph after each iteration. Without detaching, PyTorch accumulates the entire computational graph across all $T$ steps, and memory usage grows quickly. With 20 iterations, this caused out-of-memory errors until I added explicit \texttt{.detach()} calls after each projection step.

\subsection{DeepFool Implementation}

DeepFool~\cite{moosavi2016deepfool} is fundamentally different from the other two attacks. It does not work with a fixed perturbation budget. Instead, it finds the smallest perturbation that changes the model's prediction by iteratively estimating the nearest decision boundary.

Since each image has its own nearest decision boundary, DeepFool processes images one at a time --- no batching. At each iteration, the algorithm computes the gradient of every class logit with respect to the input. For each class $k \neq \hat{k}$ (where $\hat{k}$ is the current predicted class), it estimates the distance to the corresponding decision boundary via a linear approximation:

\begin{equation}
    d_k = \frac{|f_k(x) - f_{\hat{k}}(x)|}{\|w_k\|_2}
\end{equation}

where $w_k = \nabla_x f_k(x) - \nabla_x f_{\hat{k}}(x)$. The algorithm then takes the smallest $d_k$, computes the corresponding minimum perturbation, and applies it. The process repeats until either the prediction changes or the iteration limit is reached. I apply an overshoot factor of 0.02 --- scaling the total perturbation by $1.02$ --- to ensure the perturbed input actually crosses the decision boundary rather than landing exactly on it.

Per-image processing makes DeepFool substantially slower than the batched attacks. But the payoff is that it produces near-minimal perturbations, which directly measure each sample's \textit{robustness radius} --- how far it sits from the nearest decision boundary.

\subsection{Adversarial Training Implementation}

Adversarial training follows the PGD-based procedure from Madry et al.~\cite{madry2018towards}. At each training step, PGD adversarial examples are generated from the current mini-batch, and the model is then trained on these adversarial inputs rather than the clean ones.

There is a subtle but important issue with batch normalization that I had to learn the hard way. When generating adversarial examples, you run a forward pass through the model. If the model is in training mode during this forward pass, the adversarial inputs corrupt the batch normalization running statistics (running mean and variance). The fix: switch to \texttt{model.eval()} before the PGD attack, then back to \texttt{model.train()} for the actual parameter update. Without this, adversarial training simply does not converge --- the model oscillates and never achieves good robust accuracy. I spent some time debugging this before identifying the cause.

Optimizer: SGD with momentum 0.9, weight decay $5 \times 10^{-4}$, initial learning rate 0.1, cosine annealing schedule. Model checkpoints were saved based on PGD robustness accuracy on the test set, not clean accuracy --- robustness is the whole point. All metrics were logged to MLflow at each epoch.

\section{Development Environment}

\begin{itemize}
    \item \textbf{Operating System:} macOS (Apple Silicon)
    \item \textbf{Python Version:} 3.13
    \item \textbf{PyTorch Version:} 2.x with MPS (Metal Performance Shaders) backend
    \item \textbf{Hardware:} Apple M4 Pro, 16GB unified memory
    \item \textbf{Experiment Tracking:} MLflow
    \item \textbf{Deployment:} Docker, HuggingFace Spaces
\end{itemize}

All training used Apple's MPS backend for GPU acceleration. Standard training (50 epochs) took about 55 minutes. Adversarial training took roughly 4.5 hours --- approximately $7\times$ longer per epoch, because every training step requires running a 7-step PGD attack.

\section{Challenges and Solutions}

Several non-trivial implementation issues came up during development:

\textbf{DeepFool numerical stability.} DeepFool computes gradients for all 10 classes inside a loop, which requires \texttt{retain\_graph=True} in PyTorch (otherwise the computation graph is freed after the first \texttt{backward()} call). Additionally, gradient differences between classes are sometimes near-zero, causing division-by-zero in the distance computation. Adding $10^{-8}$ to the norm denominators resolved this.

\textbf{PGD memory accumulation.} As mentioned earlier, failing to detach adversarial tensors between PGD iterations causes PyTorch to retain the full computation graph across all steps. At 20 iterations, this quickly exhausts memory. Explicit \texttt{.detach()} after each projection fixed it.

\textbf{Batch normalization corruption.} Switching to \texttt{model.eval()} during PGD generation inside the adversarial training loop was essential. Without this, running statistics diverge and training does not converge properly.

\textbf{MPS backend issues.} PyTorch's MPS support is still maturing. I encountered some numerical discrepancies between MPS and CPU results, and a few operations that were not yet implemented on MPS. In those cases I fell back to CPU and verified that the outputs matched.

\section{Code Organization}

The codebase is organized by functionality:

\begin{verbatim}
adversarial-robustness/
+-- src/
|   +-- models/
|   |   +-- resnet.py          # ResNet-18 for CIFAR-10
|   +-- attacks/
|   |   +-- fgsm.py            # FGSM attack
|   |   +-- pgd.py             # PGD attack
|   |   +-- deepfool.py        # DeepFool attack
|   +-- training/
|   |   +-- standard.py        # Standard training loop
|   |   +-- adversarial.py     # PGD adversarial training
|   +-- utils/
|   |   +-- data.py            # Data loading and transforms
|   |   +-- metrics.py         # Evaluation metrics
|   +-- api/
|       +-- main.py            # FastAPI backend
+-- frontend/
|   +-- app.py                 # Streamlit demo interface
+-- scripts/
|   +-- train.py               # Training entry point
|   +-- evaluate.py            # Evaluation entry point
+-- configs/
|   +-- config.yaml            # Hyperparameters
+-- checkpoints/               # Saved model weights
+-- tests/
|   +-- test_attacks.py        # Unit tests
+-- Dockerfile                 # Container for deployment
\end{verbatim}

Experiments are run through \texttt{scripts/train.py} and \texttt{scripts/\allowbreak evaluate.py}, with command-line arguments for selecting training mode and evaluation parameters. Hyperparameters are stored in a YAML configuration file.

\section{Deployment}

I built a web demo to make the results tangible. It is a Streamlit application where users can select a CIFAR-10 image (or upload their own), pick an attack type and epsilon value, and immediately see what happens. The interface displays the original image alongside the perturbation (amplified for visibility) and the adversarial image, with confidence scores from both the standard and robust models shown side by side. The application is containerized with Docker and hosted on HuggingFace Spaces.

\section{Summary}

This chapter covered the full implementation: model architecture, three attack algorithms, adversarial training pipeline, and a web-based demo. The key engineering decisions --- skipping input normalization to keep $\epsilon$ values interpretable, managing batch normalization during adversarial training, and careful memory management in iterative attacks --- were driven by the practical requirements of adversarial robustness experiments. The next chapter presents the results obtained with this implementation.
