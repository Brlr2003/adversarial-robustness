\chapter{IMPLEMENTATION}
\label{ch:implementation}

\section{Introduction}

Now I get to the actual code. This chapter covers the implementation: the attack algorithms, the training loops, and the web demo. Everything is in PyTorch. I wrote FGSM, PGD, and DeepFool from scratch, built two training pipelines (standard and adversarial), and put together a Streamlit app on HuggingFace Spaces where people can try the attacks live.

Turning the methodology from the previous chapter into working software meant dealing with memory issues, numerical edge cases, and PyTorch-specific quirks. MLflow handled the experiment tracking.

\section{System Architecture}

The code is split into five modules. The \textbf{Model Module} has the ResNet-18 adapted for CIFAR-10. The \textbf{Attack Module} has FGSM, PGD, and DeepFool, all with the same function signature so I can swap them freely. The \textbf{Training Module} runs standard and adversarial training with MLflow logging. The \textbf{Evaluation Module} computes robustness metrics. The \textbf{Frontend Module} is the Streamlit demo.

I kept things modular on purpose. If I want to add a fourth attack later, I just write one new file and plug it in without changing anything else.

\section{Implementation Details}

\subsection{Model Implementation}

The model is a ResNet-18~\cite{he2016deep}, but I had to modify it because the original was built for $224 \times 224$ ImageNet images and CIFAR-10 only has $32 \times 32$. With the original first layer (a $7 \times 7$ conv with stride 2 followed by max pooling), the feature maps shrink way too fast and you lose most of the spatial information before the residual blocks even start. I swapped in a $3 \times 3$ conv with stride 1 and padding 1, and got rid of the max pooling. That fixed it.

The network has four residual layers, each with two BasicBlock modules. Each BasicBlock does two $3 \times 3$ convolutions with batch normalization and ReLU, and there is a skip connection that adds the input directly to the output (that is the ``residual'' part). When the spatial dimensions need to shrink (in layers 2 through 4), I use a $1 \times 1$ convolution in the shortcut path to match the new dimensions. The channel count goes from 64 to 128 to 256 to 512 across the four layers. At the end, adaptive average pooling brings everything down to $1 \times 1$ spatially, and a fully connected layer outputs 10 class logits. In total the model has about 11.17 million parameters.

For weight initialization I used Kaiming normal for the convolutional layers and constant initialization for the batch normalization layers, which is the standard approach for ResNets.

One design decision that I want to highlight is about input normalization. I kept images in the $[0, 1]$ pixel range and did not apply the usual channel-wise normalization (subtracting the mean and dividing by the standard deviation). I did this on purpose because adversarial perturbation budgets ($\epsilon$) are defined in pixel space. If I had applied normalization, I would have needed to adjust the epsilon values to account for the scaling, which adds complexity for no real benefit. By keeping inputs in $[0, 1]$, an $\epsilon = 8/255 \approx 0.031$ directly means a maximum per-pixel change of 8 intensity levels out of 255, which is the standard value used in the adversarial robustness literature~\cite{madry2018towards}.

\subsection{Data Loading and Preprocessing}

I loaded CIFAR-10 using torchvision. For training data, I applied some standard augmentations: random cropping with 4 pixels of padding and random horizontal flips, then conversion to tensors. For the test set I only converted to tensors with no augmentation. Batches of 128 images were loaded using PyTorch's DataLoader with 4 worker processes and pinned memory to speed up GPU transfers.

\subsection{FGSM Attack Implementation}

I implemented the FGSM attack following Goodfellow et al.~\cite{goodfellow2015explaining}, as shown in Algorithm~\ref{alg:fgsm}. It takes in a batch of images with their true labels, computes the cross-entropy loss, runs backpropagation to get the gradients with respect to the input, and generates the adversarial examples in one step.

\begin{algorithm}[htbp]
\caption{Fast Gradient Sign Method (FGSM)}
\label{alg:fgsm}
\begin{algorithmic}[1]
\REQUIRE Input image $x$, true label $y$, model $f_\theta$, perturbation budget $\epsilon$
\ENSURE Adversarial example $x^{adv}$
\STATE Compute loss: $L = J(\theta, x, y)$
\STATE Compute gradient: $g = \nabla_x L$
\STATE Compute perturbation: $\delta = \epsilon \cdot \text{sign}(g)$
\STATE Generate adversarial example: $x^{adv} = \text{clip}(x + \delta, 0, 1)$
\RETURN $x^{adv}$
\end{algorithmic}
\end{algorithm}

In practice, I set \texttt{requires\_grad = True} on the input tensor before the forward pass so that PyTorch tracks the gradients with respect to the input. After backpropagation, I take the sign of the gradient element-wise and scale it by $\epsilon$. The resulting adversarial images get clamped to $[0, 1]$ so they stay valid. Everything runs on full batches for efficiency.

\subsection{PGD Attack Implementation}

PGD is essentially FGSM applied multiple times with smaller steps, as described by Madry et al.~\cite{madry2018towards}. I show the full procedure in Algorithm~\ref{alg:pgd}. After each gradient step, the result gets projected back into the $L_\infty$ $\epsilon$-ball around the original input so the perturbation stays within budget.

\begin{algorithm}[htbp]
\caption{Projected Gradient Descent (PGD) Attack}
\label{alg:pgd}
\begin{algorithmic}[1]
\REQUIRE Input $x$, label $y$, model $f_\theta$, budget $\epsilon$, step size $\alpha$, iterations $T$
\ENSURE Adversarial example $x^{adv}$
\STATE Initialize: $x^{adv}_0 = x + \text{Uniform}(-\epsilon, \epsilon)$ \COMMENT{Random start}
\FOR{$t = 0$ to $T-1$}
    \STATE Compute loss: $L = J(\theta, x^{adv}_t, y)$
    \STATE Compute gradient: $g = \nabla_x L$
    \STATE Update: $x^{adv}_{t+1} = x^{adv}_t + \alpha \cdot \text{sign}(g)$
    \STATE Project: $x^{adv}_{t+1} = \Pi_{x,\epsilon}(x^{adv}_{t+1})$ \COMMENT{Project to $\epsilon$-ball}
    \STATE Clip: $x^{adv}_{t+1} = \text{clip}(x^{adv}_{t+1}, 0, 1)$
\ENDFOR
\RETURN $x^{adv}_T$
\end{algorithmic}
\end{algorithm}

The attack starts from a random point within the $\epsilon$-ball, which helps it find different adversarial examples and avoid getting stuck in local optima. At each step, I compute the gradient of the cross-entropy loss with respect to the current adversarial image and take a step of size $\alpha$ in the sign direction. The projection step clamps the perturbation $x^{adv} - x$ to $[-\epsilon, \epsilon]$ element-wise to satisfy the $L_\infty$ constraint, and then the whole thing gets clamped to $[0, 1]$ for valid pixel values. One thing I had to be careful about is detaching the gradients after each iteration, otherwise the computational graph keeps growing and you run out of memory fast.

\subsection{DeepFool Implementation}

DeepFool~\cite{moosavi2016deepfool} works quite differently from the other two attacks. Instead of using a fixed perturbation budget, it finds the smallest perturbation that will change the model's prediction. It does this by figuring out which decision boundary is closest and pushing the input just past it.

Because each image has its own closest decision boundary, DeepFool has to process images one at a time rather than in batches. At each iteration, it computes the gradient of every class logit with respect to the input. For each class $k$ that is not the original prediction, it calculates how far away the corresponding decision boundary is using a linearized approximation:

\begin{equation}
    d_k = \frac{|f_k(x) - f_{\hat{k}}(x)|}{\|w_k\|_2}
\end{equation}

where $\hat{k}$ is the original predicted class and $w_k = \nabla_x f_k(x) - \nabla_x f_{\hat{k}}(x)$ is the difference in gradients. The algorithm picks the class with the smallest distance and computes the minimum perturbation to cross that boundary. This keeps going until the prediction changes or the iteration limit is hit. I also apply an overshoot of 0.02 to the total perturbation to make sure the input actually crosses the boundary and does not just land right on it.

The per-image processing makes DeepFool quite a bit slower than the batched attacks, but the payoff is that it finds near-minimal perturbations. This gives you a useful measure of each sample's \textit{robustness radius}, which is essentially how far the input sits from the nearest decision boundary.

\subsection{Adversarial Training Implementation}

For adversarial training I followed the PGD-based approach from Madry et al.~\cite{madry2018towards}. The idea is simple in principle: at each training step, generate PGD adversarial examples from the current mini-batch and train the model on those instead of the clean inputs.

One thing that caused me trouble was batch normalization. The issue is that when you generate adversarial examples, you do a forward pass through the model, and if the model is in training mode, those adversarial inputs update the batch norm running statistics. That corrupts them. The fix is to call \texttt{model.eval()} before generating the attack and \texttt{model.train()} after, before the actual weight update. Took me a while to figure out why my adversarial training was not converging until I realized this was the problem.

For the optimizer I used SGD with momentum (0.9) and weight decay ($5 \times 10^{-4}$), with a cosine annealing schedule starting from a learning rate of 0.1. I saved model checkpoints based on PGD robustness accuracy rather than clean accuracy, since the whole point of adversarial training is to maximize robustness. All the metrics (loss, clean accuracy, robust accuracy, learning rate) get logged to MLflow at each epoch so I can compare experiments easily.

\section{Development Environment}

I developed and trained everything using the following setup:

\begin{itemize}
    \item \textbf{Operating System:} macOS (Apple Silicon)
    \item \textbf{Python Version:} 3.13
    \item \textbf{PyTorch Version:} 2.x with MPS (Metal Performance Shaders) backend
    \item \textbf{Hardware:} Apple M4 Pro, 16GB unified memory
    \item \textbf{Experiment Tracking:} MLflow
    \item \textbf{Deployment:} Docker, HuggingFace Spaces
\end{itemize}

All training ran on Apple's Metal Performance Shaders (MPS) backend, which gives GPU acceleration on Apple Silicon. Standard training for 50 epochs took about 55 minutes, which is reasonable. Adversarial training, on the other hand, took about 4.5 hours because of the overhead from generating PGD attacks at every single training step (roughly 7$\times$ slower per epoch).

\section{Challenges and Solutions}

Here are the main issues I ran into and how I solved them:

\textbf{DeepFool numerical issues:} DeepFool computes gradients for all 10 classes in a loop, and you need \texttt{retain\_graph=True} or PyTorch throws away the graph after the first backward pass. I also had to add $10^{-8}$ to the norm denominators because sometimes the gradient differences are basically zero and you get NaN values.

\textbf{PGD memory leak:} If you forget to call \texttt{.detach()} on the adversarial images between PGD iterations, PyTorch keeps building up the computational graph across all steps. With 20 iterations, memory usage explodes. Detaching after each projection step fixed it.

\textbf{Batch norm corruption:} Already mentioned above, but switching to \texttt{model.eval()} during PGD generation inside the adversarial training loop was essential. Without it, the running mean and variance get thrown off by the adversarial data.

\textbf{MPS backend quirks:} Apple Silicon GPU support in PyTorch is still maturing. Some operations gave slightly different results on MPS vs CPU, and a few were not supported at all. I had to fall back to CPU for certain things and double-check that the numbers matched.

\section{Code Organization}

I organized the codebase by functionality, keeping things modular:

\begin{verbatim}
adversarial-robustness/
+-- src/
|   +-- models/
|   |   +-- resnet.py          # ResNet-18 for CIFAR-10
|   +-- attacks/
|   |   +-- fgsm.py            # FGSM attack
|   |   +-- pgd.py             # PGD attack
|   |   +-- deepfool.py        # DeepFool attack
|   +-- training/
|   |   +-- standard.py        # Standard training loop
|   |   +-- adversarial.py     # PGD adversarial training
|   +-- utils/
|   |   +-- data.py            # Data loading and transforms
|   |   +-- metrics.py         # Evaluation metrics
|   +-- api/
|       +-- main.py            # FastAPI backend
+-- frontend/
|   +-- app.py                 # Streamlit demo interface
+-- scripts/
|   +-- train.py               # Training entry point
|   +-- evaluate.py            # Evaluation entry point
+-- configs/
|   +-- config.yaml            # Hyperparameters
+-- checkpoints/               # Saved model weights
+-- tests/
|   +-- test_attacks.py        # Unit tests
+-- Dockerfile                 # Container for deployment
\end{verbatim}

You run experiments through \texttt{scripts/train.py} and \texttt{scripts/\allowbreak evaluate.py}, which take command-line arguments for choosing between standard/adversarial training and setting evaluation parameters. All the hyperparameters live in a YAML config file.

\section{Deployment}

I also built a web demo so people can actually see what adversarial attacks do. It is a Streamlit app where you pick a CIFAR-10 image (or upload your own), choose an attack and an epsilon value, and see the results from both models instantly. The interface shows the original image next to the perturbation (amplified so it is visible) and the adversarial image, plus the confidence scores from both the standard and robust models. I containerized it with Docker and put it on HuggingFace Spaces.

\section{Summary}

In this chapter I covered the full implementation of the adversarial attack and defense framework. The modular design keeps model architectures, attack algorithms, training procedures, and evaluation as separate components with consistent interfaces. The key decisions I made along the way (keeping inputs in $[0, 1]$ pixel space, handling batch normalization carefully during adversarial training, tracking everything with MLflow) were driven by both the practical needs of adversarial robustness research and good software engineering practices. The next chapter presents the experimental results I got using this implementation.