\chapter{IMPLEMENTATION}
\label{ch:implementation}

\section{Introduction}

In this chapter I walk through how I actually built everything: the adversarial attack algorithms, the defense mechanisms, and the experimental framework that ties it all together. The whole thing is written in PyTorch, with my own implementations of all three attacks (FGSM, PGD, DeepFool), both standard and adversarial training pipelines, and an interactive demo application that I deployed on HuggingFace Spaces.

Going from the methodology described in the previous chapter to working code involved dealing with quite a few practical issues around computational efficiency, numerical stability, and making sure results are reproducible. I used MLflow to track all experiments so I could compare results across different configurations.

\section{System Architecture}

I organized the code into a modular architecture so that each part can be developed and tested independently. There are five main modules:

The \textbf{Model Module} has the ResNet-18 architecture adapted for CIFAR-10. The \textbf{Attack Module} contains my implementations of each adversarial attack, all following the same interface so they are interchangeable. The \textbf{Training Module} handles both standard and adversarial training loops, with MLflow tracking built in. The \textbf{Evaluation Module} runs the robustness evaluations across different attack types and perturbation budgets. And the \textbf{Frontend Module} provides an interactive Streamlit-based demo.

The nice thing about this setup is that I can swap out or add new attacks without touching the training or evaluation code, and the same evaluation pipeline works for both the standard and adversarially trained models.

\section{Implementation Details}

\subsection{Model Implementation}

For CIFAR-10 I implemented a ResNet-18~\cite{he2016deep} with some modifications to handle the smaller image size. The original ResNet-18 was designed for ImageNet images at $224 \times 224$ pixels, but CIFAR-10 images are only $32 \times 32$. If you use the standard architecture as-is, the aggressive downsampling in the first layers destroys too much spatial information. So I made two changes: I replaced the initial $7 \times 7$ convolution (stride 2) with a $3 \times 3$ convolution (stride 1, padding 1), and I removed the initial max pooling layer entirely. This keeps the spatial resolution intact in the early layers, which matters a lot when your input is this small.

The network has four residual layers, each with two BasicBlock modules. Each BasicBlock does two $3 \times 3$ convolutions with batch normalization and ReLU, and there is a skip connection that adds the input directly to the output (that is the ``residual'' part). When the spatial dimensions need to shrink (in layers 2 through 4), I use a $1 \times 1$ convolution in the shortcut path to match the new dimensions. The channel count goes from 64 to 128 to 256 to 512 across the four layers. At the end, adaptive average pooling brings everything down to $1 \times 1$ spatially, and a fully connected layer outputs 10 class logits. In total the model has about 11.17 million parameters.

For weight initialization I used Kaiming normal for the convolutional layers and constant initialization for the batch normalization layers, which is the standard approach for ResNets.

One design decision that I want to highlight is about input normalization. I kept images in the $[0, 1]$ pixel range and did not apply the usual channel-wise normalization (subtracting the mean and dividing by the standard deviation). I did this on purpose because adversarial perturbation budgets ($\epsilon$) are defined in pixel space. If I had applied normalization, I would have needed to adjust the epsilon values to account for the scaling, which adds complexity for no real benefit. By keeping inputs in $[0, 1]$, an $\epsilon = 8/255 \approx 0.031$ directly means a maximum per-pixel change of 8 intensity levels out of 255, which is the standard value used in the adversarial robustness literature~\cite{madry2018towards}.

\subsection{Data Loading and Preprocessing}

I loaded CIFAR-10 using torchvision. For training data, I applied some standard augmentations: random cropping with 4 pixels of padding and random horizontal flips, then conversion to tensors. For the test set I only converted to tensors with no augmentation. Batches of 128 images were loaded using PyTorch's DataLoader with 4 worker processes and pinned memory to speed up GPU transfers.

\subsection{FGSM Attack Implementation}

I implemented the FGSM attack following Goodfellow et al.~\cite{goodfellow2015explaining}, as shown in Algorithm~\ref{alg:fgsm}. It takes in a batch of images with their true labels, computes the cross-entropy loss, runs backpropagation to get the gradients with respect to the input, and generates the adversarial examples in one step.

\begin{algorithm}[htbp]
\caption{Fast Gradient Sign Method (FGSM)}
\label{alg:fgsm}
\begin{algorithmic}[1]
\REQUIRE Input image $x$, true label $y$, model $f_\theta$, perturbation budget $\epsilon$
\ENSURE Adversarial example $x^{adv}$
\STATE Compute loss: $L = J(\theta, x, y)$
\STATE Compute gradient: $g = \nabla_x L$
\STATE Compute perturbation: $\delta = \epsilon \cdot \text{sign}(g)$
\STATE Generate adversarial example: $x^{adv} = \text{clip}(x + \delta, 0, 1)$
\RETURN $x^{adv}$
\end{algorithmic}
\end{algorithm}

In practice, I set \texttt{requires\_grad = True} on the input tensor before the forward pass so that PyTorch tracks the gradients with respect to the input. After backpropagation, I take the sign of the gradient element-wise and scale it by $\epsilon$. The resulting adversarial images get clamped to $[0, 1]$ so they stay valid. Everything runs on full batches for efficiency.

\subsection{PGD Attack Implementation}

PGD is essentially FGSM applied multiple times with smaller steps, as described by Madry et al.~\cite{madry2018towards}. I show the full procedure in Algorithm~\ref{alg:pgd}. After each gradient step, the result gets projected back into the $L_\infty$ $\epsilon$-ball around the original input so the perturbation stays within budget.

\begin{algorithm}[htbp]
\caption{Projected Gradient Descent (PGD) Attack}
\label{alg:pgd}
\begin{algorithmic}[1]
\REQUIRE Input $x$, label $y$, model $f_\theta$, budget $\epsilon$, step size $\alpha$, iterations $T$
\ENSURE Adversarial example $x^{adv}$
\STATE Initialize: $x^{adv}_0 = x + \text{Uniform}(-\epsilon, \epsilon)$ \COMMENT{Random start}
\FOR{$t = 0$ to $T-1$}
    \STATE Compute loss: $L = J(\theta, x^{adv}_t, y)$
    \STATE Compute gradient: $g = \nabla_x L$
    \STATE Update: $x^{adv}_{t+1} = x^{adv}_t + \alpha \cdot \text{sign}(g)$
    \STATE Project: $x^{adv}_{t+1} = \Pi_{x,\epsilon}(x^{adv}_{t+1})$ \COMMENT{Project to $\epsilon$-ball}
    \STATE Clip: $x^{adv}_{t+1} = \text{clip}(x^{adv}_{t+1}, 0, 1)$
\ENDFOR
\RETURN $x^{adv}_T$
\end{algorithmic}
\end{algorithm}

The attack starts from a random point within the $\epsilon$-ball, which helps it find different adversarial examples and avoid getting stuck in local optima. At each step, I compute the gradient of the cross-entropy loss with respect to the current adversarial image and take a step of size $\alpha$ in the sign direction. The projection step clamps the perturbation $x^{adv} - x$ to $[-\epsilon, \epsilon]$ element-wise to satisfy the $L_\infty$ constraint, and then the whole thing gets clamped to $[0, 1]$ for valid pixel values. One thing I had to be careful about is detaching the gradients after each iteration, otherwise the computational graph keeps growing and you run out of memory fast.

\subsection{DeepFool Implementation}

DeepFool~\cite{moosavi2016deepfool} works quite differently from the other two attacks. Instead of using a fixed perturbation budget, it finds the smallest perturbation that will change the model's prediction. It does this by figuring out which decision boundary is closest and pushing the input just past it.

Because each image has its own closest decision boundary, DeepFool has to process images one at a time rather than in batches. At each iteration, it computes the gradient of every class logit with respect to the input. For each class $k$ that is not the original prediction, it calculates how far away the corresponding decision boundary is using a linearized approximation:

\begin{equation}
    d_k = \frac{|f_k(x) - f_{\hat{k}}(x)|}{\|w_k\|_2}
\end{equation}

where $\hat{k}$ is the original predicted class and $w_k = \nabla_x f_k(x) - \nabla_x f_{\hat{k}}(x)$ is the difference in gradients. The algorithm picks the class with the smallest distance and computes the minimum perturbation to cross that boundary. This keeps going until the prediction changes or the iteration limit is hit. I also apply an overshoot of 0.02 to the total perturbation to make sure the input actually crosses the boundary and does not just land right on it.

The per-image processing makes DeepFool quite a bit slower than the batched attacks, but the payoff is that it finds near-minimal perturbations. This gives you a useful measure of each sample's \textit{robustness radius}, which is essentially how far the input sits from the nearest decision boundary.

\subsection{Adversarial Training Implementation}

For adversarial training I followed the PGD-based approach from Madry et al.~\cite{madry2018towards}. The idea is simple in principle: at each training step, generate PGD adversarial examples from the current mini-batch and train the model on those instead of the clean inputs.

There is one subtle detail that took me a while to get right, and it involves batch normalization. When generating the adversarial examples, the model needs to be in evaluation mode so the batch normalization running statistics do not get corrupted by the adversarial inputs. After generating the adversarial batch, I switch the model back to training mode for the actual parameter update. If you skip this step, the batch norm statistics get messed up and training does not converge properly.

For the optimizer I used SGD with momentum (0.9) and weight decay ($5 \times 10^{-4}$), with a cosine annealing schedule starting from a learning rate of 0.1. I saved model checkpoints based on PGD robustness accuracy rather than clean accuracy, since the whole point of adversarial training is to maximize robustness. All the metrics (loss, clean accuracy, robust accuracy, learning rate) get logged to MLflow at each epoch so I can compare experiments easily.

\section{Development Environment}

I developed and trained everything using the following setup:

\begin{itemize}
    \item \textbf{Operating System:} macOS (Apple Silicon)
    \item \textbf{Python Version:} 3.13
    \item \textbf{PyTorch Version:} 2.x with MPS (Metal Performance Shaders) backend
    \item \textbf{Hardware:} Apple M4 Pro, 16GB unified memory
    \item \textbf{Experiment Tracking:} MLflow
    \item \textbf{Deployment:} Docker, HuggingFace Spaces
\end{itemize}

All training ran on Apple's Metal Performance Shaders (MPS) backend, which gives GPU acceleration on Apple Silicon. Standard training for 50 epochs took about 55 minutes, which is reasonable. Adversarial training, on the other hand, took about 4.5 hours because of the overhead from generating PGD attacks at every single training step (roughly 7$\times$ slower per epoch).

\section{Challenges and Solutions}

I ran into several challenges during development that are worth mentioning:

\textbf{Numerical stability in DeepFool:} Computing gradients for all classes at once required careful management of the computational graph using \texttt{retain\_graph=True} during backward passes. A small epsilon value of $10^{-8}$ was added to norm computations to prevent division by zero when gradient differences approached zero.

\textbf{Memory management during PGD:} The multi-step PGD attack accumulates a computational graph across iterations if gradients are not properly detached. The solution was to call \texttt{.detach()} on the adversarial images after each projection step, preventing graph accumulation while maintaining gradient flow within each iteration.

\textbf{Batch normalization during adversarial training:} Generating adversarial examples with the model in training mode caused the batch normalization running statistics to be corrupted by adversarial inputs. The solution was to switch to evaluation mode during attack generation and back to training mode for the actual parameter update.

\textbf{Apple Silicon compatibility:} Some PyTorch operations had limited MPS support at the time of development, requiring occasional fallback to CPU for specific operations and careful testing of numerical equivalence between MPS and CPU results.

\section{Code Organization}

I organized the codebase by functionality, keeping things modular:

\begin{verbatim}
adversarial-robustness/
+-- src/
|   +-- models/
|   |   +-- resnet.py          # ResNet-18 for CIFAR-10
|   +-- attacks/
|   |   +-- fgsm.py            # FGSM attack
|   |   +-- pgd.py             # PGD attack
|   |   +-- deepfool.py        # DeepFool attack
|   +-- training/
|   |   +-- standard.py        # Standard training loop
|   |   +-- adversarial.py     # PGD adversarial training
|   +-- utils/
|   |   +-- data.py            # Data loading and transforms
|   |   +-- metrics.py         # Evaluation metrics
|   +-- api/
|       +-- main.py            # FastAPI backend
+-- frontend/
|   +-- app.py                 # Streamlit demo interface
+-- scripts/
|   +-- train.py               # Training entry point
|   +-- evaluate.py            # Evaluation entry point
+-- configs/
|   +-- config.yaml            # Hyperparameters
+-- checkpoints/               # Saved model weights
+-- tests/
|   +-- test_attacks.py        # Unit tests
+-- Dockerfile                 # Container for deployment
\end{verbatim}

The entry points (\texttt{scripts/train.py} and \texttt{scripts/\allowbreak evaluate.py}) accept command-line arguments to select training mode (standard, adversarial, or both) and evaluation configurations. Configuration parameters are loaded from a YAML file, ensuring reproducibility across experiments.

\section{Deployment}

To make the research more tangible, I also built an interactive web application and deployed it online. The frontend is built with Streamlit and lets visitors pick a sample CIFAR-10 image (or upload their own), choose an attack method and perturbation budget, and see what happens to both the standard and adversarially trained models in real time.

The app shows the original image, the adversarial perturbation (amplified so you can actually see it), and the adversarial image side by side, along with confidence scores from both models. I think this kind of visualization really helps make the abstract idea of adversarial vulnerability concrete and easy to understand. The whole thing is containerized with Docker and deployed on HuggingFace Spaces so anyone can try it.

\section{Summary}

In this chapter I covered the full implementation of the adversarial attack and defense framework. The modular design keeps model architectures, attack algorithms, training procedures, and evaluation as separate components with consistent interfaces. The key decisions I made along the way (keeping inputs in $[0, 1]$ pixel space, handling batch normalization carefully during adversarial training, tracking everything with MLflow) were driven by both the practical needs of adversarial robustness research and good software engineering practices. The next chapter presents the experimental results I got using this implementation.