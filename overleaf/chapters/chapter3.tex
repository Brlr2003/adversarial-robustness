\chapter{METHODOLOGY}
\label{ch:methodology}

\section{Introduction}

In this chapter I describe how I went about studying adversarial attacks and defenses on deep neural networks. The approach is experimental: I implemented multiple attack algorithms, ran them on standard benchmark datasets, and then tested adversarial training as a way to defend against them. I tried to keep everything reproducible and set things up so that I could compare different attack configurations in a systematic way.

\section{Research Design}

The research follows four main phases:

\begin{enumerate}
    \item \textbf{Model Training:} Train baseline convolutional neural networks on benchmark datasets to establish clean accuracy baselines.
    
    \item \textbf{Attack Implementation:} Implement white-box adversarial attacks (FGSM, BIM/PGD, DeepFool) and evaluate their effectiveness against the baseline models.
    
    \item \textbf{Defense Application:} Apply adversarial training to the models and evaluate the resulting robustness improvements.
    
    \item \textbf{Comparative Analysis:} Systematically compare attacks and defenses across multiple metrics and configurations.
\end{enumerate}

\section{Datasets}

\subsection{MNIST}

MNIST~\cite{lecun1998mnist} is a dataset of 70,000 grayscale images of handwritten digits (0 through 9), split into 60,000 for training and 10,000 for testing. Each image is 28$\times$28 pixels. I chose MNIST as a starting benchmark because it is simple enough to train on quickly, which makes it useful for testing that everything works before moving to harder datasets.

\subsection{CIFAR-10}

CIFAR-10 has 60,000 color images spread across 10 classes (things like cars, birds, cats, etc.), with 50,000 for training and 10,000 for testing. The images are 32$\times$32 pixels with 3 color channels. Compared to MNIST, CIFAR-10 is a much harder classification problem, which is why I used it as the main dataset for evaluating how well the attacks work on a more realistic model.

% TODO: Add figure showing sample images from datasets
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/datasets.png}
%     \caption{Sample images from MNIST (left) and CIFAR-10 (right) datasets.}
%     \label{fig:datasets}
% \end{figure}

\section{Model Architectures}

\subsection{MNIST Model}

For MNIST I used a small convolutional neural network with the following layers:

\begin{itemize}
    \item Conv2D (32 filters, 3$\times$3) $\rightarrow$ ReLU $\rightarrow$ Conv2D (64 filters, 3$\times$3) $\rightarrow$ ReLU
    \item MaxPooling (2$\times$2) $\rightarrow$ Dropout (0.25)
    \item Flatten $\rightarrow$ Dense (128) $\rightarrow$ ReLU $\rightarrow$ Dropout (0.5)
    \item Dense (10) $\rightarrow$ Softmax
\end{itemize}

\subsection{CIFAR-10 Model}

For CIFAR-10 I went with a ResNet-18 architecture that I adapted for 32$\times$32 images. The full details of how I modified the standard ResNet-18 to work with CIFAR-10's smaller resolution are in Chapter~\ref{ch:implementation}.

\section{Attack Implementations}

\subsection{Fast Gradient Sign Method (FGSM)}

I implemented the FGSM attack following the standard formulation shown in Equation~\ref{eq:fgsm_method}:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm_method}
\end{equation}

I varied the perturbation budget $\epsilon$ to see how the attack success rate changes with perturbation size. For MNIST the typical values used are $\epsilon \in \{0.1, 0.2, 0.3\}$, and for CIFAR-10 they are $\epsilon \in \{2/255, 4/255, 8/255\}$.

\subsection{Basic Iterative Method / Projected Gradient Descent (BIM/PGD)}

The iterative version of the attack works as follows:

\begin{equation}
    x^{adv}_{t+1} = \Pi_{x,\epsilon}\left(x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\right)
    \label{eq:pgd_method}
\end{equation}

where $\Pi_{x,\epsilon}$ projects the result back into the $\epsilon$-ball around $x$. Parameters include:
\begin{itemize}
    \item Step size $\alpha$: typically $\epsilon/10$ or $\epsilon/4$
    \item Number of iterations: 10, 20, or 40 steps
    \item Random initialization for PGD variant
\end{itemize}

\subsection{DeepFool}

DeepFool~\cite{moosavidezfooli2016deepfool} takes a different approach from FGSM and PGD. Instead of working with a fixed perturbation budget, it tries to find the smallest possible perturbation that will make the model change its prediction. It does this by linearizing the classifier at each step and computing how far the input is from the nearest decision boundary.

I describe the full pseudocode and implementation details in Chapter~\ref{ch:implementation}.

\subsection{(Optional) Carlini-Wagner Attack}

If time and computational resources allow, the C\&W $L_2$ attack~\cite{carlini2017towards} could also be implemented:

\begin{equation}
    \min_\delta \|\delta\|_2 + c \cdot f(x + \delta)
\end{equation}

where $f$ is chosen such that $f(x') \leq 0$ implies misclassification.


\section{Defense: Adversarial Training}

\subsection{FGSM Adversarial Training}

For MNIST, I applied FGSM-based adversarial training following the approach in~\cite{goodfellow2015explaining}:

\begin{enumerate}
    \item For each mini-batch, generate adversarial examples using FGSM
    \item Mix adversarial and clean examples (50/50 ratio)
    \item Update model parameters on the mixed batch
\end{enumerate}

\subsection{PGD Adversarial Training}

For CIFAR-10, I used PGD-based adversarial training following the method from~\cite{madry2018towards}:

\begin{enumerate}
    \item For each mini-batch, generate adversarial examples using multi-step PGD
    \item Train on adversarial examples only (or mixed batches)
    \item Use appropriate perturbation budget ($\epsilon = 8/255$ for $L_\infty$)
\end{enumerate}

The specific training hyperparameters used for both standard and adversarial training are detailed in Chapter~\ref{ch:results}.

\section{Evaluation Metrics}

For every combination of model, dataset, attack, and defense, I measured the following:

\subsection{Accuracy Metrics}

\begin{itemize}
    \item \textbf{Clean Accuracy:} Classification accuracy on unperturbed test data.
    \item \textbf{Robust Accuracy:} Classification accuracy under adversarial perturbations for different $\epsilon$ values.
    \item \textbf{Attack Success Rate:} Percentage of correctly classified samples that become misclassified after perturbation.
\end{itemize}

\subsection{Perturbation Metrics}

\begin{itemize}
    \item \textbf{Average $L_2$ norm:} Mean Euclidean distance between original and adversarial examples.
    \item \textbf{Average $L_\infty$ norm:} Maximum per-pixel perturbation averaged across samples.
\end{itemize}

\subsection{Comparison Table}

Table~\ref{tab:methodology_comparison} summarizes the planned experimental configurations.

\begin{table}[htbp]
    \centering
    \caption{Summary of experimental configurations}
    \label{tab:methodology_comparison}
    \begin{tabular}{llll}
        \toprule
        \textbf{Dataset} & \textbf{Model} & \textbf{Attacks} & \textbf{Defense} \\
        \midrule
        MNIST & Small CNN & FGSM, PGD, DeepFool & FGSM-AT \\
        CIFAR-10 & ResNet-18/Custom & FGSM, PGD, DeepFool & PGD-AT \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Tools and Technologies}

I used the following tools and technologies for the implementation:

\begin{itemize}
    \item \textbf{Python 3.x:} Primary programming language
    \item \textbf{PyTorch:} Deep learning framework for model implementation
    \item \textbf{Custom:} Adversarial attack implementations
    \item \textbf{NumPy, Matplotlib:} Data manipulation and visualization
    \item \textbf{Hardware:} Apple M4 Pro with Metal Performance Shaders (MPS) backend
\end{itemize}

\section{Summary}

To sum up, the methodology I described here gives a structured way to study adversarial attacks and defenses. By implementing multiple attacks on standard benchmarks and testing adversarial training as a defense, I aim to get practical insights into how robust (or fragile) deep neural networks really are. The next chapter goes into the implementation details of everything described above.
