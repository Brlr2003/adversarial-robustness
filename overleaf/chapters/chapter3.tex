\chapter{METHODOLOGY}
\label{ch:methodology}

\section{Introduction}

This chapter describes the experimental setup. I implemented three attack algorithms, ran them on two standard benchmarks, and then applied adversarial training as a defense. Below I go through each component: datasets, models, attack formulations, defense strategy, and evaluation metrics.

\section{Research Design}

The work was organized into four phases:

\begin{enumerate}
    \item \textbf{Model Training:} Train baseline convolutional neural networks on benchmark datasets and establish clean accuracy numbers.

    \item \textbf{Attack Implementation:} Implement FGSM, BIM/PGD, and DeepFool as white-box attacks and measure their effectiveness against the trained baselines.

    \item \textbf{Defense Application:} Retrain models using PGD-based adversarial training and evaluate the resulting robustness gains.

    \item \textbf{Comparative Analysis:} Compare all attacks and both models (standard and robust) across multiple metrics.
\end{enumerate}

\section{Datasets}

\subsection{MNIST}

MNIST~\cite{lecun1998mnist} contains 70,000 grayscale images of handwritten digits (0--9), split into 60,000 training and 10,000 test images, each $28 \times 28$ pixels. It is a simple benchmark --- models can reach high accuracy with very little effort --- so I used it primarily for debugging and verifying that the attack implementations were correct before running the full experiments on CIFAR-10.

\subsection{CIFAR-10}

CIFAR-10 consists of 60,000 color images in 10 classes (cars, birds, cats, etc.), with 50,000 for training and 10,000 for testing. Each image is $32 \times 32 \times 3$. It is considerably harder than MNIST, and the models needed for good performance are correspondingly more complex. This made it the right choice for the main experiments --- the attacks and defenses are more interesting to evaluate when the underlying classification problem is non-trivial.

% TODO: Add figure showing sample images from datasets
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/datasets.png}
%     \caption{Sample images from MNIST (left) and CIFAR-10 (right) datasets.}
%     \label{fig:datasets}
% \end{figure}

\section{Model Architectures}

\subsection{MNIST Model}

For MNIST, a simple CNN was sufficient:

\begin{itemize}
    \item Conv2D (32 filters, 3$\times$3) $\rightarrow$ ReLU $\rightarrow$ Conv2D (64 filters, 3$\times$3) $\rightarrow$ ReLU
    \item MaxPooling (2$\times$2) $\rightarrow$ Dropout (0.25)
    \item Flatten $\rightarrow$ Dense (128) $\rightarrow$ ReLU $\rightarrow$ Dropout (0.5)
    \item Dense (10) $\rightarrow$ Softmax
\end{itemize}

\subsection{CIFAR-10 Model}

For CIFAR-10, I used a ResNet-18 adapted for $32 \times 32$ input resolution. The standard ResNet-18 was designed for $224 \times 224$ ImageNet images, so some modifications were necessary --- the details are in Chapter~\ref{ch:implementation}.

\section{Attack Implementations}

\subsection{Fast Gradient Sign Method (FGSM)}

The FGSM attack follows the standard formulation from Equation~\ref{eq:fgsm_method}:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm_method}
\end{equation}

I tested several perturbation budgets for each dataset. For MNIST: $\epsilon \in \{0.1, 0.2, 0.3\}$. For CIFAR-10: $\epsilon \in \{2/255, 4/255, 8/255\}$.

\subsection{Basic Iterative Method / Projected Gradient Descent (BIM/PGD)}

The iterative variant applies FGSM repeatedly with a smaller step size, projecting back onto the $\epsilon$-ball after each step:

\begin{equation}
    x^{adv}_{t+1} = \Pi_{x,\epsilon}\left(x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\right)
    \label{eq:pgd_method}
\end{equation}

The projection $\Pi_{x,\epsilon}$ ensures the perturbation stays within the allowed $L_\infty$ budget. Key parameters:
\begin{itemize}
    \item Step size $\alpha$: typically $\epsilon/10$ or $\epsilon/4$
    \item Number of iterations: 10, 20, or 40
    \item Random initialization within the $\epsilon$-ball (this is what distinguishes PGD from BIM)
\end{itemize}

\subsection{DeepFool}

DeepFool~\cite{moosavidezfooli2016deepfool} operates on a different principle entirely. Instead of perturbing within a fixed budget, it seeks the smallest perturbation that will cause the model to change its prediction. At each step it linearizes the classifier and estimates the distance to the nearest decision boundary. This process repeats until the prediction flips or an iteration limit is reached.

The full pseudocode and implementation details are in Chapter~\ref{ch:implementation}.

\subsection{(Optional) Carlini-Wagner Attack}

If computational resources had allowed it, the C\&W $L_2$ attack~\cite{carlini2017towards} would also have been interesting to include:

\begin{equation}
    \min_\delta \|\delta\|_2 + c \cdot f(x + \delta)
\end{equation}

where $f$ is designed so that $f(x') \leq 0$ implies misclassification.


\section{Defense: Adversarial Training}

\subsection{FGSM Adversarial Training}

For the MNIST experiments, I used FGSM-based adversarial training following~\cite{goodfellow2015explaining}:

\begin{enumerate}
    \item Generate FGSM adversarial examples for each mini-batch
    \item Combine them with clean examples at a 50/50 ratio
    \item Update model parameters on the combined batch
\end{enumerate}

\subsection{PGD Adversarial Training}

For CIFAR-10, the defense is PGD-based adversarial training as described in~\cite{madry2018towards}:

\begin{enumerate}
    \item For each mini-batch, generate adversarial examples via multi-step PGD
    \item Train on the adversarial examples (or a mix of adversarial and clean)
    \item Perturbation budget: $\epsilon = 8/255$ under the $L_\infty$ norm
\end{enumerate}

The full set of training hyperparameters is reported in Chapter~\ref{ch:results}.

\section{Evaluation Metrics}

The following metrics were tracked for every experiment:

\subsection{Accuracy Metrics}

\begin{itemize}
    \item \textbf{Clean Accuracy:} Accuracy on the unperturbed test set.
    \item \textbf{Robust Accuracy:} Accuracy on adversarially perturbed inputs, measured at each $\epsilon$ value.
    \item \textbf{Attack Success Rate:} The fraction of correctly-classified clean samples that get misclassified after perturbation.
\end{itemize}

\subsection{Perturbation Metrics}

\begin{itemize}
    \item \textbf{Average $L_2$ norm:} Mean Euclidean distance between clean and adversarial examples.
    \item \textbf{Average $L_\infty$ norm:} Mean of the maximum per-pixel perturbation across the test set.
\end{itemize}

\subsection{Comparison Table}

Table~\ref{tab:methodology_comparison} summarizes the experimental configurations.

\begin{table}[htbp]
    \centering
    \caption{Summary of experimental configurations}
    \label{tab:methodology_comparison}
    \begin{tabular}{llll}
        \toprule
        \textbf{Dataset} & \textbf{Model} & \textbf{Attacks} & \textbf{Defense} \\
        \midrule
        MNIST & Small CNN & FGSM, PGD, DeepFool & FGSM-AT \\
        CIFAR-10 & ResNet-18/Custom & FGSM, PGD, DeepFool & PGD-AT \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Tools and Technologies}

The implementation used the following:

\begin{itemize}
    \item \textbf{Python 3.x:} Primary language
    \item \textbf{PyTorch:} Deep learning framework
    \item \textbf{Custom implementations:} All attacks written from scratch (no adversarial ML libraries)
    \item \textbf{NumPy, Matplotlib:} Numerical computation and plotting
    \item \textbf{Hardware:} Apple M4 Pro with Metal Performance Shaders (MPS) as the GPU backend
\end{itemize}

\section{Summary}

The experimental methodology described above provides a controlled framework for comparing adversarial attacks and evaluating adversarial training as a defense. Everything runs on the same models, same datasets, same codebase. The next chapter covers the implementation.
