\chapter{METHODOLOGY}
\label{ch:methodology}

\section{Introduction}

In this chapter I lay out how the experiments were structured. Three attack algorithms, two benchmark datasets, and adversarial training as the defense. I go through each piece below: datasets, models, the math behind each attack, the defense setup, and evaluation metrics.

\section{Research Design}

I organized the work into four phases:

\begin{enumerate}
    \item \textbf{Model Training:} Train baseline CNNs on the benchmark datasets. Get clean accuracy numbers.

    \item \textbf{Attack Implementation:} Code up FGSM, BIM/PGD, and DeepFool. Run them against the baselines.

    \item \textbf{Defense Application:} Apply PGD-based adversarial training. See what happens to robustness.

    \item \textbf{Comparative Analysis:} Put everything in tables. Compare attacks and models across metrics.
\end{enumerate}

\section{Datasets}

\subsection{MNIST}

MNIST~\cite{lecun1998mnist} is 70,000 grayscale images of handwritten digits, 0 through 9. The training set has 60,000 images and the test set 10,000. Each image is $28 \times 28$ pixels. It is an easy dataset --- most models solve it without much trouble. I used MNIST mainly for debugging. When you are writing an attack from scratch, you want to test it on something simple first to make sure it even works before moving on to harder problems.

\subsection{CIFAR-10}

CIFAR-10 is 60,000 color images in 10 classes (cars, birds, cats, and so on), split 50,000/10,000 for training and testing. Images are $32 \times 32$ with 3 color channels. Much harder than MNIST. You need a real model to get good accuracy here, which is exactly the point --- I wanted the attacks and defenses to be evaluated on something non-trivial.

% TODO: Add figure showing sample images from datasets
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/datasets.png}
%     \caption{Sample images from MNIST (left) and CIFAR-10 (right) datasets.}
%     \label{fig:datasets}
% \end{figure}

\section{Model Architectures}

\subsection{MNIST Model}

For MNIST, a small CNN:

\begin{itemize}
    \item Conv2D (32 filters, 3$\times$3) $\rightarrow$ ReLU $\rightarrow$ Conv2D (64 filters, 3$\times$3) $\rightarrow$ ReLU
    \item MaxPooling (2$\times$2) $\rightarrow$ Dropout (0.25)
    \item Flatten $\rightarrow$ Dense (128) $\rightarrow$ ReLU $\rightarrow$ Dropout (0.5)
    \item Dense (10) $\rightarrow$ Softmax
\end{itemize}

Nothing special. Enough to get good accuracy on MNIST.

\subsection{CIFAR-10 Model}

For CIFAR-10 I went with ResNet-18, adapted for $32 \times 32$ inputs. The standard ResNet-18 expects $224 \times 224$ images so modifications were needed. I describe exactly what I changed in Chapter~\ref{ch:implementation}.

\section{Attack Implementations}

\subsection{Fast Gradient Sign Method (FGSM)}

FGSM follows the formulation in Equation~\ref{eq:fgsm_method}:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm_method}
\end{equation}

I tested multiple values of $\epsilon$. For MNIST: $\epsilon \in \{0.1, 0.2, 0.3\}$. For CIFAR-10: $\epsilon \in \{2/255, 4/255, 8/255\}$. The idea is to sweep over perturbation sizes and see how the attack success rate changes.

\subsection{Basic Iterative Method / Projected Gradient Descent (BIM/PGD)}

The iterative version --- apply FGSM repeatedly with a smaller step and project back:

\begin{equation}
    x^{adv}_{t+1} = \Pi_{x,\epsilon}\left(x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\right)
    \label{eq:pgd_method}
\end{equation}

$\Pi_{x,\epsilon}$ projects back into the $\epsilon$-ball. The parameters I varied:
\begin{itemize}
    \item Step size $\alpha$: either $\epsilon/10$ or $\epsilon/4$
    \item Iterations: 10, 20, or 40 steps
    \item Random initialization (the PGD variant starts from a random point in the $\epsilon$-ball, BIM starts from the clean input)
\end{itemize}

\subsection{DeepFool}

DeepFool~\cite{moosavidezfooli2016deepfool} works on a completely different principle. No fixed epsilon budget. Instead it finds the \emph{smallest} perturbation that flips the prediction, by linearizing the classifier at each step and computing the distance to the nearest decision boundary. The pseudocode and details are in Chapter~\ref{ch:implementation}.

\subsection{(Optional) Carlini-Wagner Attack}

I had originally planned to also include C\&W~\cite{carlini2017towards}:

\begin{equation}
    \min_\delta \|\delta\|_2 + c \cdot f(x + \delta)
\end{equation}

where $f$ is chosen so that $f(x') \leq 0$ means misclassification. Unfortunately it proved too computationally expensive to run on my hardware within the time constraints.


\section{Defense: Adversarial Training}

\subsection{FGSM Adversarial Training}

For MNIST I used FGSM-based adversarial training, following~\cite{goodfellow2015explaining}:

\begin{enumerate}
    \item Generate FGSM adversarial examples from each mini-batch
    \item Mix adversarial and clean examples 50/50
    \item Train on the mixed batch
\end{enumerate}

\subsection{PGD Adversarial Training}

For CIFAR-10, the serious defense: PGD-based adversarial training from~\cite{madry2018towards}.

\begin{enumerate}
    \item At each mini-batch, generate PGD adversarial examples (multi-step)
    \item Train on the adversarial examples
    \item Perturbation budget of $\epsilon = 8/255$ under $L_\infty$
\end{enumerate}

Full hyperparameters are in Chapter~\ref{ch:results}.

\section{Evaluation Metrics}

For each experiment:

\subsection{Accuracy Metrics}

\begin{itemize}
    \item \textbf{Clean Accuracy:} How well the model does on normal, unperturbed test images.
    \item \textbf{Robust Accuracy:} How well it does when the test images are adversarially perturbed. Measured at each $\epsilon$.
    \item \textbf{Attack Success Rate:} Of the images the model gets right on clean data, what fraction does the attack flip?
\end{itemize}

\subsection{Perturbation Metrics}

\begin{itemize}
    \item \textbf{Average $L_2$ norm:} Mean Euclidean distance between the clean and adversarial images.
    \item \textbf{Average $L_\infty$ norm:} The mean of the max per-pixel change across test images.
\end{itemize}

\subsection{Comparison Table}

Table~\ref{tab:methodology_comparison} summarizes what was run.

\begin{table}[htbp]
    \centering
    \caption{Summary of experimental configurations}
    \label{tab:methodology_comparison}
    \begin{tabular}{llll}
        \toprule
        \textbf{Dataset} & \textbf{Model} & \textbf{Attacks} & \textbf{Defense} \\
        \midrule
        MNIST & Small CNN & FGSM, PGD, DeepFool & FGSM-AT \\
        CIFAR-10 & ResNet-18/Custom & FGSM, PGD, DeepFool & PGD-AT \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Tools and Technologies}

\begin{itemize}
    \item \textbf{Python 3.x}
    \item \textbf{PyTorch:} for all model and training code
    \item \textbf{Custom attack implementations:} no external adversarial ML libraries used
    \item \textbf{NumPy, Matplotlib:} for data handling and plots
    \item \textbf{Hardware:} Apple M4 Pro, MPS (Metal Performance Shaders) backend for GPU
\end{itemize}

\section{Summary}

That is the setup. Standard datasets, standard model, three attacks implemented from scratch, PGD adversarial training for defense, and a set of metrics to evaluate everything. The next chapter gets into the implementation details.
