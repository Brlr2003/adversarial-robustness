\chapter{METHODOLOGY}
\label{ch:methodology}

\section{Introduction}

This chapter describes the experimental setup. We cover the datasets, model architectures, attack formulations, defense method, and evaluation metrics.

\section{Research Design}

We structured the work into four phases:

\begin{enumerate}
    \item \textbf{Model Training:} Train baseline CNNs on the benchmark datasets and establish clean accuracy.

    \item \textbf{Attack Implementation:} Implement FGSM, BIM/PGD, and DeepFool from scratch and evaluate them against the baselines.

    \item \textbf{Defense Application:} Apply PGD-based adversarial training and measure the resulting robustness.

    \item \textbf{Comparative Analysis:} Compare all attacks and models under identical evaluation conditions.
\end{enumerate}

\section{Datasets}

\subsection{MNIST}

MNIST~\cite{lecun1998mnist} contains 70,000 grayscale images of handwritten digits (0--9), split into 60,000 training and 10,000 test images at $28 \times 28$ resolution. Standard models achieve above 99\% accuracy on this dataset. We used MNIST primarily for validating our attack implementations before moving to the more challenging CIFAR-10.

\subsection{CIFAR-10}

CIFAR-10 contains 60,000 color images across 10 classes, with 50,000 training and 10,000 test images at $32 \times 32$ resolution with 3 color channels. This dataset is substantially harder than MNIST and requires a capable model architecture to achieve good accuracy. We chose CIFAR-10 as our primary benchmark because it provides a non-trivial classification problem on which to evaluate attacks and defenses.

% TODO: Add figure showing sample images from datasets
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/datasets.png}
%     \caption{Sample images from MNIST (left) and CIFAR-10 (right) datasets.}
%     \label{fig:datasets}
% \end{figure}

\section{Model Architectures}

\subsection{MNIST Model}

For MNIST we used a small CNN:

\begin{itemize}
    \item Conv2D (32 filters, 3$\times$3) $\rightarrow$ ReLU $\rightarrow$ Conv2D (64 filters, 3$\times$3) $\rightarrow$ ReLU
    \item MaxPooling (2$\times$2) $\rightarrow$ Dropout (0.25)
    \item Flatten $\rightarrow$ Dense (128) $\rightarrow$ ReLU $\rightarrow$ Dropout (0.5)
    \item Dense (10) $\rightarrow$ Softmax
\end{itemize}

This is a standard architecture for MNIST that achieves the accuracy needed for our purposes.

\subsection{CIFAR-10 Model}

For CIFAR-10 we used ResNet-18, modified for $32 \times 32$ inputs. The standard ResNet-18 architecture is designed for $224 \times 224$ images, so we made several changes to the first convolutional layer and pooling. The specific modifications are described in Chapter~\ref{ch:implementation}.

\section{Attack Implementations}

\subsection{Fast Gradient Sign Method (FGSM)}

FGSM is given by Equation~\ref{eq:fgsm_method}:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm_method}
\end{equation}

We evaluated FGSM across multiple $\epsilon$ values. For MNIST: $\epsilon \in \{0.1, 0.2, 0.3\}$. For CIFAR-10: $\epsilon \in \{2/255, 4/255, 8/255\}$. This sweep allows us to characterize how attack success scales with perturbation budget.

\subsection{Basic Iterative Method / Projected Gradient Descent (BIM/PGD)}

PGD applies FGSM iteratively with a smaller step size and projects back into the constraint set after each step:

\begin{equation}
    x^{adv}_{t+1} = \Pi_{x,\epsilon}\left(x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\right)
    \label{eq:pgd_method}
\end{equation}

Here $\Pi_{x,\epsilon}$ denotes projection onto the $\epsilon$-ball. We varied the following parameters:
\begin{itemize}
    \item Step size $\alpha$: $\epsilon/10$ or $\epsilon/4$
    \item Number of iterations: 10, 20, or 40
    \item Initialization: random start within the $\epsilon$-ball (PGD) versus starting from the clean input (BIM)
\end{itemize}

\subsection{DeepFool}

DeepFool~\cite{moosavidezfooli2016deepfool} differs from the above methods in that it does not operate within a fixed $\epsilon$ budget. Instead, it finds the \emph{smallest} perturbation that changes the model's prediction by iteratively linearizing the classifier and computing the distance to the nearest decision boundary. The full algorithm is described in Chapter~\ref{ch:implementation}.

\subsection{(Optional) Carlini-Wagner Attack}

We initially planned to include the C\&W attack~\cite{carlini2017towards}:

\begin{equation}
    \min_\delta \|\delta\|_2 + c \cdot f(x + \delta)
\end{equation}

where $f$ is designed so that $f(x') \leq 0$ implies misclassification. However, the computational cost proved prohibitive on our hardware, and we were unable to run it within a reasonable time frame.


\section{Defense: Adversarial Training}

\subsection{FGSM Adversarial Training}

For MNIST we used FGSM-based adversarial training following Goodfellow et al.~\cite{goodfellow2015explaining}:

\begin{enumerate}
    \item Generate FGSM adversarial examples from each mini-batch
    \item Mix adversarial and clean examples at a 50/50 ratio
    \item Train on the combined batch
\end{enumerate}

\subsection{PGD Adversarial Training}

For CIFAR-10 we used PGD-based adversarial training following Madry et al.~\cite{madry2018towards}:

\begin{enumerate}
    \item For each mini-batch, generate adversarial examples using multi-step PGD
    \item Train on the adversarial examples
    \item Perturbation budget: $\epsilon = 8/255$ under $L_\infty$
\end{enumerate}

The full set of hyperparameters is given in Chapter~\ref{ch:results}.

\section{Evaluation Metrics}

We report the following metrics for each experiment:

\subsection{Accuracy Metrics}

\begin{itemize}
    \item \textbf{Clean Accuracy:} Accuracy on unperturbed test images.
    \item \textbf{Robust Accuracy:} Accuracy on adversarially perturbed test images, measured at each $\epsilon$.
    \item \textbf{Attack Success Rate:} The fraction of correctly classified clean images that the attack successfully causes to be misclassified.
\end{itemize}

\subsection{Perturbation Metrics}

\begin{itemize}
    \item \textbf{Average $L_2$ norm:} Mean Euclidean distance between clean and adversarial images.
    \item \textbf{Average $L_\infty$ norm:} Mean maximum per-pixel change across the test set.
\end{itemize}

\subsection{Comparison Table}

Table~\ref{tab:methodology_comparison} summarizes the experimental configurations.

\begin{table}[htbp]
    \centering
    \caption{Summary of experimental configurations}
    \label{tab:methodology_comparison}
    \begin{tabular}{llll}
        \toprule
        \textbf{Dataset} & \textbf{Model} & \textbf{Attacks} & \textbf{Defense} \\
        \midrule
        MNIST & Small CNN & FGSM, PGD, DeepFool & FGSM-AT \\
        CIFAR-10 & ResNet-18/Custom & FGSM, PGD, DeepFool & PGD-AT \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Tools and Technologies}

\begin{itemize}
    \item \textbf{Python 3.x}
    \item \textbf{PyTorch:} all model training and attack implementations
    \item \textbf{Custom implementations:} no external adversarial ML libraries
    \item \textbf{NumPy, Matplotlib:} data handling and visualization
    \item \textbf{Hardware:} Apple M4 Pro with MPS (Metal Performance Shaders) backend
\end{itemize}

\section{Summary}

This chapter described the experimental design: standard benchmark datasets, a well-known model architecture, three attacks implemented from scratch, PGD adversarial training as the defense, and a set of metrics for evaluation. The next chapter covers the implementation.

