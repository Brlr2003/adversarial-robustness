\chapter{SOURCE CODE}
\label{ch:appendix_code}

This appendix provides the core Python implementations for the adversarial attack algorithms, training procedures, and model architecture discussed in Chapter~\ref{ch:implementation}. The complete codebase is available at \url{https://github.com/Brlr2003/adversarial-robustness}.

\section{ResNet-18 Model Architecture}

\begin{lstlisting}[language=Python, caption={ResNet-18 Architecture for CIFAR-10}, label={lst:resnet}]
import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    """Basic residual block with two 3x3 convolutions."""
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels,
            kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels,
            kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels,
                    kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels),
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    """ResNet adapted for CIFAR-10 (32x32 inputs)."""
    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.in_channels = 64
        # CIFAR-10: 3x3 conv instead of 7x7, no maxpool
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
            stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64,
            num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128,
            num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256,
            num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512,
            num_blocks[3], stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, block, out_channels,
                    num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for s in strides:
            layers.append(block(self.in_channels,
                out_channels, s))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.avg_pool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        return out


def resnet18_cifar10(num_classes=10):
    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)
\end{lstlisting}

\section{FGSM Attack Implementation}

\begin{lstlisting}[language=Python, caption={FGSM Attack Implementation}, label={lst:fgsm}]
import torch
import torch.nn as nn


def fgsm_attack(model, images, labels, epsilon,
                device="cpu"):
    """
    Fast Gradient Sign Method (FGSM) attack.

    Args:
        model: Target neural network
        images: Clean input images (B, C, H, W) in [0, 1]
        labels: True labels (B,)
        epsilon: Maximum perturbation (L_inf bound)
        device: Device to run on

    Returns:
        Adversarial images (B, C, H, W) in [0, 1]
    """
    images = images.clone().detach().to(device)
    labels = labels.clone().detach().to(device)
    images.requires_grad = True

    # Forward pass
    outputs = model(images)
    loss = nn.CrossEntropyLoss()(outputs, labels)

    # Backward pass
    model.zero_grad()
    loss.backward()

    # Generate adversarial examples
    adv_images = images + epsilon * images.grad.sign()
    adv_images = adv_images.clamp(0, 1).detach()

    return adv_images
\end{lstlisting}

\section{PGD Attack Implementation}

\begin{lstlisting}[language=Python, caption={PGD Attack Implementation}, label={lst:pgd}]
import torch
import torch.nn as nn


def pgd_attack(model, images, labels, epsilon, alpha,
               steps, random_start=True, device="cpu"):
    """
    Projected Gradient Descent (PGD) attack.

    Args:
        model: Target neural network
        images: Clean input images (B, C, H, W) in [0, 1]
        labels: True labels (B,)
        epsilon: Maximum perturbation (L_inf bound)
        alpha: Step size per iteration
        steps: Number of PGD iterations
        random_start: Start from random point in ball
        device: Device to run on

    Returns:
        Adversarial images (B, C, H, W) in [0, 1]
    """
    images = images.clone().detach().to(device)
    labels = labels.clone().detach().to(device)
    adv_images = images.clone().detach()

    if random_start:
        adv_images = adv_images + torch.empty_like(
            adv_images).uniform_(-epsilon, epsilon)
        adv_images = adv_images.clamp(0, 1).detach()

    loss_fn = nn.CrossEntropyLoss()

    for _ in range(steps):
        adv_images.requires_grad = True
        outputs = model(adv_images)
        loss = loss_fn(outputs, labels)

        model.zero_grad()
        loss.backward()

        # Step in gradient direction
        grad_sign = adv_images.grad.sign()
        adv_images = adv_images.detach() + alpha * grad_sign

        # Project back into epsilon-ball
        perturbation = torch.clamp(
            adv_images - images, min=-epsilon, max=epsilon)
        adv_images = (images + perturbation).clamp(
            0, 1).detach()

    return adv_images
\end{lstlisting}

\section{DeepFool Attack Implementation}

\begin{lstlisting}[language=Python, caption={DeepFool Attack Implementation}, label={lst:deepfool}]
import torch
import torch.nn as nn


def deepfool_attack(model, images, max_iterations=50,
                    overshoot=0.02, num_classes=10,
                    device="cpu"):
    """
    DeepFool attack: finds minimal perturbation to
    cross the nearest decision boundary.

    Args:
        model: Target neural network
        images: Clean input images (B, C, H, W)
        max_iterations: Maximum iterations per image
        overshoot: Overshoot parameter
        num_classes: Number of classes
        device: Device to run on

    Returns:
        (adversarial_images, perturbation_norms)
    """
    model.eval()
    images = images.clone().detach().to(device)
    batch_size = images.shape[0]
    adv_images = images.clone()
    perturbation_norms = torch.zeros(batch_size,
                                     device=device)

    for idx in range(batch_size):
        x = images[idx:idx+1].clone().requires_grad_(True)
        output = model(x)
        orig_class = output.argmax(dim=1).item()
        total_pert = torch.zeros_like(x)

        for _ in range(max_iterations):
            x_pert = (images[idx:idx+1] + total_pert
                     ).requires_grad_(True)
            output = model(x_pert)
            if output.argmax(dim=1).item() != orig_class:
                break

            # Compute gradients for all classes
            gradients = []
            for k in range(num_classes):
                if x_pert.grad is not None:
                    x_pert.grad.zero_()
                output[0, k].backward(retain_graph=True)
                gradients.append(x_pert.grad.clone())

            # Find closest decision boundary
            f_orig = output[0, orig_class]
            min_dist = float("inf")
            best_pert = None

            for k in range(num_classes):
                if k == orig_class:
                    continue
                w_k = gradients[k] - gradients[orig_class]
                f_diff = (output[0, k] - f_orig
                         ).abs().item()
                w_norm = w_k.flatten().norm().item()
                if w_norm == 0:
                    continue
                dist = f_diff / w_norm
                if dist < min_dist:
                    min_dist = dist
                    best_pert = (f_diff / (w_norm**2
                        + 1e-8)) * w_k

            if best_pert is None:
                break
            total_pert += best_pert.detach()

        total_pert = (1 + overshoot) * total_pert
        adv_images[idx] = (images[idx]
            + total_pert.squeeze(0)).clamp(0, 1)
        perturbation_norms[idx] = (
            total_pert.flatten().norm())

    return adv_images.detach(), perturbation_norms.detach()
\end{lstlisting}

\section{Adversarial Training Implementation}

\begin{lstlisting}[language=Python, caption={PGD Adversarial Training Loop}, label={lst:adv_training}]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from src.attacks.pgd import pgd_attack


def train_adversarial(model, train_loader, test_loader,
                      config, adv_config, device,
                      checkpoint_dir="./checkpoints"):
    """
    PGD-based adversarial training (Madry et al., 2018).
    """
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        model.parameters(),
        lr=config["learning_rate"],
        momentum=config["momentum"],
        weight_decay=config["weight_decay"])
    scheduler = CosineAnnealingLR(
        optimizer, T_max=config["epochs"])
    best_accuracy = 0.0

    for epoch in range(1, config["epochs"] + 1):
        model.train()
        train_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images = images.to(device)
            labels = labels.to(device)

            # Generate adversarial examples
            model.eval()  # BN in eval mode for attack
            adv_images = pgd_attack(
                model, images, labels,
                epsilon=adv_config["epsilon"],
                alpha=adv_config["alpha"],
                steps=adv_config["steps"],
                random_start=True, device=device)
            model.train()  # Back to train mode

            # Train on adversarial examples
            optimizer.zero_grad()
            outputs = model(adv_images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        scheduler.step()

        # Evaluate and save best checkpoint
        pgd_acc = evaluate_pgd(model, test_loader,
                               adv_config, device)
        if pgd_acc > best_accuracy:
            best_accuracy = pgd_acc
            torch.save({
                "model_state_dict": model.state_dict(),
                "epoch": epoch,
                "pgd_accuracy": pgd_acc,
            }, f"{checkpoint_dir}/robust_best.pt")

    return model
\end{lstlisting}

\section{Evaluation Utilities}

\begin{lstlisting}[language=Python, caption={Robustness Evaluation}, label={lst:eval}]
import torch


def evaluate_robustness(model, test_loader, attack_fn,
                        epsilon_values, device):
    """
    Evaluate model robustness across epsilon values.

    Args:
        model: Neural network model
        test_loader: Test data loader
        attack_fn: Attack function
        epsilon_values: List of epsilon values
        device: Device to run on

    Returns:
        Dictionary of accuracies for each epsilon
    """
    model.eval()
    results = {}

    for epsilon in epsilon_values:
        correct = 0
        total = 0

        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            adv_images = attack_fn(
                model, images, labels, epsilon,
                device=device)

            with torch.no_grad():
                outputs = model(adv_images)
                _, predicted = outputs.max(1)
                correct += (
                    predicted.eq(labels).sum().item())
                total += labels.size(0)

        results[epsilon] = 100.0 * correct / total

    return results
\end{lstlisting}