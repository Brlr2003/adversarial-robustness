\chapter{LITERATURE REVIEW}
\label{ch:literature}

\section{Introduction}

Before getting into my own work, I need to cover what has already been done in this area. There is a surprisingly large body of research on adversarial examples at this point, touching both the machine learning and security communities. I will go through the theory first (why do adversarial examples exist?), then the attacks (how do you generate them?), and finally the defenses (what can you do about them?).

\section{Theoretical Background}

\subsection{Adversarial Examples and Threat Models}

The whole thing started with Szegedy et al.~\cite{szegedy2014intriguing}, who found that deep networks can be systematically fooled by small perturbations. They titled their paper ``Intriguing Properties of Neural Networks'' and the finding caught people off guard, because everyone assumed these models were learning meaningful, robust features from data. Turns out that is not quite the case.

Goodfellow et al.~\cite{goodfellow2015explaining} came along shortly after with a more intuitive explanation. They argued that the problem comes from the high-dimensional linear nature of deep networks, and to prove their point they introduced the Fast Gradient Sign Method (FGSM), which can generate adversarial examples with just a single gradient step on the loss function.

\subsection{Threat Model Taxonomy}

There are a few ways to categorize adversarial attacks:

\begin{itemize}
    \item \textbf{How much does the attacker know?} In a \emph{white-box} setting, the attacker has full access to the model, its weights, and its gradients. In a \emph{black-box} setting, they can only feed inputs in and see what comes out.

    \item \textbf{What is the attacker trying to do?} An \emph{untargeted} attack just wants to cause any wrong answer. A \emph{targeted} attack wants to force the model to output a specific wrong class.

    \item \textbf{How big can the perturbation be?} This is usually measured with norm constraints: $L_\infty$ limits the maximum change to any single pixel, $L_2$ limits the total Euclidean distance, and $L_0$ limits how many pixels can be changed at all.
\end{itemize}

For a more detailed breakdown of these categories and how they apply across domains, the surveys by Yuan et al.~\cite{yuan2019adversarial} and Ren et al.~\cite{ren2020adversarial} are useful references.

\section{Related Work}

\subsection{Gradient-Based Attacks}

\subsubsection{Fast Gradient Sign Method (FGSM)}

FGSM~\cite{goodfellow2015explaining} is one of the simplest and most well-known attacks. It works by taking a single step in the direction of the gradient of the loss with respect to the input:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm}
\end{equation}

where $x$ is the original input, $\epsilon$ controls how large the perturbation can be, $J$ is the loss function, $\theta$ represents the model parameters, and $y$ is the true label. FGSM is fast to compute since it only needs one forward and one backward pass, but the trade-off is that it does not always find the strongest possible adversarial example.

\subsubsection{Basic Iterative Method (BIM) / Projected Gradient Descent (PGD)}

To get around FGSM's limitation, Kurakin et al.~\cite{kurakin2016adversarialscale} proposed the Basic Iterative Method (BIM), which simply applies FGSM multiple times with a smaller step size:

\begin{equation}
    x^{adv}_{t+1} = \text{Clip}_{x,\epsilon}\{x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\}
    \label{eq:bim}
\end{equation}

Madry et al.~\cite{madry2018towards} later formalized this idea as Projected Gradient Descent (PGD), which adds a random starting point within the $\epsilon$-ball. PGD has since become one of the most widely used attacks for benchmarking model robustness, and it is generally considered the strongest first-order attack available.

\subsection{Optimization-Based Attacks}

\subsubsection{DeepFool}

Moosavi-Dezfooli et al.~\cite{moosavidezfooli2016deepfool} took a different approach with DeepFool. Instead of using a fixed perturbation budget, DeepFool iteratively linearizes the classifier around the current point and finds the closest decision boundary. It then computes the smallest perturbation needed to cross that boundary. The result is that DeepFool tends to produce much smaller perturbations than FGSM while still successfully fooling the model.

\subsubsection{Carlini-Wagner (C\&W) Attack}

Carlini and Wagner~\cite{carlini2017towards} approached the problem from a more formal optimization angle:

\begin{equation}
    \min_\delta \|\delta\|_p + c \cdot f(x + \delta)
    \label{eq:cw}
\end{equation}

where $f$ is an objective function that encourages misclassification. What made the C\&W attack particularly important is that it was able to break several defenses that were thought to be effective at the time. It is still one of the strongest attacks out there, although it takes significantly more computation than gradient-based methods.

\subsubsection{Universal Adversarial Perturbations}

In a somewhat unsettling finding, Moosavi-Dezfooli et al.~\cite{moosavidezfooli2017universal} showed that you can find a single perturbation vector that fools a classifier on most inputs. This means the vulnerability is not just about individual images but rather something more systematic about how these networks work.

\subsection{Black-Box Attacks and Transferability}

Papernot et al.~\cite{papernot2017practical} explored the idea of black-box attacks using substitute models. The basic idea is that if you can train your own model to mimic the target, you can craft adversarial examples on your model and they will often transfer to the target. This transferability property has serious security implications because it means an attacker does not actually need access to the target model's internals to fool it.

\section{Defense Mechanisms}

\subsection{Adversarial Training}

The most straightforward defense is adversarial training, where you include adversarial examples in the training process so the model learns to handle them~\cite{goodfellow2015explaining,kurakin2016adversarialscale,madry2018towards}. Formally, the training objective becomes a min-max problem:

\begin{equation}
    \min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \max_{\|\delta\| \leq \epsilon} J(\theta, x + \delta, y) \right]
    \label{eq:adv_training}
\end{equation}

Madry et al.~\cite{madry2018towards} showed that using PGD to generate the adversarial examples during training leads to models with much better robustness. One downside is the computational cost, since you need to run PGD at every training step. Shafahi et al.~\cite{shafahi2019adversarialfree} addressed this with ``Adversarial Training for Free,'' a method that reuses gradient computations to cut down the overhead.

\subsection{Gradient Masking and Obfuscation}

Another line of defense tries to make gradients unusable for the attacker, essentially hiding the gradient information. It sounds clever, but Athalye et al.~\cite{athalye2018obfuscated} put out a paper showing that most gradient masking defenses are basically security theater. They came up with backward pass differentiable approximation (BPDA) and similar tricks that bypass the masking entirely. A lot of defenses that looked promising at the time turned out to be broken.

\subsection{Other Defense Approaches}

Beyond adversarial training and gradient masking, there are a few other families of defenses worth mentioning:

\begin{itemize}
    \item \textbf{Input preprocessing:} Run the input through some transformation (compression, denoising, etc.) before feeding it to the model, hoping to strip out the adversarial noise.
    \item \textbf{Certified defenses:} These come with mathematical proofs that the model will not change its prediction within a certain perturbation radius. The guarantees are real but usually come at a steep accuracy cost.
    \item \textbf{Randomized smoothing:} Add Gaussian noise to the input and classify based on the majority vote, which gives you certifiable robustness.
\end{itemize}

Recent surveys provide a more detailed look at these methods and how they compare~\cite{ren2020adversarial,chakraborty2018survey,zhou2022adversarial,macas2024adversarial}.

\section{Research Gap}

While reading through the literature, a few things stood out to me as missing:

\begin{enumerate}
    \item Papers tend to look at one attack or one defense. I could not find many studies that put FGSM, PGD, and DeepFool side by side on the same model with the same evaluation setup.

    \item Different papers report accuracy-robustness trade-offs in different ways, using different epsilon values, different numbers of PGD steps, etc. It is hard to compare across studies.

    \item A lot of papers skip over the engineering details. They mention using PyTorch and give the hyperparameters, but the actual implementation choices (like how to handle batch normalization during adversarial training) are left out.
\end{enumerate}

That is what motivated me to do this thesis. I wanted one consistent study where I implement everything myself, run all the attacks on the same model, and document exactly how I did it.

\section{Summary}

Looking at the literature as a whole, it is clear that adversarial vulnerability is not a bug that can be easily patched but rather something fundamental about how deep neural networks work. The attacks range from simple one-step gradient methods to sophisticated optimization-based approaches, and on the defense side, adversarial training is still the most reliable option even though it comes with real computational costs and accuracy trade-offs. The following chapters build on this foundation with my own implementations and experiments.
