\chapter{LITERATURE REVIEW}
\label{ch:literature}

\section{Introduction}

In this chapter we review the existing literature on adversarial examples for deep neural networks. We cover the theoretical foundations, the main attack methods, and the proposed defenses.

\section{Theoretical Background}

\subsection{Adversarial Examples and Threat Models}

Szegedy et al.~\cite{szegedy2014intriguing} first demonstrated that neural networks with high test accuracy can be fooled by adding small, imperceptible perturbations to their inputs. Prior to this result, it was generally assumed that these networks were learning meaningful features of the data. The finding that small perturbations could cause confident misclassification suggested otherwise.

Goodfellow et al.~\cite{goodfellow2015explaining} proposed that the vulnerability arises from the linear behavior of neural networks in high-dimensional spaces. Even though the networks contain nonlinear activation functions, a perturbation that is small in each dimension can accumulate across many dimensions and produce a large change in the output. To demonstrate this, they introduced FGSM --- a method that generates adversarial examples using a single gradient step. The method is simple, and the fact that it works well indicates that the vulnerability is systematic rather than incidental.

\subsection{Threat Model Taxonomy}

Adversarial attacks are typically categorized along three axes:

\begin{itemize}
    \item \textbf{Attacker knowledge.} In the \emph{white-box} setting, the attacker has full access to the model, including architecture, weights, and gradients. In the \emph{black-box} setting, the attacker can only query the model and observe outputs.

    \item \textbf{Attacker goal.} \emph{Untargeted} attacks aim to cause any misclassification. \emph{Targeted} attacks aim to produce a specific incorrect prediction.

    \item \textbf{Perturbation constraint.} The perturbation is typically bounded under an $L_p$ norm. $L_\infty$ bounds the maximum change per pixel, $L_2$ bounds the Euclidean distance, and $L_0$ bounds the number of modified pixels.
\end{itemize}

We refer the reader to Yuan et al.~\cite{yuan2019adversarial} and Ren et al.~\cite{ren2020adversarial} for thorough surveys of threat models.

\section{Related Work}

\subsection{Gradient-Based Attacks}

\subsubsection{Fast Gradient Sign Method (FGSM)}

FGSM~\cite{goodfellow2015explaining} computes the gradient of the loss with respect to the input, takes the sign, and scales by the perturbation budget $\epsilon$:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm}
\end{equation}

It requires one forward pass and one backward pass. Because it takes only a single step, FGSM does not necessarily find the strongest adversarial example within the $\epsilon$-ball.

\subsubsection{Basic Iterative Method (BIM) / Projected Gradient Descent (PGD)}

Kurakin et al.~\cite{kurakin2016adversarialscale} extended FGSM by applying it iteratively with a smaller step size:

\begin{equation}
    x^{adv}_{t+1} = \text{Clip}_{x,\epsilon}\{x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\}
    \label{eq:bim}
\end{equation}

Madry et al.~\cite{madry2018towards} formalized this as projected gradient descent on the inner maximization problem. The main modification was initializing from a random point within the $\epsilon$-ball, which helps the attack explore the loss landscape more effectively. PGD has since become the standard first-order attack for evaluating adversarial robustness. If a model withstands PGD, this is taken as evidence of robustness against first-order adversaries.

\subsection{Optimization-Based Attacks}

\subsubsection{DeepFool}

Moosavi-Dezfooli et al.~\cite{moosavidezfooli2016deepfool} asked a different question: what is the \emph{smallest} perturbation that changes the prediction? DeepFool answers this by iteratively linearizing the classifier and computing the distance to the nearest decision boundary. The resulting perturbations are much smaller than those of FGSM, yet they still cause misclassification. We remark that DeepFool provides geometric information about the model --- specifically, the distance from each data point to the nearest decision boundary.

\subsubsection{Carlini-Wagner (C\&W) Attack}

Carlini and Wagner~\cite{carlini2017towards} formulated adversarial example generation as an optimization problem:

\begin{equation}
    \min_\delta \|\delta\|_p + c \cdot f(x + \delta)
    \label{eq:cw}
\end{equation}

where $f$ is designed such that $f(x') \leq 0$ implies misclassification. The C\&W attack demonstrated that several defenses previously claimed to be effective were in fact broken. It remains one of the strongest attacks, though it is computationally expensive compared to FGSM or PGD.

\subsubsection{Universal Adversarial Perturbations}

Moosavi-Dezfooli et al.~\cite{moosavidezfooli2017universal} showed that a single, fixed perturbation can fool a classifier on the majority of inputs. This is not an input-specific perturbation --- the same noise pattern is applied to all images. The existence of such universal perturbations suggests that the vulnerability is structural rather than tied to individual inputs.

\subsection{Black-Box Attacks and Transferability}

Papernot et al.~\cite{papernot2017practical} showed that white-box access is not required. An attacker can train a substitute model that approximates the target, generate adversarial examples against the substitute, and transfer them to the target model. This transfer attack works well in practice, which means an attacker does not need knowledge of the target model's internals.

\section{Defense Mechanisms}

\subsection{Adversarial Training}

The most direct defense is to augment training with adversarial examples~\cite{goodfellow2015explaining,kurakin2016adversarialscale,madry2018towards}. This amounts to solving a min-max optimization problem:

\begin{equation}
    \min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \max_{\|\delta\| \leq \epsilon} J(\theta, x + \delta, y) \right]
    \label{eq:adv_training}
\end{equation}

Madry et al.~\cite{madry2018towards} showed that using PGD for the inner maximization produces substantially better robustness than using FGSM. The cost is computational: running a multi-step attack at each training iteration makes training roughly 5--10$\times$ slower. Shafahi et al.~\cite{shafahi2019adversarialfree} addressed this by reusing gradient computations, reducing the overhead of adversarial training.

\subsection{Gradient Masking and Obfuscation}

A number of defenses attempted to prevent attacks by making gradients uninformative. Athalye et al.~\cite{athalye2018obfuscated} showed that nearly all such defenses could be circumvented. They developed BPDA (backward pass differentiable approximation) and related techniques to recover useful gradients despite the defense. Many defenses that had been reported as effective turned out to provide little actual robustness.

\subsection{Other Defense Approaches}

Several other defense strategies have been explored:

\begin{itemize}
    \item \textbf{Input preprocessing:} Applying transformations such as JPEG compression or denoising before classification, with the goal of removing adversarial perturbations.
    \item \textbf{Certified defenses:} Providing formal guarantees that the prediction will not change within a given perturbation radius. These guarantees come at the cost of reduced accuracy.
    \item \textbf{Randomized smoothing:} Classifying by majority vote over noisy copies of the input, which yields certifiable robustness in a probabilistic sense.
\end{itemize}

We refer the reader to the surveys by Ren et al.~\cite{ren2020adversarial}, Chakraborty et al.~\cite{chakraborty2018survey}, Zhou et al.~\cite{zhou2022adversarial}, and Macas et al.~\cite{macas2024adversarial} for further detail.

\section{Research Gap}

We identify several gaps in the existing literature:

\begin{enumerate}
    \item Most works focus on a single attack or a single defense. We found it difficult to locate a study that compares FGSM, PGD, and DeepFool on the same model under the same evaluation protocol. Different papers use different configurations, which makes cross-paper comparison unreliable.

    \item The accuracy--robustness trade-off is hard to assess from the literature because experimental setups vary: different $\epsilon$ values, different numbers of PGD steps, different architectures. It is often unclear whether differences in reported numbers come from the method or the setup.

    \item Implementation details are frequently underspecified. Papers report hyperparameters but omit details that matter in practice --- for example, the handling of batch normalization during adversarial training, which we found to have a substantial effect on the final model.
\end{enumerate}

This thesis addresses these gaps by providing a single study with a unified codebase, consistent evaluation, and sufficient implementation detail for reproducibility.

\section{Summary}

The literature indicates that adversarial vulnerability is a fundamental property of current deep learning models rather than an artifact that can be patched. Attacks range from single-step methods (FGSM) to full optimization procedures (C\&W). On the defense side, adversarial training~\cite{madry2018towards} remains the most reliable approach, despite its cost in training time and clean accuracy. The remainder of this thesis builds on these foundations with our own implementations and experiments.

