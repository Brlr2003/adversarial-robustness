\chapter{LITERATURE REVIEW}
\label{ch:literature}

\section{Introduction}

There is by now a large and growing literature on adversarial examples, spanning machine learning and computer security. Before describing my own experiments, I want to review the key ideas and results that form the basis for this work. The structure here follows a natural progression: first the theory (why adversarial examples exist), then the attacks (how to construct them), and then the defenses (what can be done about them).

\section{Theoretical Background}

\subsection{Adversarial Examples and Threat Models}

Szegedy et al.~\cite{szegedy2014intriguing} discovered that deep neural networks are vulnerable to small, seemingly random perturbations of their inputs. The paper was titled ``Intriguing Properties of Neural Networks,'' and the result genuinely surprised people --- the assumption had been that these networks learn robust, meaningful feature representations. That assumption turned out to be wrong, or at the very least incomplete.

Shortly after, Goodfellow et al.~\cite{goodfellow2015explaining} offered what remains one of the more compelling explanations. They argued that the vulnerability stems from the high-dimensional linear nature of deep networks. In high dimensions, even a small per-coordinate perturbation, when applied consistently in the direction of the gradient, accumulates into a large change in the network's output. To make this argument concrete, they introduced FGSM, which constructs adversarial examples with a single gradient computation.

\subsection{Threat Model Taxonomy}

Adversarial attacks can be categorized along several axes:

\begin{itemize}
    \item \textbf{Attacker's knowledge.} In \emph{white-box} attacks, the adversary has complete access to model weights and can compute gradients. In \emph{black-box} attacks, only input-output access is available.

    \item \textbf{Attacker's goal.} \emph{Untargeted} attacks aim to cause any misclassification. \emph{Targeted} attacks aim to force a specific wrong output class.

    \item \textbf{Perturbation constraints.} Typically measured by $L_p$ norms: $L_\infty$ bounds the maximum change to any single pixel, $L_2$ bounds the Euclidean magnitude of the perturbation, and $L_0$ counts how many pixels are modified.
\end{itemize}

Yuan et al.~\cite{yuan2019adversarial} and Ren et al.~\cite{ren2020adversarial} provide thorough surveys covering these categories and how they apply across different problem domains.

\section{Related Work}

\subsection{Gradient-Based Attacks}

\subsubsection{Fast Gradient Sign Method (FGSM)}

FGSM~\cite{goodfellow2015explaining} remains one of the simplest attacks in the literature. Given an input $x$ with true label $y$, it computes:

\begin{equation}
    x^{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
    \label{eq:fgsm}
\end{equation}

Here $J$ is the loss function, $\theta$ the model parameters, and $\epsilon$ the perturbation budget. The attack requires only one forward pass and one backward pass, which makes it very fast. The downside is that it only takes a single step in the gradient direction, so it generally does not find the worst-case perturbation within the allowed budget.

\subsubsection{Basic Iterative Method (BIM) / Projected Gradient Descent (PGD)}

Kurakin et al.~\cite{kurakin2016adversarialscale} proposed a natural extension: apply FGSM iteratively with a smaller step size.

\begin{equation}
    x^{adv}_{t+1} = \text{Clip}_{x,\epsilon}\{x^{adv}_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^{adv}_t, y))\}
    \label{eq:bim}
\end{equation}

Madry et al.~\cite{madry2018towards} later cast this procedure as Projected Gradient Descent on the inner maximization problem, with an additional random initialization step. Their formulation of PGD has since become the de facto standard for evaluating adversarial robustness. It is generally regarded as the strongest first-order attack --- meaning that if a model is robust to PGD, there is reasonable evidence that it is robust to any attack that only uses gradient information.

\subsection{Optimization-Based Attacks}

\subsubsection{DeepFool}

Moosavi-Dezfooli et al.~\cite{moosavidezfooli2016deepfool} approached the problem differently. Rather than perturbing within a fixed budget, DeepFool iteratively approximates the nearest decision boundary by linearizing the classifier, then computes the minimum perturbation needed to cross it. The perturbations it produces are typically much smaller than those from FGSM, yet they still succeed in changing the prediction. This makes DeepFool particularly useful for measuring the \emph{robustness radius} of a model --- that is, how far data points sit from the nearest decision boundary.

\subsubsection{Carlini-Wagner (C\&W) Attack}

Carlini and Wagner~\cite{carlini2017towards} formulated adversarial example generation as a formal optimization problem:

\begin{equation}
    \min_\delta \|\delta\|_p + c \cdot f(x + \delta)
    \label{eq:cw}
\end{equation}

where $f$ encourages misclassification. The C\&W attack proved especially important because it broke several defenses that had appeared effective against weaker attacks. It remains among the strongest attacks available, though its computational cost is considerably higher than gradient-based methods.

\subsubsection{Universal Adversarial Perturbations}

Moosavi-Dezfooli et al.~\cite{moosavidezfooli2017universal} showed something particularly striking: there exist single perturbation vectors that, when added to almost any input, cause misclassification. This is a strong result. It implies that the vulnerability is not an artifact of individual inputs but something structural about the learned representations.

\subsection{Black-Box Attacks and Transferability}

Papernot et al.~\cite{papernot2017practical} demonstrated that black-box attacks are feasible through a substitute model approach. The idea: train a local model to approximate the target, generate adversarial examples against the local model, and rely on the fact that they transfer. And they do transfer, surprisingly often. This transferability phenomenon has significant security implications --- it means that white-box access is not a prerequisite for mounting effective attacks.

\section{Defense Mechanisms}

\subsection{Adversarial Training}

The most conceptually straightforward defense is adversarial training: augment the training data with adversarial examples so that the model learns to classify them correctly~\cite{goodfellow2015explaining,kurakin2016adversarialscale,madry2018towards}. The training objective becomes a min-max problem:

\begin{equation}
    \min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \max_{\|\delta\| \leq \epsilon} J(\theta, x + \delta, y) \right]
    \label{eq:adv_training}
\end{equation}

Madry et al.~\cite{madry2018towards} showed that solving the inner maximization with PGD during training yields substantially more robust models than using FGSM alone. The main drawback is cost: you need to run a multi-step attack at every training iteration, which increases training time by a factor of roughly $5$--$10\times$. Shafahi et al.~\cite{shafahi2019adversarialfree} later proposed ``Adversarial Training for Free,'' which recycles gradient computations to reduce this overhead.

\subsection{Gradient Masking and Obfuscation}

A different line of defense attempts to deny the attacker useful gradient information --- essentially, making gradients uninformative or zero. This looked promising initially, but Athalye et al.~\cite{athalye2018obfuscated} showed convincingly that these defenses provide a false sense of security. Their paper introduced backward pass differentiable approximation (BPDA) and related techniques that circumvent gradient masking. Many defenses that had appeared robust turned out to be broken once evaluated with these stronger attacks.

\subsection{Other Defense Approaches}

Several additional defense families have been explored:

\begin{itemize}
    \item \textbf{Input preprocessing:} Apply transformations to the input --- JPEG compression, denoising, etc. --- before classification, with the hope of removing adversarial perturbations.
    \item \textbf{Certified defenses:} Provide mathematical guarantees that the model's prediction will not change within some perturbation radius. The guarantees are formal, but they generally come with significant accuracy penalties.
    \item \textbf{Randomized smoothing:} Classify based on a majority vote over noisy copies of the input. This yields certifiable robustness guarantees with a probabilistic flavor.
\end{itemize}

For a broader treatment of these and other approaches, see the surveys by Ren et al.~\cite{ren2020adversarial}, Chakraborty et al.~\cite{chakraborty2018survey}, Zhou et al.~\cite{zhou2022adversarial}, and Macas et al.~\cite{macas2024adversarial}.

\section{Research Gap}

In reviewing the literature, a few gaps became apparent to me:

\begin{enumerate}
    \item Most papers focus on a single attack or a single defense in isolation. Comparisons that put FGSM, PGD, and DeepFool side by side, on the same model, with the same evaluation protocol, are surprisingly rare.

    \item Accuracy--robustness trade-offs are reported using different epsilon values, different PGD step counts, and different evaluation procedures, which makes it hard to compare results across papers.

    \item Implementation details are often left out. Authors mention PyTorch and list hyperparameters, but practical issues --- like how to handle batch normalization statistics during adversarial training --- tend not to be discussed.
\end{enumerate}

These gaps motivated the present work. I wanted a single, self-contained study where everything is implemented from scratch, evaluated consistently, and documented in enough detail that someone else could reproduce it.

\section{Summary}

The literature makes clear that adversarial vulnerability is not some minor edge case but rather a fundamental property of how deep neural networks operate. Attacks range from simple one-step gradient methods (FGSM) to sophisticated optimization procedures (C\&W), and on the defense side, adversarial training remains the most empirically reliable option despite its computational cost and the accuracy penalty it incurs. The following chapters describe my own implementations and experiments that build on this foundation.
