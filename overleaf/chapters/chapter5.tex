\chapter{RESULTS AND DISCUSSION}
\label{ch:results}

\section{Introduction}

This chapter reports the experimental results. I evaluated FGSM, PGD, and DeepFool on CIFAR-10, measured the vulnerability of a standard ResNet-18, applied PGD-based adversarial training, and compared the three attacks head-to-head. All experiments used the framework described in Chapter~\ref{ch:implementation}, with MLflow tracking throughout.

\section{Experimental Setup}

\subsection{Training Configuration}

Two ResNet-18 models were trained on CIFAR-10 with identical hyperparameters --- same optimizer, same schedule, same number of epochs. The only difference: one was trained normally, the other with PGD adversarial training. Table~\ref{tab:training_config} lists the full configuration.

\begin{table}[htbp]
    \centering
    \caption{Training hyperparameters for standard and adversarial training}
    \label{tab:training_config}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Standard Training} & \textbf{Adversarial Training} \\
        \midrule
        Batch size & 128 & 128 \\
        Optimizer & SGD & SGD \\
        Learning rate & 0.1 & 0.1 \\
        Momentum & 0.9 & 0.9 \\
        Weight decay & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
        LR schedule & Cosine annealing & Cosine annealing \\
        Epochs & 50 & 50 \\
        Data augmentation & RandomCrop, HFlip & RandomCrop, HFlip \\
        \bottomrule
    \end{tabular}
\end{table}

Standard training completed in roughly 55 minutes. Adversarial training took about 4.5 hours --- the $7\times$ slowdown comes from running 7-step PGD at every training iteration.

\subsection{Attack Parameters}

Table~\ref{tab:attack_params} lists the parameters used for each attack during evaluation.

\begin{table}[htbp]
    \centering
    \caption{Attack parameters used during evaluation}
    \label{tab:attack_params}
    \begin{tabular}{lp{8cm}}
        \toprule
        \textbf{Attack} & \textbf{Parameters} \\
        \midrule
        FGSM & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$ \\
        PGD & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$, step size $\alpha = \epsilon/4$, iterations $T = 20$, random start \\
        DeepFool & Max iterations = 50, overshoot = 0.02, 10 classes \\
        \bottomrule
    \end{tabular}
\end{table}

For the inner attack during adversarial training, I used PGD with $\epsilon = 8/255$, $\alpha = 2/255$, and 7 iterations with random start, which matches the setup in Madry et al.~\cite{madry2018towards}.

\subsection{Evaluation Protocol}

All attacks were evaluated on the full CIFAR-10 test set (10,000 images). PGD used a single random restart, which is standard for benchmarking adversarially trained models. Attack success rates were computed only over samples that the model classified correctly on clean inputs.

Figure~\ref{fig:demo_interface} shows the interactive demo deployed on HuggingFace Spaces, where one can compare the two models under different attacks and perturbation budgets.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/stats.png}
    \caption{Interactive demo interface deployed on HuggingFace Spaces, showing model accuracy metrics and attack configuration controls for real-time adversarial robustness evaluation.}
    \label{fig:demo_interface}
\end{figure}

\section{Baseline Results}

\subsection{Model Performance on Clean Data}

Table~\ref{tab:clean_accuracy} shows the clean test accuracy for both models.

\begin{table}[htbp]
    \centering
    \caption{Clean accuracy of trained models on CIFAR-10}
    \label{tab:clean_accuracy}
    \begin{tabular}{lc}
        \toprule
        \textbf{Model} & \textbf{Clean Accuracy (\%)} \\
        \midrule
        Standard ResNet-18 & 94.1 \\
        Adversarially Trained ResNet-18 & 83.9 \\
        \bottomrule
    \end{tabular}
\end{table}

The standard model reaches 94.1\%, which is consistent with published results for ResNet-18 on CIFAR-10 without input normalization. The adversarially trained model achieves 83.9\% --- a drop of 10.2 percentage points. This accuracy penalty is expected and well-documented in the literature~\cite{madry2018towards}; I return to it in Section~\ref{sec:tradeoffs}.

\section{Attack Evaluation Results}

\subsection{FGSM Attack Results}

Table~\ref{tab:fgsm_results} reports robust accuracy under FGSM across several perturbation budgets.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under FGSM attack on CIFAR-10}
    \label{tab:fgsm_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 68.2 & 42.5 & 18.3 & 5.1 \\
        Adv-Trained & 76.8 & 68.4 & 56.0 & 38.2 \\
        \bottomrule
    \end{tabular}
\end{table}

The standard model degrades rapidly: from 94.1\% clean accuracy down to just 18.3\% at $\epsilon = 8/255$. The adversarially trained model retains 56.0\% at the same budget --- a 37.7 percentage point advantage. Figure~\ref{fig:attack_fgsm} shows a visual example of FGSM on a sample image.

\subsection{PGD Attack Results}

PGD-20 results are in Table~\ref{tab:pgd_results}. This is the stronger evaluation.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under PGD-20 attack on CIFAR-10}
    \label{tab:pgd_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 41.3 & 12.8 & 1.2 & 0.1 \\
        Adv-Trained & 72.1 & 58.3 & 45.1 & 22.6 \\
        \bottomrule
    \end{tabular}
\end{table}

The numbers here are striking. At $\epsilon = 8/255$, PGD reduces the standard model to 1.2\% accuracy --- essentially zero. Compare that to 18.3\% under FGSM: the single-step attack substantially underestimates the model's true vulnerability. The adversarially trained model holds at 45.1\% under PGD, a 43.9 percentage point improvement over the standard model. Even at $\epsilon = 16/255$, where the standard model is at 0.1\%, the robust model still achieves 22.6\%. Figure~\ref{fig:attack_pgd} illustrates the PGD attack on a sample image.

\subsection{DeepFool Attack Results}

DeepFool operates differently --- it finds the minimal perturbation rather than perturbing within a fixed budget. Results are in Table~\ref{tab:deepfool_results}.

\begin{table}[htbp]
    \centering
    \caption{DeepFool attack results on CIFAR-10}
    \label{tab:deepfool_results}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{Success Rate (\%)} & \textbf{Avg. $L_2$ Perturbation} & \textbf{Avg. $L_\infty$ Perturbation} \\
        \midrule
        Standard & 91.1 & 0.248 & 0.021 \\
        Adv-Trained & 62.0 & 0.892 & 0.074 \\
        \bottomrule
    \end{tabular}
\end{table}

Against the standard model, DeepFool succeeds on 91.1\% of correctly classified images with an average $L_2$ perturbation of just 0.248 (Figure~\ref{fig:attack_deepfool}). That number is remarkably small. Against the adversarially trained model, it requires roughly $3.6\times$ larger perturbations on average and only reaches 62.0\% success. The decision boundaries have clearly moved further from the data after adversarial training.

\subsection{Visual Examples}

Figure~\ref{fig:adversarial_examples} shows adversarial examples on a CIFAR-10 cat image. At $\epsilon = 8/255$, the visual difference between the original and adversarial images is imperceptible to the eye. Yet the standard model misclassifies with near-100\% confidence, while the adversarially trained model still gets it right.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/FGSM.png}
        \caption{FGSM attack ($\epsilon = 8/255$): the standard model is fooled with near-100\% confidence on the wrong class, while the robust model correctly identifies the cat with approximately 52\% confidence.}
        \label{fig:attack_fgsm}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/PGD.png}
        \caption{PGD attack ($\epsilon = 8/255$, 20 steps): the stronger iterative attack also fools the standard model with near-100\% confidence, while the robust model maintains the correct prediction.}
        \label{fig:attack_pgd}
    \end{subfigure}

    \caption{Visual comparison of FGSM and PGD adversarial attacks on a CIFAR-10 cat image. Each panel shows the original image, the adversarial perturbation (amplified for visibility), the adversarial image, and the predictions of both the standard model (fooled) and the robust model (correct). The perturbations are imperceptible to human observers yet cause confident misclassification in the standard model.}
    \label{fig:adversarial_examples}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/DeepFool.png}
    \caption{DeepFool attack on the same CIFAR-10 cat image: finds the minimal perturbation to cross the decision boundary. The standard model is fooled, while the robust model resists the minimal perturbation. Unlike FGSM and PGD (Figure~\ref{fig:adversarial_examples}), DeepFool does not use a fixed $\epsilon$ budget but instead computes the smallest perturbation needed to change the prediction.}
    \label{fig:attack_deepfool}
\end{figure}

An interesting observation from the demo: confidence scores. When the standard model is fooled, it does not just give a wrong answer --- it gives a wrong answer with 99\% confidence. The adversarially trained model behaves quite differently. Even on inputs where it predicts correctly, it tends to output something like 50--60\% confidence rather than near-100\%. In a sense, the robust model ``knows'' it is being challenged. This calibration property turns out to be practically useful --- I discuss it further below.

\section{Adversarial Training Results}

\subsection{Training Dynamics}

Standard training reached 94.1\% test accuracy after 50 epochs. Training accuracy hit around 98\%, and loss decreased steadily from about 1.55 to 0.034. No obvious signs of overfitting --- test accuracy continued improving throughout.

Adversarial training showed a different pattern. The training loss remained elevated compared to standard training, which makes sense since the model sees adversarial (i.e., harder) examples at every step. Clean test accuracy settled around 83.9\%. PGD robustness accuracy --- which I used for checkpoint selection --- peaked at 50.6\%.

\subsection{PGD Adversarial Training on CIFAR-10}

Table~\ref{tab:adv_training_comparison} gives the full comparison at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comprehensive comparison at $\epsilon = 8/255$ on CIFAR-10}
    \label{tab:adv_training_comparison}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Clean} & \textbf{FGSM} & \textbf{PGD-20} & \textbf{DeepFool} & \textbf{Avg. Robust} \\
        \midrule
        Standard & 94.1 & 18.3 & 1.2 & 8.9 & 9.5 \\
        Adv-Trained & 83.9 & 56.0 & 45.1 & 38.0 & 46.4 \\
        \midrule
        Improvement & $-10.2$ & $+37.7$ & $+43.9$ & $+29.1$ & $+36.9$ \\
        \bottomrule
    \end{tabular}
\end{table}

The trade-off is clear: 10.2 percentage points of clean accuracy buys an average of 36.9 percentage points across three attacks. The largest gain is against PGD ($+43.9$ pp), which is expected --- PGD is the attack used during training. But the gains transfer to the other attacks as well.

\subsection{Accuracy Trade-offs}
\label{sec:tradeoffs}

A 10.2\% drop in clean accuracy is real, and it is not something to dismiss. But the trade-off is well understood in the literature. Adversarial training forces the model to rely on features that are stable under worst-case perturbations, and these are not necessarily the same features that maximize clean-data performance. Whether the trade-off is acceptable depends on the deployment context. For a safety-critical application, a model that consistently classifies at 84\% under all conditions is arguably more useful than one that achieves 94\% on clean data but collapses to 1\% when attacked.

\subsection{Computational Cost}

Adversarial training costs roughly $7\times$ more computation per epoch. On my Apple M4 Pro, 50 epochs of standard training took about 55 minutes; adversarial training took about 4.5 hours. The overhead comes directly from running 7-step PGD on every mini-batch --- seven extra forward and backward passes per batch. For CIFAR-10 with ResNet-18 this is manageable, but it would become a significant bottleneck at larger scales.

\section{Attack Comparison}

\subsection{Success Rate and Strength Comparison}

Table~\ref{tab:attack_comparison} compares all three attacks against the standard model at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comparison of attack methods on CIFAR-10 standard model ($\epsilon = 8/255$)}
    \label{tab:attack_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Attack} & \textbf{Robust Acc. (\%)} & \textbf{Avg. $L_2$} & \textbf{Avg. $L_\infty$} & \textbf{Time per batch (ms)} \\
        \midrule
        FGSM & 18.3 & 1.351 & 0.031 & 12 \\
        PGD-20 & 1.2 & 1.351 & 0.031 & 185 \\
        DeepFool & 8.9 & 0.248 & 0.021 & 2,450 \\
        \bottomrule
    \end{tabular}
\end{table}

PGD-20 is the strongest attack by a wide margin, bringing the standard model to 1.2\% --- this is why it has become the standard evaluation method for adversarial robustness~\cite{madry2018towards}. Note that FGSM and PGD use the same $L_\infty$ budget ($0.031$), but PGD finds much more damaging perturbations within that budget through its iterative procedure. DeepFool lands between the two in attack strength (8.9\% accuracy) but uses substantially smaller perturbations ($L_2 = 0.248$ vs.\ 1.351). This highlights its purpose: finding the \emph{minimum} perturbation needed, not the most damaging one within a fixed budget.

The computation times reflect the algorithmic differences: FGSM needs one gradient computation, PGD needs 20, and DeepFool processes each image individually with per-class gradients at every iteration.

\subsection{Perturbation Analysis}

The perturbation comparison reveals something important about the standard model's geometry. FGSM and PGD both saturate the $\epsilon$ budget --- they perturb every pixel by exactly $\epsilon$ in the signed gradient direction. DeepFool, by contrast, uses perturbations that are $5.4\times$ smaller in $L_2$ norm, and yet it still fools 91.1\% of images. This tells us that the decision boundaries of the standard model lie very close to the data manifold. The average robustness radius is just 0.248 in $L_2$ distance --- not much room at all.

For the adversarially trained model, DeepFool needs perturbations averaging 0.892 in $L_2$. So adversarial training does exactly what it should: it pushes the decision boundaries away from the data.

\section{Discussion}

\subsection{Key Findings}

Four main takeaways from these experiments:

First, standard deep learning models are extraordinarily fragile. A ResNet-18 with 94.1\% clean accuracy drops to 1.2\% under a 20-step PGD attack at $\epsilon = 8/255$. The perturbation is imperceptible to humans. This is, by any measure, a near-complete failure of the model.

Second, evaluating robustness with FGSM alone is misleading. FGSM gives 18.3\% accuracy, which paints a much rosier picture than the 1.2\% that PGD reveals. Anyone relying solely on FGSM evaluations would significantly overestimate how safe their model is.

Third, PGD-based adversarial training provides real robustness gains. Robust accuracy under PGD goes from near-zero to 45.1\%, at the cost of 10.2 percentage points of clean accuracy. These numbers are consistent with what Madry et al.~\cite{madry2018towards} reported.

Fourth, adversarial training affects confidence calibration in a useful way. The standard model assigns near-100\% confidence to its wrong predictions on adversarial inputs. The robust model's confidence outputs are more spread out, more reflective of actual uncertainty. In deployment scenarios, knowing when a model is uncertain is often as valuable as the prediction itself.

\subsection{Comparison with Literature}

My results align well with published benchmarks. Madry et al.~\cite{madry2018towards} reported 45--47\% PGD robustness at $\epsilon = 8/255$ for adversarially trained ResNet on CIFAR-10; I obtained 45.1\%. Clean accuracy of 83.9\% falls within the 82--87\% range typically reported in the literature, with variation depending on exact training details.

The DeepFool results are likewise consistent with the original work by Moosavi-Dezfooli et al.~\cite{moosavi2016deepfool}, who demonstrated that standard networks have small robustness radii under minimal-perturbation attacks.

\subsection{Limitations and Observations}

Several limitations should be noted. The experiments are restricted to CIFAR-10, which has small $32 \times 32$ images; the same $\epsilon$ budget may have different perceptual effects at higher resolutions. Only one architecture was tested (ResNet-18), and others could behave differently. For PGD evaluation I used a single random restart --- multiple restarts would give a tighter upper bound on vulnerability. And I only considered the $L_\infty$ threat model for FGSM and PGD; the $L_2$ setting could yield different conclusions.

\section{Summary}

The experiments paint a clear picture: standard deep neural networks are severely vulnerable to adversarial perturbations, and PGD is the most effective first-order attack for exposing this vulnerability. Adversarial training recovers a meaningful degree of robustness at the expense of some clean accuracy, and the numbers I obtained match published benchmarks. These results underscore the importance of robustness evaluation in deep learning and motivate the directions for future work discussed in the next chapter.
