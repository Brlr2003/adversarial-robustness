\chapter{RESULTS AND DISCUSSION}
\label{ch:results}

\section{Introduction}

In this chapter we present the results of our experiments. We evaluate all three attacks on CIFAR-10, report the effect of adversarial training, and compare the attacks under identical conditions. All experiments used the framework described in Chapter~\ref{ch:implementation}, with results tracked via MLflow.

\section{Experimental Setup}

\subsection{Training Configuration}

We trained two ResNet-18 models on CIFAR-10: one with standard training and one with PGD adversarial training. Both models used identical hyperparameters except for the adversarial training component. Table~\ref{tab:training_config} lists the training configuration.

\begin{table}[htbp]
    \centering
    \caption{Training hyperparameters for standard and adversarial training}
    \label{tab:training_config}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Standard Training} & \textbf{Adversarial Training} \\
        \midrule
        Batch size & 128 & 128 \\
        Optimizer & SGD & SGD \\
        Learning rate & 0.1 & 0.1 \\
        Momentum & 0.9 & 0.9 \\
        Weight decay & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
        LR schedule & Cosine annealing & Cosine annealing \\
        Epochs & 50 & 50 \\
        Data augmentation & RandomCrop, HFlip & RandomCrop, HFlip \\
        \bottomrule
    \end{tabular}
\end{table}

Standard training took approximately 55 minutes. Adversarial training took approximately 4.5 hours, roughly $7\times$ slower due to the PGD computation at each training step.

\subsection{Attack Parameters}

Table~\ref{tab:attack_params} lists the attack configurations used during evaluation.

\begin{table}[htbp]
    \centering
    \caption{Attack parameters used during evaluation}
    \label{tab:attack_params}
    \begin{tabular}{lp{8cm}}
        \toprule
        \textbf{Attack} & \textbf{Parameters} \\
        \midrule
        FGSM & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$ \\
        PGD & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$, step size $\alpha = \epsilon/4$, iterations $T = 20$, random start \\
        DeepFool & Max iterations = 50, overshoot = 0.02, 10 classes \\
        \bottomrule
    \end{tabular}
\end{table}

For the inner PGD attack during adversarial training, we used $\epsilon = 8/255$, $\alpha = 2/255$, 7 iterations, and random start, following Madry et al.~\cite{madry2018towards}.

\subsection{Evaluation Protocol}

All evaluations were performed on the full CIFAR-10 test set (10,000 images). PGD used one random restart, which is standard for evaluating adversarially trained models. Attack success rates were computed only over images that the model classified correctly on clean data.

We also built an interactive web demo on HuggingFace Spaces (Figure~\ref{fig:demo_interface}) that allows comparison of both models under any attack and $\epsilon$ value.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/stats.png}
    \caption{Interactive demo interface deployed on HuggingFace Spaces, showing model accuracy metrics and attack configuration controls for real-time adversarial robustness evaluation.}
    \label{fig:demo_interface}
\end{figure}

\section{Baseline Results}

\subsection{Model Performance on Clean Data}

Table~\ref{tab:clean_accuracy} reports the clean test accuracy of both models.

\begin{table}[htbp]
    \centering
    \caption{Clean accuracy of trained models on CIFAR-10}
    \label{tab:clean_accuracy}
    \begin{tabular}{lc}
        \toprule
        \textbf{Model} & \textbf{Clean Accuracy (\%)} \\
        \midrule
        Standard ResNet-18 & 94.1 \\
        Adversarially Trained ResNet-18 & 83.9 \\
        \bottomrule
    \end{tabular}
\end{table}

The standard model achieves 94.1\%, which is consistent with published results for ResNet-18 on CIFAR-10 without input normalization. The adversarially trained model achieves 83.9\%, a reduction of 10.2 percentage points. This trade-off is consistent with prior work~\cite{madry2018towards} and is discussed further in Section~\ref{sec:tradeoffs}.

\section{Attack Evaluation Results}

\subsection{FGSM Attack Results}

Table~\ref{tab:fgsm_results} reports robust accuracy under FGSM at varying $\epsilon$.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under FGSM attack on CIFAR-10}
    \label{tab:fgsm_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 68.2 & 42.5 & 18.3 & 5.1 \\
        Adv-Trained & 76.8 & 68.4 & 56.0 & 38.2 \\
        \bottomrule
    \end{tabular}
\end{table}

We observe that the standard model drops from 94.1\% to 18.3\% at $\epsilon = 8/255$. The adversarially trained model retains 56.0\% accuracy, a gain of 37.7 percentage points. Figure~\ref{fig:attack_fgsm} shows visual examples.

\subsection{PGD Attack Results}

Table~\ref{tab:pgd_results} reports results under PGD-20. This is the stronger evaluation.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under PGD-20 attack on CIFAR-10}
    \label{tab:pgd_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 41.3 & 12.8 & 1.2 & 0.1 \\
        Adv-Trained & 72.1 & 58.3 & 45.1 & 22.6 \\
        \bottomrule
    \end{tabular}
\end{table}

We find that the standard model retains only 1.2\% accuracy at $\epsilon = 8/255$ under PGD, compared to 18.3\% under FGSM. The single-step attack gives a misleading picture of the model's actual vulnerability. The adversarially trained model retains 45.1\% at the same $\epsilon$. At $\epsilon = 16/255$, the standard model is reduced to 0.1\% (effectively random), while the adversarially trained model still achieves 22.6\%. See Figure~\ref{fig:attack_pgd}.

\subsection{DeepFool Attack Results}

DeepFool finds the minimum perturbation per image rather than operating within a fixed budget. Table~\ref{tab:deepfool_results} reports the results.

\begin{table}[htbp]
    \centering
    \caption{DeepFool attack results on CIFAR-10}
    \label{tab:deepfool_results}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{Success Rate (\%)} & \textbf{Avg. $L_2$ Perturbation} & \textbf{Avg. $L_\infty$ Perturbation} \\
        \midrule
        Standard & 91.1 & 0.248 & 0.021 \\
        Adv-Trained & 62.0 & 0.892 & 0.074 \\
        \bottomrule
    \end{tabular}
\end{table}

DeepFool fools 91.1\% of correctly classified images on the standard model with an average $L_2$ perturbation of only 0.248. This indicates that the decision boundaries of the standard model are very close to the data points. For the adversarially trained model, DeepFool requires approximately $3.6\times$ larger perturbations and achieves a 62.0\% success rate. The decision boundaries have moved further from the data. See Figure~\ref{fig:attack_deepfool}.

\subsection{Visual Examples}

Figure~\ref{fig:adversarial_examples} shows a CIFAR-10 image attacked with FGSM and PGD at $\epsilon = 8/255$. The perturbation is imperceptible, yet the standard model changes its prediction with high confidence.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/FGSM.png}
        \caption{FGSM attack ($\epsilon = 8/255$): the standard model is fooled with near-100\% confidence on the wrong class, while the robust model correctly identifies the cat with approximately 52\% confidence.}
        \label{fig:attack_fgsm}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/PGD.png}
        \caption{PGD attack ($\epsilon = 8/255$, 20 steps): the stronger iterative attack also fools the standard model with near-100\% confidence, while the robust model maintains the correct prediction.}
        \label{fig:attack_pgd}
    \end{subfigure}

    \caption{Visual comparison of FGSM and PGD adversarial attacks on a CIFAR-10 cat image. Each panel shows the original image, the adversarial perturbation (amplified for visibility), the adversarial image, and the predictions of both the standard model (fooled) and the robust model (correct). The perturbations are imperceptible to human observers yet cause confident misclassification in the standard model.}
    \label{fig:adversarial_examples}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/DeepFool.png}
    \caption{DeepFool attack on the same CIFAR-10 cat image: finds the minimal perturbation to cross the decision boundary. The standard model is fooled, while the robust model resists the minimal perturbation. Unlike FGSM and PGD (Figure~\ref{fig:adversarial_examples}), DeepFool does not use a fixed $\epsilon$ budget but instead computes the smallest perturbation needed to change the prediction.}
    \label{fig:attack_deepfool}
\end{figure}

We also note a pattern in the confidence scores. The standard model, when fooled, outputs near-100\% confidence on the wrong class. It does not indicate any uncertainty. The adversarially trained model, by contrast, tends to produce confidence scores in the 50--60\% range even on correct predictions. This suggests that adversarial training produces better-calibrated confidence outputs --- the model assigns lower confidence when the input is ambiguous or perturbed.

\section{Adversarial Training Results}

\subsection{Training Dynamics}

For standard training, the loss decreased from approximately 1.55 to 0.034 over 50 epochs. Training accuracy reached 98\% and test accuracy settled at 94.1\%. We observed no overfitting.

Adversarial training exhibited different dynamics. The training loss remained higher throughout, which is expected since the model is trained on adversarial examples that are harder to classify. Clean test accuracy settled at 83.9\%. PGD robust accuracy, which we used for checkpoint selection, peaked at 50.6\%.

\subsection{PGD Adversarial Training on CIFAR-10}

Table~\ref{tab:adv_training_comparison} compares the two models at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comprehensive comparison at $\epsilon = 8/255$ on CIFAR-10}
    \label{tab:adv_training_comparison}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Clean} & \textbf{FGSM} & \textbf{PGD-20} & \textbf{DeepFool} & \textbf{Avg. Robust} \\
        \midrule
        Standard & 94.1 & 18.3 & 1.2 & 8.9 & 9.5 \\
        Adv-Trained & 83.9 & 56.0 & 45.1 & 38.0 & 46.4 \\
        \midrule
        Improvement & $-10.2$ & $+37.7$ & $+43.9$ & $+29.1$ & $+36.9$ \\
        \bottomrule
    \end{tabular}
\end{table}

The adversarially trained model loses 10.2 percentage points of clean accuracy but gains an average of 36.9 points across the three robustness metrics. The largest gain is on PGD (+43.9 pp), which is expected since PGD is the attack used during training. The gains on FGSM and DeepFool indicate that robustness transfers across attack methods.

\subsection{Accuracy Trade-offs}
\label{sec:tradeoffs}

The 10.2 percentage point reduction in clean accuracy is consistent with known results on the accuracy--robustness trade-off. We remark that the practical question is whether a model with 94.1\% clean accuracy and 1.2\% robust accuracy is preferable to one with 83.9\% clean accuracy and 45.1\% robust accuracy. For safety-critical applications, the latter is clearly more appropriate.

The accuracy reduction arises because adversarial training encourages the model to learn features that are stable under worst-case perturbations, and these features are not necessarily the same ones that maximize clean accuracy. There is a genuine tension between these two objectives, and the 10.2 percentage point cost reflects this tension on CIFAR-10 with ResNet-18.

\subsection{Computational Cost}

Adversarial training is approximately $7\times$ slower per epoch than standard training: 55 minutes versus 4.5 hours for 50 epochs. This is because 7-step PGD at each training batch requires 7 additional forward and backward passes. On our M4 Pro hardware this was manageable for CIFAR-10, but it would become a bottleneck for larger datasets and models.

\section{Attack Comparison}

\subsection{Success Rate and Strength Comparison}

Table~\ref{tab:attack_comparison} compares all three attacks on the standard model at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comparison of attack methods on CIFAR-10 standard model ($\epsilon = 8/255$)}
    \label{tab:attack_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Attack} & \textbf{Robust Acc. (\%)} & \textbf{Avg. $L_2$} & \textbf{Avg. $L_\infty$} & \textbf{Time per batch (ms)} \\
        \midrule
        FGSM & 18.3 & 1.351 & 0.031 & 12 \\
        PGD-20 & 1.2 & 1.351 & 0.031 & 185 \\
        DeepFool & 8.9 & 0.248 & 0.021 & 2,450 \\
        \bottomrule
    \end{tabular}
\end{table}

PGD is the strongest attack, reducing accuracy to 1.2\% compared to 18.3\% for FGSM and 8.9\% for DeepFool. We note that FGSM and PGD operate under the same $L_\infty$ budget, but PGD finds stronger perturbations within that budget through iteration. DeepFool uses substantially less perturbation ($L_2 = 0.248$ versus 1.351) because it seeks the minimum perturbation rather than maximum damage within a fixed budget.

In terms of computational cost, FGSM requires 12 ms per batch, PGD requires 185 ms (20 gradient steps), and DeepFool requires 2,450 ms due to per-image processing with per-class gradients. PGD offers the best trade-off between attack strength and computational cost for evaluation purposes. DeepFool provides complementary information (the robustness radius) but is too slow for routine evaluation.

\subsection{Perturbation Analysis}

FGSM and PGD both saturate the $\epsilon$ budget, perturbing every pixel by exactly $\epsilon$ in the gradient direction. DeepFool, by contrast, uses $5.4\times$ less $L_2$ perturbation on average while still fooling 91.1\% of images. This indicates that the decision boundaries of the standard model lie very close to the data --- the average distance from a data point to the nearest wrong-class boundary is 0.248 in $L_2$.

After adversarial training, DeepFool requires an average of 0.892 $L_2$ perturbation, approximately $3.6\times$ larger. This confirms that adversarial training moves the decision boundaries further from the data.

\section{Discussion}

\subsection{Key Findings}

We summarize our main findings:

First, the standard model is highly vulnerable. It achieves 94.1\% clean accuracy but only 1.2\% under PGD at $\epsilon = 8/255$, with imperceptible perturbations.

Second, single-step attacks underestimate the true vulnerability. FGSM gives 18.3\% robust accuracy where PGD gives 1.2\%. Evaluating with FGSM alone would give a misleading picture of the model's actual robustness.

Third, adversarial training provides real robustness. PGD robust accuracy increases from 1.2\% to 45.1\%, at a cost of 10.2 percentage points of clean accuracy. Our numbers are consistent with Madry et al.~\cite{madry2018towards}, which provides confidence that the implementation is correct.

Fourth, adversarial training improves confidence calibration. The standard model assigns near-100\% confidence to incorrect predictions under attack. The adversarially trained model produces more distributed confidence scores, which is useful for detecting unreliable predictions.

\subsection{Comparison with Literature}

Our results are consistent with published numbers. Madry et al.~\cite{madry2018towards} reported 45--47\% PGD robust accuracy at $\epsilon = 8/255$ on CIFAR-10 with adversarial training; we obtain 45.1\%. Our clean accuracy of 83.9\% falls within the 82--87\% range reported in the literature. The DeepFool robustness radii are consistent with Moosavi-Dezfooli et al.~\cite{moosavidezfooli2016deepfool}, confirming that standard networks have small margins around the data.

\subsection{Limitations and Observations}

We note several limitations. Our experiments are restricted to CIFAR-10 ($32 \times 32$ images), and the same $\epsilon$ budget has different perceptual meaning at higher resolutions. We tested only ResNet-18; other architectures may exhibit different robustness properties. We used one PGD random restart; more restarts would provide tighter bounds on vulnerability. We evaluated only $L_\infty$-bounded attacks; $L_2$-bounded attacks may yield different conclusions.

\section{Summary}

We find that standard models are highly vulnerable to adversarial perturbations, and that PGD reveals this vulnerability more accurately than FGSM. Adversarial training provides genuine robustness at the cost of clean accuracy. Our results are consistent with the literature, and the comparison across three attack methods provides a unified view of the accuracy--robustness trade-off.

