\chapter{RESULTS AND DISCUSSION}
\label{ch:results}

\section{Introduction}

This chapter presents the results from my experiments. I evaluated the three adversarial attacks on CIFAR-10 using ResNet-18 models, measured how vulnerable the standard model is, tested how well PGD-based adversarial training works as a defense, and compared the attacks against each other. Everything was run using the framework I described in Chapter~\ref{ch:implementation}, and I tracked all experiments with MLflow to keep things reproducible.

\section{Experimental Setup}

\subsection{Training Configuration}

I trained two ResNet-18 models on CIFAR-10: one with standard training and one with adversarial training. Both used the same base architecture and optimizer settings, which are summarized in Table~\ref{tab:training_config}.

\begin{table}[htbp]
    \centering
    \caption{Training hyperparameters for standard and adversarial training}
    \label{tab:training_config}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Standard Training} & \textbf{Adversarial Training} \\
        \midrule
        Batch size & 128 & 128 \\
        Optimizer & SGD & SGD \\
        Learning rate & 0.1 & 0.1 \\
        Momentum & 0.9 & 0.9 \\
        Weight decay & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
        LR schedule & Cosine annealing & Cosine annealing \\
        Epochs & 50 & 50 \\
        Data augmentation & RandomCrop, HFlip & RandomCrop, HFlip \\
        \bottomrule
    \end{tabular}
\end{table}

Standard training finished in about 55 minutes, while adversarial training took roughly 4.5 hours, which is about $7\times$ longer because of the overhead from generating PGD adversarial examples at every training step.

\subsection{Attack Parameters}

Table~\ref{tab:attack_params} summarizes the parameters used for each attack during evaluation.

\begin{table}[htbp]
    \centering
    \caption{Attack parameters used during evaluation}
    \label{tab:attack_params}
    \begin{tabular}{lp{8cm}}
        \toprule
        \textbf{Attack} & \textbf{Parameters} \\
        \midrule
        FGSM & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$ \\
        PGD & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$, step size $\alpha = \epsilon/4$, iterations $T = 20$, random start \\
        DeepFool & Max iterations = 50, overshoot = 0.02, 10 classes \\
        \bottomrule
    \end{tabular}
\end{table}

For adversarial training, PGD was used as the inner attack with $\epsilon = 8/255$, step size $\alpha = 2/255$, and 7 iterations with random start, following the configuration recommended by Madry et al.~\cite{madry2018towards}.

\subsection{Evaluation Protocol}

I evaluated all attacks on the full CIFAR-10 test set (10,000 images). For PGD I used a single random restart during evaluation, which is the standard practice when benchmarking against adversarial training. Attack success rates were computed based on samples that the model classified correctly on clean data.

Figure~\ref{fig:demo_interface} shows the interactive demo I deployed on HuggingFace Spaces, where you can see the model performance metrics and compare the standard and adversarially trained models under different attack settings in real time.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/stats.png}
    \caption{Interactive demo interface deployed on HuggingFace Spaces, showing model accuracy metrics and attack configuration controls for real-time adversarial robustness evaluation.}
    \label{fig:demo_interface}
\end{figure}

\section{Baseline Results}

\subsection{Model Performance on Clean Data}

Table~\ref{tab:clean_accuracy} reports the classification accuracy of both models on clean (unperturbed) test data.

\begin{table}[htbp]
    \centering
    \caption{Clean accuracy of trained models on CIFAR-10}
    \label{tab:clean_accuracy}
    \begin{tabular}{lc}
        \toprule
        \textbf{Model} & \textbf{Clean Accuracy (\%)} \\
        \midrule
        Standard ResNet-18 & 94.1 \\
        Adversarially Trained ResNet-18 & 83.9 \\
        \bottomrule
    \end{tabular}
\end{table}

The standard model got 94.1\% clean accuracy, which lines up with what others have reported for ResNet-18 on CIFAR-10 without input normalization. The adversarially trained model came in at 83.9\%, so there is a 10.2 percentage point drop. This kind of accuracy loss is a well-known side effect of adversarial training~\cite{madry2018towards}, and I discuss it more in Section~\ref{sec:tradeoffs}.

\section{Attack Evaluation Results}

\subsection{FGSM Attack Results}

Table~\ref{tab:fgsm_results} shows the robust accuracy of both models under FGSM attacks at varying perturbation budgets.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under FGSM attack on CIFAR-10}
    \label{tab:fgsm_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 68.2 & 42.5 & 18.3 & 5.1 \\
        Adv-Trained & 76.8 & 68.4 & 56.0 & 38.2 \\
        \bottomrule
    \end{tabular}
\end{table}

The standard model's accuracy drops fast under FGSM, going from 94.1\% all the way down to 18.3\% at the standard benchmark perturbation of $\epsilon = 8/255$. The adversarially trained model does much better, holding on to 56.0\% accuracy at the same level, which is a 37.7 percentage point improvement. Figure~\ref{fig:attack_fgsm} shows what this looks like visually on a sample image.

\subsection{PGD Attack Results}

Table~\ref{tab:pgd_results} presents robust accuracy under PGD-20 attacks, which represent a stronger evaluation than FGSM due to the iterative optimization.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under PGD-20 attack on CIFAR-10}
    \label{tab:pgd_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 41.3 & 12.8 & 1.2 & 0.1 \\
        Adv-Trained & 72.1 & 58.3 & 45.1 & 22.6 \\
        \bottomrule
    \end{tabular}
\end{table}

PGD is much more effective than FGSM against the standard model, as you can see in Figure~\ref{fig:attack_pgd}. At $\epsilon = 8/255$, the standard model only has 1.2\% accuracy left under PGD, compared to 18.3\% under FGSM. This really shows that single-step attacks like FGSM underestimate how vulnerable the model actually is. The adversarially trained model keeps 45.1\% accuracy under PGD at $\epsilon = 8/255$, which is a 43.9 percentage point improvement. At the strongest perturbation of $\epsilon = 16/255$, the standard model is basically completely fooled (0.1\%), while the robust model still gets 22.6\% right.

\subsection{DeepFool Attack Results}

DeepFool works differently from the other two attacks. Instead of working within a fixed perturbation budget, it finds the smallest perturbation needed to change each prediction. The results are in Table~\ref{tab:deepfool_results}.

\begin{table}[htbp]
    \centering
    \caption{DeepFool attack results on CIFAR-10}
    \label{tab:deepfool_results}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{Success Rate (\%)} & \textbf{Avg. $L_2$ Perturbation} & \textbf{Avg. $L_\infty$ Perturbation} \\
        \midrule
        Standard & 91.1 & 0.248 & 0.021 \\
        Adv-Trained & 62.0 & 0.892 & 0.074 \\
        \bottomrule
    \end{tabular}
\end{table}

DeepFool manages to fool 91.1\% of the correctly classified images for the standard model (see Figure~\ref{fig:attack_deepfool}), with an average $L_2$ perturbation of just 0.248. For the adversarially trained model, the perturbations need to be about $3.6\times$ larger on average to get a misclassification, and the success rate drops to 62.0\%. This tells us that adversarial training is genuinely pushing the decision boundaries further away from the data points, giving the model a larger robustness radius.

\subsection{Visual Examples}

Figure~\ref{fig:adversarial_examples} shows what adversarial attacks actually look like on CIFAR-10 images. Even at $\epsilon = 8/255$, you really cannot tell the difference between the original and adversarial images just by looking at them. But the standard model gets completely fooled and is very confident about its wrong answer. The adversarially trained model, on the other hand, still gets it right and gives more reasonable confidence scores.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/FGSM.png}
        \caption{FGSM attack ($\epsilon = 8/255$): the standard model is fooled with near-100\% confidence on the wrong class, while the robust model correctly identifies the cat with approximately 52\% confidence.}
        \label{fig:attack_fgsm}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/PGD.png}
        \caption{PGD attack ($\epsilon = 8/255$, 20 steps): the stronger iterative attack also fools the standard model with near-100\% confidence, while the robust model maintains the correct prediction.}
        \label{fig:attack_pgd}
    \end{subfigure}

    \caption{Visual comparison of FGSM and PGD adversarial attacks on a CIFAR-10 cat image. Each panel shows the original image, the adversarial perturbation (amplified for visibility), the adversarial image, and the predictions of both the standard model (fooled) and the robust model (correct). The perturbations are imperceptible to human observers yet cause confident misclassification in the standard model.}
    \label{fig:adversarial_examples}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/DeepFool.png}
    \caption{DeepFool attack on the same CIFAR-10 cat image: finds the minimal perturbation to cross the decision boundary. The standard model is fooled, while the robust model resists the minimal perturbation. Unlike FGSM and PGD (Figure~\ref{fig:adversarial_examples}), DeepFool does not use a fixed $\epsilon$ budget but instead computes the smallest perturbation needed to change the prediction.}
    \label{fig:attack_deepfool}
\end{figure}

One thing that stood out while using the interactive demo is how different the confidence levels are between the two models. The standard model often gives near-100\% confidence to its wrong predictions on adversarial inputs, while the adversarially trained model spreads its probabilities out more, typically giving around 50 to 60\% confidence. This suggests that adversarial training does not just improve robustness but also makes the model more honest about when it is uncertain.

\section{Adversarial Training Results}

\subsection{Training Dynamics}

Standard training reached 94.1\% test accuracy after 50 epochs, with training accuracy hitting about 98\%. The training loss went down steadily from around 1.55 in the first epoch to 0.034 by the end, and there was no obvious overfitting since the test accuracy kept improving throughout.

Adversarial training behaved quite differently. The training loss stayed higher than standard training because the model is being trained on adversarial examples, which are harder. Clean test accuracy settled around 83.9\%, and PGD robustness accuracy (which is what I used to select the best checkpoint) peaked at 50.6\%.

\subsection{PGD Adversarial Training on CIFAR-10}

Table~\ref{tab:adv_training_comparison} provides a comprehensive comparison of the standard and adversarially trained models across all attack types at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comprehensive comparison at $\epsilon = 8/255$ on CIFAR-10}
    \label{tab:adv_training_comparison}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Clean} & \textbf{FGSM} & \textbf{PGD-20} & \textbf{DeepFool} & \textbf{Avg. Robust} \\
        \midrule
        Standard & 94.1 & 18.3 & 1.2 & 8.9 & 9.5 \\
        Adv-Trained & 83.9 & 56.0 & 45.1 & 38.0 & 46.4 \\
        \midrule
        Improvement & $-10.2$ & $+37.7$ & $+43.9$ & $+29.1$ & $+36.9$ \\
        \bottomrule
    \end{tabular}
\end{table}

The adversarially trained model gives up 10.2 percentage points of clean accuracy but picks up an average of 36.9 percentage points in robust accuracy across all three attacks. The biggest improvement is against PGD (+43.9 pp), which makes sense since PGD is the attack that was used during training.

\subsection{Accuracy Trade-offs}
\label{sec:tradeoffs}

Losing 10.2\% in clean accuracy is not trivial, but it is a well-understood cost of adversarial training. The trade-off happens because the model is forced to learn features that can handle worst-case perturbations, and those are not always the same features that give you the best clean accuracy. In practice, if you are deploying a model in a safety-critical setting, a model that correctly classifies 84\% of images under all conditions is more useful than one that gets 94\% on clean data but falls apart completely when attacked.

\subsection{Computational Cost}

Adversarial training used about $7\times$ more computation per epoch than standard training. On my Apple M4 Pro, 50 epochs of standard training on CIFAR-10 with ResNet-18 took about 55 minutes, while adversarial training took about 4.5 hours. The extra time comes from running 7-step PGD at every training batch, which means 7 additional forward and backward passes per batch. This is a real practical limitation, especially if you want to scale up to larger models or datasets.

\section{Attack Comparison}

\subsection{Success Rate and Strength Comparison}

Table~\ref{tab:attack_comparison} provides a detailed comparison of the three attacks against the standard model at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comparison of attack methods on CIFAR-10 standard model ($\epsilon = 8/255$)}
    \label{tab:attack_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Attack} & \textbf{Robust Acc. (\%)} & \textbf{Avg. $L_2$} & \textbf{Avg. $L_\infty$} & \textbf{Time per batch (ms)} \\
        \midrule
        FGSM & 18.3 & 1.351 & 0.031 & 12 \\
        PGD-20 & 1.2 & 1.351 & 0.031 & 185 \\
        DeepFool & 8.9 & 0.248 & 0.021 & 2,450 \\
        \bottomrule
    \end{tabular}
\end{table}

PGD-20 is clearly the strongest attack, bringing the standard model down to 1.2\% accuracy compared to 18.3\% for FGSM and 8.9\% for DeepFool. This is why PGD is considered the go-to attack for robustness evaluation~\cite{madry2018towards}. Both FGSM and PGD use the full $\epsilon$ budget ($L_\infty = 0.031$), so they end up with the same maximum perturbation size, but PGD's iterative approach finds much more damaging perturbations within that same budget. DeepFool falls in between in terms of attack strength but uses much smaller perturbations ($L_2 = 0.248$ vs.\ 1.351), which shows its ability to find near-minimal perturbations.

The computation time differences make sense given the algorithms: FGSM needs just one forward-backward pass, PGD needs 20, and DeepFool processes each image individually with gradient computations for all classes at every iteration.

\subsection{Perturbation Analysis}

Something I found interesting is how the perturbation sizes compare across attacks. FGSM and PGD both use the full $\epsilon$ budget, perturbing every pixel by exactly $\epsilon$ in the signed gradient direction. DeepFool, on the other hand, produces perturbations that are $5.4\times$ smaller in $L_2$ norm on average, and yet it still manages a 91.1\% success rate against the standard model. What this tells us is that the standard model's decision boundaries are really close to the data points. The average robustness radius is only 0.248 in $L_2$ distance.

For the adversarially trained model, DeepFool needs perturbations averaging 0.892 in $L_2$, which confirms that adversarial training does push the decision boundaries much further from the data.

\section{Discussion}

\subsection{Key Findings}

Here are the main things I took away from the experiments:

First, standard deep learning models are extremely vulnerable. The standard ResNet-18 achieves 94.1\% clean accuracy, but under a 20-step PGD attack at $\epsilon = 8/255$ (a perturbation you cannot see with your eyes) it drops to 1.2\%. That is a near-complete failure, and it validates the security concerns that researchers have been raising.

Second, you cannot rely on FGSM alone to evaluate robustness. FGSM suggests the model still has 18.3\% accuracy, which sounds bad but nowhere near as bad as what PGD reveals (1.2\%). If someone only tested with FGSM, they would significantly underestimate how vulnerable their model is.

Third, adversarial training genuinely helps. PGD-based adversarial training brings robust accuracy from near-zero up to 45.1\% under PGD attack, at the cost of a 10.2\% drop in clean accuracy. This trade-off is consistent with what Madry et al.~\cite{madry2018towards} and others have reported.

Fourth, adversarial training also improves confidence calibration. The standard model gives near-100\% confidence to its wrong adversarial predictions, while the robust model produces more spread-out probability outputs. This is actually quite useful in practice because in real deployments, knowing when your model is uncertain can be just as important as the prediction itself.

\subsection{Comparison with Literature}

My results match up well with what has been published before. Madry et al.~\cite{madry2018towards} reported around 45 to 47\% PGD robustness at $\epsilon = 8/255$ for adversarially trained ResNet on CIFAR-10, and I got 45.1\%, so that is right in line. The clean accuracy of my robust model (83.9\%) also falls within the typical range of 82 to 87\% that you see in the literature depending on training details.

The DeepFool numbers are also consistent with the original work by Moosavi-Dezfooli et al.~\cite{moosavi2016deepfool}, who showed that deep networks tend to have small robustness radii when measured with minimal perturbation attacks.

\subsection{Limitations and Observations}

There are some limitations worth noting. I only ran experiments on CIFAR-10, which has small $32 \times 32$ images, so results could be different on higher-resolution datasets where the same perturbation budgets have different visual effects. I also only tested one architecture (ResNet-18), and different architectures might behave differently. For PGD evaluation I used a single random restart, which might slightly underestimate the true vulnerability; multiple restarts would give tighter bounds. And finally, I only looked at the $L_\infty$ threat model for FGSM and PGD; the $L_2$ threat model might tell a different story.

\section{Summary}

The experiments confirm that standard deep neural networks are extremely vulnerable to adversarial attacks and that adversarial training is an effective way to defend against them. PGD stands out as the strongest first-order attack, reducing standard model accuracy to near-zero with perturbations that humans cannot even notice. Adversarial training brings back a meaningful level of robustness at the cost of some clean accuracy, and my results match the published benchmarks closely. These findings underline why adversarial robustness research matters, and the next chapter wraps up with conclusions and directions for future work.