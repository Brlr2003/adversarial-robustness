\chapter{RESULTS AND DISCUSSION}
\label{ch:results}

\section{Introduction}

Here are the results. I ran all three attacks on CIFAR-10, measured how badly they break a standard ResNet-18, applied adversarial training, and compared everything. All of this used the framework from Chapter~\ref{ch:implementation}, and I tracked everything with MLflow.

\section{Experimental Setup}

\subsection{Training Configuration}

I trained two ResNet-18 models on CIFAR-10. Identical in every way except one: the second one used PGD adversarial training. Same optimizer, same schedule, same epochs. Table~\ref{tab:training_config} has all the details.

\begin{table}[htbp]
    \centering
    \caption{Training hyperparameters for standard and adversarial training}
    \label{tab:training_config}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Standard Training} & \textbf{Adversarial Training} \\
        \midrule
        Batch size & 128 & 128 \\
        Optimizer & SGD & SGD \\
        Learning rate & 0.1 & 0.1 \\
        Momentum & 0.9 & 0.9 \\
        Weight decay & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
        LR schedule & Cosine annealing & Cosine annealing \\
        Epochs & 50 & 50 \\
        Data augmentation & RandomCrop, HFlip & RandomCrop, HFlip \\
        \bottomrule
    \end{tabular}
\end{table}

Standard training: about 55 minutes. Adversarial training: about 4.5 hours. Roughly a $7\times$ slowdown from the PGD generation at every step.

\subsection{Attack Parameters}

Table~\ref{tab:attack_params} lists what I used for each attack.

\begin{table}[htbp]
    \centering
    \caption{Attack parameters used during evaluation}
    \label{tab:attack_params}
    \begin{tabular}{lp{8cm}}
        \toprule
        \textbf{Attack} & \textbf{Parameters} \\
        \midrule
        FGSM & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$ \\
        PGD & $\epsilon \in \{2/255, 4/255, 8/255, 16/255\}$, step size $\alpha = \epsilon/4$, iterations $T = 20$, random start \\
        DeepFool & Max iterations = 50, overshoot = 0.02, 10 classes \\
        \bottomrule
    \end{tabular}
\end{table}

For the inner attack during adversarial training: PGD with $\epsilon = 8/255$, $\alpha = 2/255$, 7 iterations, random start. This matches Madry et al.~\cite{madry2018towards}.

\subsection{Evaluation Protocol}

Everything evaluated on the full CIFAR-10 test set --- all 10,000 images. PGD used one random restart (standard for benchmarking adversarially trained models). Attack success rates computed only over images the model classifies correctly on clean data.

Figure~\ref{fig:demo_interface} shows the web demo on HuggingFace Spaces. You can compare both models under any attack and epsilon in real time.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/stats.png}
    \caption{Interactive demo interface deployed on HuggingFace Spaces, showing model accuracy metrics and attack configuration controls for real-time adversarial robustness evaluation.}
    \label{fig:demo_interface}
\end{figure}

\section{Baseline Results}

\subsection{Model Performance on Clean Data}

Table~\ref{tab:clean_accuracy}: clean test accuracy.

\begin{table}[htbp]
    \centering
    \caption{Clean accuracy of trained models on CIFAR-10}
    \label{tab:clean_accuracy}
    \begin{tabular}{lc}
        \toprule
        \textbf{Model} & \textbf{Clean Accuracy (\%)} \\
        \midrule
        Standard ResNet-18 & 94.1 \\
        Adversarially Trained ResNet-18 & 83.9 \\
        \bottomrule
    \end{tabular}
\end{table}

94.1\% for the standard model. This is in line with what other people get for ResNet-18 on CIFAR-10 without input normalization. The adversarially trained model gets 83.9\% --- a 10.2 percentage point penalty. Not surprising; this trade-off is well documented~\cite{madry2018towards}. More on this in Section~\ref{sec:tradeoffs}.

\section{Attack Evaluation Results}

\subsection{FGSM Attack Results}

Table~\ref{tab:fgsm_results}: robust accuracy under FGSM at different epsilon values.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under FGSM attack on CIFAR-10}
    \label{tab:fgsm_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 68.2 & 42.5 & 18.3 & 5.1 \\
        Adv-Trained & 76.8 & 68.4 & 56.0 & 38.2 \\
        \bottomrule
    \end{tabular}
\end{table}

The standard model drops from 94.1\% to 18.3\% at $\epsilon = 8/255$. That is a big drop for a perturbation you cannot see. The adversarially trained model holds at 56.0\% --- a 37.7 pp advantage. Figure~\ref{fig:attack_fgsm} shows a visual example.

\subsection{PGD Attack Results}

Table~\ref{tab:pgd_results}: PGD-20 results. This is the real test.

\begin{table}[htbp]
    \centering
    \caption{Robust accuracy (\%) under PGD-20 attack on CIFAR-10}
    \label{tab:pgd_results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & $\epsilon=2/255$ & $\epsilon=4/255$ & $\epsilon=8/255$ & $\epsilon=16/255$ \\
        \midrule
        Standard & 41.3 & 12.8 & 1.2 & 0.1 \\
        Adv-Trained & 72.1 & 58.3 & 45.1 & 22.6 \\
        \bottomrule
    \end{tabular}
\end{table}

1.2\%. That is what is left of the standard model at $\epsilon = 8/255$ under PGD. Compare that to 18.3\% under FGSM --- the single-step attack gives a very misleading picture. The model looks sort of okay under FGSM but is basically completely broken under PGD. The adversarially trained model keeps 45.1\% at $\epsilon = 8/255$. At $\epsilon = 16/255$, the standard model is at 0.1\% (essentially random) while the robust model still gets 22.6\%. See Figure~\ref{fig:attack_pgd}.

\subsection{DeepFool Attack Results}

DeepFool is different --- it finds the minimum perturbation per image. Table~\ref{tab:deepfool_results}.

\begin{table}[htbp]
    \centering
    \caption{DeepFool attack results on CIFAR-10}
    \label{tab:deepfool_results}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{Success Rate (\%)} & \textbf{Avg. $L_2$ Perturbation} & \textbf{Avg. $L_\infty$ Perturbation} \\
        \midrule
        Standard & 91.1 & 0.248 & 0.021 \\
        Adv-Trained & 62.0 & 0.892 & 0.074 \\
        \bottomrule
    \end{tabular}
\end{table}

91.1\% of correctly classified images fooled, with an average $L_2$ perturbation of just 0.248. That is tiny. The decision boundaries are very, very close to the data. For the adversarially trained model, DeepFool needs about $3.6\times$ bigger perturbations and only gets 62.0\% success. The boundaries moved. See Figure~\ref{fig:attack_deepfool}.

\subsection{Visual Examples}

Figure~\ref{fig:adversarial_examples} --- this is probably the most intuitive way to understand what is going on. A CIFAR-10 cat image, attacked with FGSM and PGD at $\epsilon = 8/255$. Look at the original and the adversarial image side by side. Can you see a difference? I can't. But the standard model goes from ``cat'' (correct) to ``dog'' (wrong, 99\% confidence).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/FGSM.png}
        \caption{FGSM attack ($\epsilon = 8/255$): the standard model is fooled with near-100\% confidence on the wrong class, while the robust model correctly identifies the cat with approximately 52\% confidence.}
        \label{fig:attack_fgsm}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/PGD.png}
        \caption{PGD attack ($\epsilon = 8/255$, 20 steps): the stronger iterative attack also fools the standard model with near-100\% confidence, while the robust model maintains the correct prediction.}
        \label{fig:attack_pgd}
    \end{subfigure}

    \caption{Visual comparison of FGSM and PGD adversarial attacks on a CIFAR-10 cat image. Each panel shows the original image, the adversarial perturbation (amplified for visibility), the adversarial image, and the predictions of both the standard model (fooled) and the robust model (correct). The perturbations are imperceptible to human observers yet cause confident misclassification in the standard model.}
    \label{fig:adversarial_examples}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/DeepFool.png}
    \caption{DeepFool attack on the same CIFAR-10 cat image: finds the minimal perturbation to cross the decision boundary. The standard model is fooled, while the robust model resists the minimal perturbation. Unlike FGSM and PGD (Figure~\ref{fig:adversarial_examples}), DeepFool does not use a fixed $\epsilon$ budget but instead computes the smallest perturbation needed to change the prediction.}
    \label{fig:attack_deepfool}
\end{figure}

Something I noticed while playing with the demo: the confidence scores tell a story too. When the standard model gets fooled, it is not just wrong --- it is wrong with 99\% confidence. That is actually worse than being wrong with 50\% confidence, because it means the model does not ``know'' anything is off. The adversarially trained model, by contrast, tends to give 50--60\% confidence even on correct predictions. It is less sure of itself, which is actually useful. You can tell when it is being challenged.

\section{Adversarial Training Results}

\subsection{Training Dynamics}

Standard training: loss went from about 1.55 to 0.034 over 50 epochs. Training accuracy hit 98\%, test accuracy 94.1\%. No sign of overfitting.

Adversarial training: training loss stayed higher (the model is solving a harder problem --- classifying adversarial examples, not clean ones). Clean test accuracy settled at 83.9\%. PGD robustness accuracy --- the number I used for checkpoint selection --- peaked at 50.6\%.

\subsection{PGD Adversarial Training on CIFAR-10}

Table~\ref{tab:adv_training_comparison}: everything at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comprehensive comparison at $\epsilon = 8/255$ on CIFAR-10}
    \label{tab:adv_training_comparison}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Clean} & \textbf{FGSM} & \textbf{PGD-20} & \textbf{DeepFool} & \textbf{Avg. Robust} \\
        \midrule
        Standard & 94.1 & 18.3 & 1.2 & 8.9 & 9.5 \\
        Adv-Trained & 83.9 & 56.0 & 45.1 & 38.0 & 46.4 \\
        \midrule
        Improvement & $-10.2$ & $+37.7$ & $+43.9$ & $+29.1$ & $+36.9$ \\
        \bottomrule
    \end{tabular}
\end{table}

You lose 10.2 points of clean accuracy. You gain an average of 36.9 points of robust accuracy. The biggest gain is on PGD (+43.9 pp), which makes sense --- that is the attack used during training. But FGSM and DeepFool benefit too.

\subsection{Accuracy Trade-offs}
\label{sec:tradeoffs}

10.2\% is not nothing. But think about it this way: would you rather have a model that gets 94\% on clean data and 1\% when attacked, or one that gets 84\% on clean data and 45\% when attacked? If the application is anything safety-related, the answer is pretty clear.

The reason for the drop is that adversarial training forces the model to use different features. Features that are robust to worst-case perturbations are not the same features that maximize clean accuracy. There is a genuine tension between the two objectives, and 10.2\% is what that tension costs on CIFAR-10 with ResNet-18.

\subsection{Computational Cost}

Adversarial training is about $7\times$ slower per epoch. Standard training: 55 minutes for 50 epochs. Adversarial training: 4.5 hours. The reason: 7-step PGD at every training batch means 7 extra forward and 7 extra backward passes. On my M4 Pro this is manageable but on ImageNet with a bigger model it would be a serious bottleneck.

\section{Attack Comparison}

\subsection{Success Rate and Strength Comparison}

Table~\ref{tab:attack_comparison}: all three attacks head-to-head at $\epsilon = 8/255$.

\begin{table}[htbp]
    \centering
    \caption{Comparison of attack methods on CIFAR-10 standard model ($\epsilon = 8/255$)}
    \label{tab:attack_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Attack} & \textbf{Robust Acc. (\%)} & \textbf{Avg. $L_2$} & \textbf{Avg. $L_\infty$} & \textbf{Time per batch (ms)} \\
        \midrule
        FGSM & 18.3 & 1.351 & 0.031 & 12 \\
        PGD-20 & 1.2 & 1.351 & 0.031 & 185 \\
        DeepFool & 8.9 & 0.248 & 0.021 & 2,450 \\
        \bottomrule
    \end{tabular}
\end{table}

PGD wins. 1.2\% versus 18.3\% for FGSM and 8.9\% for DeepFool. And yet FGSM and PGD use the exact same $L_\infty$ budget. Same maximum perturbation. But PGD, because it iterates, finds much better (from the attacker's perspective) perturbations within that budget. DeepFool uses much smaller perturbations ($L_2 = 0.248$ vs 1.351) and still fools most images --- because its goal is different: minimum perturbation, not maximum damage.

Timing-wise: FGSM is 12 ms per batch (one gradient), PGD is 185 ms (20 gradients), DeepFool is 2,450 ms (per-image processing with class-wise gradients). This is why PGD is the standard evaluation method --- it is strong and reasonably fast. DeepFool gives you different information (robustness radius) but it is too slow for routine evaluation.

\subsection{Perturbation Analysis}

The perturbation numbers tell an interesting story. FGSM and PGD both max out the $\epsilon$ budget. Every pixel gets perturbed by exactly $\epsilon$ in the gradient direction. DeepFool, though, uses $5.4\times$ less $L_2$ norm on average and still fools 91.1\% of images. What does that mean? It means the standard model's decision boundaries are alarmingly close to the data. The average distance from a data point to the nearest wrong-class boundary is just 0.248 in $L_2$. Not much margin at all.

After adversarial training, DeepFool needs 0.892 $L_2$ on average. So the boundaries did move --- roughly $3.6\times$ further out. That is what adversarial training is supposed to do, and it does it.

\section{Discussion}

\subsection{Key Findings}

Four things stand out:

One, the standard model is shockingly fragile. 94.1\% clean accuracy, 1.2\% under PGD at $\epsilon = 8/255$. The perturbation is invisible. The failure is near-total.

Two, FGSM is not enough for evaluation. It gives 18.3\%, which sounds bad but is nowhere close to the real picture (1.2\%). If I had only tested with FGSM, I would have dramatically overestimated my model's robustness.

Three, adversarial training works. From 1.2\% to 45.1\% under PGD, at the cost of 10.2\% clean accuracy. These numbers match what Madry et al.~\cite{madry2018towards} reported, which gives me confidence the implementation is correct.

Four, adversarial training does something to confidence calibration. The standard model says ``99\% dog'' when it should say ``I'm not sure.'' The robust model spreads its probabilities more evenly. For real-world deployment, knowing when your model is uncertain is almost as important as getting the right answer.

\subsection{Comparison with Literature}

The numbers are consistent. Madry et al.~\cite{madry2018towards} got 45--47\% PGD robustness at $\epsilon = 8/255$ on adversarially trained CIFAR-10 ResNet; I got 45.1\%. Clean accuracy of 83.9\% is within the 82--87\% range you see in the literature. The DeepFool robustness radii are also in line with what Moosavi-Dezfooli et al.~\cite{moosavidezfooli2016deepfool} found --- standard networks have very small margins.

\subsection{Limitations and Observations}

Some caveats. I only used CIFAR-10, which is $32 \times 32$. The same epsilon budget looks different on bigger images. I only tested ResNet-18 --- other architectures might be more or less vulnerable. I used one PGD random restart; multiple restarts would give tighter bounds. And I only looked at $L_\infty$ for FGSM and PGD; $L_2$ could tell a different story.

\section{Summary}

Standard models are fragile. PGD exposes this most clearly. Adversarial training brings real robustness at the cost of some clean accuracy. My numbers match the literature, which is reassuring. The next chapter wraps up.
